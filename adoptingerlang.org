#+TITLE:     Adopting Erlang
#+AUTHOR:    Fred Hebert, Tristan Sloughter
#+DRAWERS: HIDDEN HINT SOLUTION
#+EMAIL:     t@crashfast.com
#+DESCRIPTION: Adopting Erlang.
#+KEYWORDS: erlang

# \setcounter{secnumdepth}{-1}

#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [oneside,11pt]
#+ATTR_LATEX: :width 4in
#+OPTIONS: H:6
#+LATEX_HEADER: \usepackage[Bjornstrup]{fncychap}
#+LATEX_HEADER: \usepackage[svgnames]{xcolor}
#+LATEX_HEADER: \usepackage[tikz]{bclogo}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{minted}
#+latex_header: \usepackage{xcolor}
#+latex_header: \usemintedstyle{monokai}    %% sets default for all source-code blocks
#+latex_header: \definecolor{friendlybg}{HTML}{f0f0f0}
#+latex_header: \definecolor{dark}{HTML}{272822}   %% custom colour for background
#+latex_header: \setminted{style=friendly, bgcolor=friendlybg, frame=lines, breaklines, breakanywhere}
#+LATEX_HEADER: \newenvironment{admonition}{\begin{bclogo}}{\end{bclogo}}
#+OPTIONS: ^:{}
#+HUGO_BASE_DIR: .
#+HUGO_SECTION: docs
#+HUGO_PAIRED_SHORTCODES: %admonition
#+hugo_auto_set_lastmod: t

* Introduction
:PROPERTIES:
:EXPORT_HUGO_SECTION: docs/introduction
:END:

** DONE Index
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_FRONT_MATTER_KEY_REPLACE: title>label
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :title "Introduction"
:EXPORT_HUGO_MENU: :menu main :weight 1001 :identifier introduction
:END:

#+BEGIN_EXPORT html
<header>
  <h1>Introduction</h1>
  <h5>
    <strong>August 3, 2019</strong>
  </h5>
</header>
#+END_EXPORT

So you've been checking Erlang out, and you've read a few tutorials and a bunch of books, but you're finding that nothing really tells you how you should set up a modern project for you and your team, and you have then found this text.  The Erlang community has collectively spent years writing introductory content, and a lot of it is still really good. So what's the hope for yet another book?

Frankly, most of the material out there is really solid to teach you the Erlang basics, and we have no pretension of replacing them, nor a desire to re-explain the same content they contain once more. Instead, we the authors felt like while they are rock solid in a lot of areas, we could help cover some blind spots.

For example, here are a bunch of interesting books:

- [[https://www.goodreads.com/book/show/808814.Programming_Erlang][Programming Erlang]], by Joe Armstrong is great to get into the philosophy behind Erlang
- [[https://www.goodreads.com/book/show/4826120-erlang-programming][Erlang Programming]], by Cesarini & Thompson is a very well-rounded practical approach to learning the language and bits of OTP
- [[https://www.goodreads.com/book/show/7438968-erlang-and-otp-in-action][Erlang and OTP in Action]], by Logan, Merritt & Carlsson is the first Erlang book that really tries to teach you OTP-first and hints at broader system design
- [[https://learnyousomeerlang.com/][Learn You Some Erlang]], by Fred Hebert, is possibly the friendliest introduction to both Erlang and OTP that tries to cover everything from basic Erlang to the design principles underpinning OTP, releases, and the whole ordeal
- [[https://www.goodreads.com/book/show/17984681-tudes-for-erlang][Études for Erlang]] by J. David Eisenberg is a fantastic companion exercise book that works with a bunch of other Erlang books, as a kind of practical complement
- [[https://www.goodreads.com/book/show/808815.Concurrent_Programming_ERLANG][Concurrent Programming ERLANG]] by Williams & Armstrong is a great piece of history from the 90s, showing earlier iterations of Erlang and how it could be applied to real world problems
- [[https://www.goodreads.com/book/show/15811999-introducing-erlang][Introducing Erlang]] by Simon St. Laurent is the most concise taste of Erlang you can get in book form
- [[https://www.erlang-in-anger.com/][Erlang in Anger]] by Fred Hebert is the only book that really contains a complete guide to debugging your Erlang systems in production
- [[https://www.goodreads.com/book/show/18324312-designing-for-scalability-with-erlang-otp][Designing for Scalability with Erlang/OTP]] by Cesarini & Vinoski is likely the most modern approach to Erlang/OTP systems with a real-world slant to it.
- [[https://blog.stenmans.org/theBeamBook/][The BEAM Book]] by Erik Stenman (and a lot of community contributors) is the most advanced resource on the virtual machine internals
- [[https://propertesting.com/][Property-Based Testing with PropEr, Erlang, and Elixir]] by Fred Hebert is the one book that teaches Property-based testing for Erlang

And there are some more too.

We intend to replace none of these. One huge omission in most (if not all) of these books, is that they tend to focus on Erlang/OTP on its own. In fact, many of these were written before massive shifts in how the community works. For example, _Learn You Some Erlang_, while very complete, was being written before _any_ community-driven build tool would see massive adoption, and before concepts such as OTP Releases would see widespread use. None of them have really been written under the new age of containerized platforms currently in place. And pretty much none of them mention how you should structure your projects to fit well within the open source Erlang ecosystem.

So this is what _Adopting Erlang_ is all about. This book (and website!) is all about filling in the niche that other books and manuals have not yet managed to properly cover. What you will learn in these pages will contain actually super useful stuff like:

- How to set yourself up to use multiple Erlang versions, because in the real world, you end up having to run multiple Erlang versions for the multiple projects your workplace or group of friends will end up using.
- We also cover how to set up editors and other tools, because chances are you may not have a good Erlang setup going even if you've already seen the basics
- How to approach OTP systems from the top. Most resources out there take a bottom-up approach, but we want you to be able to have the right project structure from day one, and then fill in the gaps with other resources as you need them
- What's needed for a good project, including dependency handling, some testing practices, handling configuration and documentation, and so on
- How to set up a good _Continuous Integration_ (CI) pipeline on common open platforms so that code reviews and automated testing can get the best support they can
- How to handle a bunch of difficult stuff nobody really teaches properly, like dealing with strings, specifically Unicode, time and proper SSL/TLS configuration
- How to deploy your Erlang systems as a self-executable bundle of files
- How to properly package your Erlang systems as Docker images, and showing how to manage their lifecycle with Kubernetes
- How to plan on setting up operations and get metrics going _outside_ of what the VM provides; think of things like logging and distributed tracing, platforms to get metrics dashboards going, and so on
- How to build a team that will start using Erlang in commercial projects
- How to interview your first Erlang experts or developers
- How to structure your practices for things such as code reviews, experience sharing, and so on.

By opposition, we will _not_ cover things like basic Erlang, core OTP behaviours, and so on. They have been covered multiple times in other resources in the past, many of which are freely available. We still have put together an appendix of cheat sheets you can refer to if you need a refresher.

Essentially, we hope for _Adopting Erlang_ to be the missing link between all the various starter books  and the more advanced material like _Erlang in Anger_ that lets you debug stuff in production. We want this book to teach you how to go from "Okay, I think I got the basics" to "let's get this project going, and let's do it right." After reading this book, you should be able to know exactly what the best practices are to fit right in with the rest of the Erlang community.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/">← Prev</a></div>
  <div><a href="/docs/introduction/about_the_authors">Next →</a></div>
</div>
#+END_EXPORT

** DONE About the Authors
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: about_the_authors
:EXPORT_HUGO_MENU: :menu main :parent introduction
:END:

*** Tristan Sloughter

Tristan is a long time Erlang programmer, having picked it up for fun while in
college and then professionally for various companies, Orbitz Worldwide,
eCDMarket, Heroku, SpaceTime Insight, Postmates, Splunk and currently as a lead
software engineer at [MyDecisive.AI](https://www.mydecisive.ai/).

While at Heroku, Tristan, along with Fred, started the build tool Rebar3 after seeing the struggles involved in onboarding new developers to Erlang projects. He is also a maintainer of the release tool Relx.

Tristan also created the website [[https://howistart.org/][How I Start]] which collects articles for various languages from experienced developers on how they setup a new project and take it to completion, while also giving a peek at the tools and packages these top developers prefer.

- [[https://sloughter.dev/][website]]
- [[https://adoptingerlang.org/twitter/tsloughter][@t_sloughter]]
- [[http://blog.erlware.org/][blog]]

*** Fred Hebert

Fred is the author of [[https://learnyousomeerlang.com][_Learn You Some Erlang_]], [[https://erlang-in-anger.com][_Erlang in Anger_]], and more recently, [[https://propertesting.com][_Property-Based Testing with PropEr, Erlang, and Elixir_]]. He is a maintainer of Rebar3, and of libraries such as recon, pobox, vmstats, and backoff.

He is a Staff SRE at Honeycomb, helping with systems reliability, incident response, and tooling. He previously held various senior and staff roles on engineering teams as a software developer most often working with Erlang at companies such as Postmates, Genetec, and Heroku.

- [[https://twitter.com/mononcqc][@mononcqc]]
- [[https://ferd.ca][blog]]

*** Evan Vigil-McClanahan

Evan has been writing Erlang professionally since 2012 and in various other languages since 2001.  He's worked with Erlang at Basho Technologies, Heroku, SpaceTime Insight, and most recently Helium, where his broad focus has been on distributed systems along with high- and low-level performance.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/introduction">← Prev</a></div>
  <div><a href="/docs/development">Next →</a></div>
</div>
#+END_EXPORT

** DONE Changelog
CLOSED: [2024-06-07 Sun 12:30]
:PROPERTIES:
:EXPORT_FILE_NAME: changelog
:EXPORT_HUGO_MENU: :menu main :parent introduction
:END:

This changelog tracks major changes to the text by snapshot version since Sept
7, 2023.

A snapshot is a copy of the site as it is on a date before some major changes
were to be made. The first was done July 6, 2024, so any changes to the text
before this date have to be seen through the [Github
repo](https://github.com/adoptingerlang/adoptingerlang/) history or the [web
archive](http://web.archive.org/web/20240000000000*/http://adoptingerlang.org).

*** [main](https://www.adoptingerlang.org/)

- [Switch to ctlptl and kind for local/test k8s cluster management](https://github.com/adoptingerlang/adoptingerlang/pull/112)

*** [Snapshot 2024-06-06](https://2024-06-06.adoptingerlang.org/)

- [Switch docker images from alpine to
  debian](https://github.com/adoptingerlang/adoptingerlang/pull/107)
- [Update to OTP-26 and update liveness and readiness probe
  descriptions](https://github.com/adoptingerlang/adoptingerlang/pull/106)
- [remove unneeded parts about setting +S schedulers in
  containers](https://github.com/adoptingerlang/adoptingerlang/pull/102)

* Development
:PROPERTIES:
:EXPORT_HUGO_SECTION: docs/development
:EXPORT_HUGO_PAIRED_SHORTCODES: %admonition
:END:
#+latex_header: \usepackage[utf8]{inputenc}
#+latex_header: \usepackage{pmboxdraw} % for directory listings
#+latex_header: \usepackage{textalpha} % for greek a
#+latex_header: \usepackage[T2A]{fontenc} % cyrilic a
#+latex_header: \DeclareUnicodeCharacter{0430}{\cyra} % cyrilic a
#+latex_header: \DeclareUnicodeCharacter{03A9}{Ω} % omega vs. ohm
#+latex_header: \DeclareUnicodeCharacter{267B}{\includegraphics[height=\fontcharht\font`\B]{./static/img/recycling.png}}
#+latex_header: \DeclareUnicodeCharacter{FDFD}{\includegraphics[height=\fontcharht\font`\B]{./static/img/bismillah.png}}
#+latex_header: \DeclareUnicodeCharacter{1F469}{\includegraphics[height=\fontcharht\font`\B]{./static/img/woman.png}}
#+latex_header: \DeclareUnicodeCharacter{1F466}{\includegraphics[height=\fontcharht\font`\B]{./static/img/boy.png}}
#+latex_header: \DeclareUnicodeCharacter{1F914}{\includegraphics[height=\fontcharht\font`\B]{./static/img/thinking.png}}
#+latex_header: \DeclareUnicodeCharacter{200D}{\hspace{0pt}}


** DONE Index
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_FRONT_MATTER_KEY_REPLACE: title>label
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :title "Development"
:EXPORT_HUGO_MENU: :menu main :weight 2001 :identifier development
:END:

#+BEGIN_EXPORT html
<header>
  <h1>Development</h1>
  <h5>
    <strong>August 3, 2019</strong>
  </h5>
</header>
#+END_EXPORT

The first section of this book is dedicated to getting a working installation, and understanding the actual structure of a project. This section will also cover how to import dependencies in your project, how to build projects that contain multiple OTP applications, write tests for your projects, and also a few other interesting topics.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/introduction/about_the_authors">← Prev</a></div>
  <div><a href="/docs/development/setup">Next →</a></div>
</div>
#+END_EXPORT

** DONE Setup
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: setup
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

You can't write Erlang without having Erlang installed, so the unsurprising first steps covered in this chapter will be to go through the basic steps required to install Erlang/OTP on most major platforms. The instructions will aim for a basic set-up on most platforms, but you'll find out that real world Erlang development is rarely done with just the basic instructions.

In fact, as teams grow and they accumulate various projects, chances are that not all services, libraries, or bits of code will all support the same exact version of Erlang, and won't be upgraded all at once. If you talk to developers who use Erlang professionally, most (aside from Windows users) will tell you that they just compile their own copies with the options they need, and with a tool that lets them switch between multiple versions. So we'll go through that.

You will also see how to install Rebar3, the official build tool for the Erlang community, and base configurations for various text editors. In later chapters, we'll additionally talk about other tools that aren't specific to Erlang such as Kubernetes or Prometheus, but let's get started with Just Erlang for now.

*** Installing Erlang/OTP

The first step is to get a proper install of Erlang/OTP in place. This is not going to be a uniform experience on all platforms, but we'll at least make sure everyone following these steps has a fully functioning setup for any work environment.

**** Choosing a Version

Erlang/OTP is released on a fairly stable and predictable schedule, with well-defined criteria for backwards-incompatible changes.

Erlang versions are numbered according to a =<Major>.<Minor>.<Patch>= scheme, as described in the [[http://erlang.org/doc/system_principles/versions.html#version_scheme][Erlang/OTP system principles]]. In some rare circumstances, other digits are bolted on as "branched" versions, which you likely won't have to care about.

Here are some example possible versions:

- 22.0
- 22.0-rc3
- 21.3
- 21.2.3
- 21.1
- 19.3
- 17.0
- R16B03 (this is a legacy version format that hasn't been used since 2014)

As you can see, the =Patch= version is not mentioned when no patch is required. The release schedule for Erlang goes a bit like this:

1. Once per year, around February or March, a release candidate for the next major version is announced (with a suffix such as =-rc1=, or =-rc2=). This release candidate is made available for users who want to build from source, in order to test that their applications and system will work well with it
2. A few months later (April to June), the major release is cut and made public. Major releases contain large new features that require bigger virtual machine changes, and are also allowed to introduce backwards-incompatible changes
3. At a frequency of every three or four months, a minor release is made public, which usually includes stability fixes and minor feature additions in individual libraries
4. If a critical bug has been found in some circumstances, either for security or stability reasons, a patch release may be announced.

#+attr_shortcode: info
#+begin_admonition
#+begin_export html
Backwards incompatible changes are usually going through a cycle of deprecation before being removed, which tends to leave ample time to adapt. The policy is described in the <a href='http://erlang.org/doc/system_principles/misc.html'>Support, Compatibility, Deprecations, and Removal</a> document published by the OTP team at Ericsson.
#+end_export
#+end_admonition

In some rare scenarios, hard-and-fast deprecations do happen (mostly by accident), and it may take a few weeks for the community to come up with workarounds.

A team that adopts Erlang will therefore likely want to adopt a maintenance schedule that fits the main releases if they want to avoid falling too far behind. While it is possible to only upgrade occasionally, you will find that it is often easier to do a bit of maintenance here and there than a lot of maintenance all at once.

Do note that patch-level releases are often only announced on the [[http://erlang.org/mailman/listinfo/erlang-questions][mailing lists]] and tagged on [[https://github.com/erlang/otp][the main git repository on GitHub]], but are otherwise not packaged on the main website.

**** Windows

If you are a Windows user, it is recommended that you use Windows 10 for any Erlang development. Prior versions can work, but community tools such as Rebar3 are only tested on Windows 10, for example.

Building on Windows from source has been notoriously difficult, and it is therefore recommended that you stick to the pre-built copies.

If you are a user of [[https://chocolatey.org][Chocolatey]], you can grab the [[https://chocolatey.org/packages/erlang][Erlang]] packages, and install them as you wish, with commands such as:

#+NAME: choco
#+BEGIN_SRC sh
choco install erlang                   # for the latest
choco install erlang --version 21.2 -m # allow many versions
choco install erlang --version 20.1 -m # and one more versions
#+END_SRC

This will add all the versions you want to your =PATH= variable, which you will then need to maintain in the right order.

Without Chocolatey, use binaries distributed on [[https://www.erlang.org/downloads][www.erlang.org/downloads]], or alternatively those built by [[https://www.erlang-solutions.com/resources/download.html][Erlang Solutions Ltd.]].

The installer for these versions comes with a wizard that will take you through all the required steps.

Do not forget to add Erlang/OTP to your =PATH= variable to contain your Erlang/OTP installation, since this will let you call it from the command-line:

1. In the start menu, search for "system environment variables" and select the "Edit the System and Environment Variables (Control Panel)" option
2. At the bottom of the "System Properties" window that has just open, press the "Environment Variables..." button
3. Select the =Path= variable (or create it if it does not exist) and click the "Edit" button
4. Add an entry for Erlang/OTP that matches the installation path, usually something like =C:\Program Files\erl10.2\bin=. The entries put earlier in the list will be loaded first.
5. Save the options
6. Close and restart any terminal you were running.

If you do development in the long term, you will be able to install multiple versions that way. You can control which one is used by changing and modifying the =PATH= variable's priorities in paths.

If you are a purist when it comes to Windows development, you may be quite comfortable in an environment such as Visual Studio, where pretty much everything can be done from within the IDE. Erlang comes from a different environment, and a lot of the instructions we'll use in this book are focused on using the command line to build everything.

If you are looking for a terminal to run the command line on Windows, various options are available:

- Use PowerShell as a terminal. Most commands in this book should work fine with it, but some edge cases may exist.
- Download and install [[https://git-scm.com/download/win][git for Windows]], which will come with a =git-bash= shell that will work well with all tooling and most commands in this book
- Try [[https://www.fosshub.com/ConEmu.htm][ConEmu]] as a nicer terminal emulator to work with
- Use [[https://cmder.net/][Cmder]] which is a Windows console emulator that packages most of the above options rather well
- Use [[https://www.cygwin.com][Cygwin]] at your own risk; you will need to rebuild your software from source to work well with it, and tools like Rebar3 dynamically figure out they're on Windows, which historically has caused a few path problems when interacting with Cygwin

You can then use the editor or IDE of your choosing to work with Erlang components.

**** OSX

While OSX makes it possible to use [[https://brew.sh/][Homebrew]] or [[https://www.erlang-solutions.com/resources/download.html][Erlang Solutions Ltd. packages]] to install pre-built versions of Erlang/OTP, you should only do so if you're trying things out the first time around. If you're planning on doing actual development for the longer haul, you'll instead want to be able to handle multiple versions at once.

The most commonly supported tool for this is [[https://github.com/kerl/kerl][kerl]]. Kerl is a wrapper around downloading, compiling, and loading various Erlang/OTP versions on a single system, and will abstract away most annoying operations.

You can install Kerl from homebrew by calling =$ brew install kerl=, or by following the instructions in its [[https://github.com/kerl/kerl#downloading][README file]].

Before installing Erlang, we will need to install and update a few dependencies, the main ones being to make sure you have [[https://developer.apple.com/xcode/][XCode]] installed and to then install OpenSSL (since OSX has terribly outdated copies of SSL by default):

#+NAME: openssl_osx
#+BEGIN_SRC sh
$ brew install openssl
...
$ ls /usr/local/Cellar/openssl/
1.0.2q
#+END_SRC

Note the full path this gives you for the local openssl install, here being =/usr/local/Cellar/openssl/1.0.2q/=

You can set the following options in your environment:

#+NAME: kerlcfg_osx
#+BEGIN_SRC sh
SSL_PATH=/usr/local/Cellar/openssl/1.0.2q/
export KERL_BUILD_BACKEND="git"
export KERL_CONFIGURE_OPTIONS="--without-javac \
                               --with-dynamic-trace=dtrace \
                               --with-ssl=${SSL_PATH}"
#+END_SRC

And ensure it's active (for example, call =source ~/.bashrc=). These options specify what is accepted or expected from the build tool. The one here disables Java bindings, and uses the new SSL install we've made. You can look at the [[https://github.com/erlang/otp/blob/master/HOWTO/INSTALL.md#configuring-1][Build Instructions]] for more configuration options.

If you want to add more content, such as =Wx= (which lets you use and build GUIs), the [[https://github.com/erlang/otp/blob/master/HOWTO/INSTALL.md#os-x-darwin][Build instructions for OSX]] contain further details to guide you.

From that point on, you can download and install your own Erlang/OTP versions:

#+NAME: kerl_osx
#+BEGIN_SRC sh
$ kerl update releases
...
# kerl build <release> <build name>
$ kerl build 21.3 21.3
...
# kerl install <build name> <target path>
$ kerl install 21.3 ~/bin/erls/21.3/
...
# make that version active
$ . ~/bin/erls/21.3/activate
# or alternatively
$ source ~/bin/erls/21.3/activate
#+END_SRC

Any installed version can then be activated on-demand. If you want to set a default version, you can put the activation command in your =.bashrc= configuration file (or any shell profile you might have).

If you are planning on using both Erlang and Elixir on your development machine, you might want to take a look at [[https://asdf-vm.com/#/core-manage-asdf-vm][=asdf=]]. It is a plugin-based installer for multiple programming languages, and can handle both Elixir and Erlang at once. You may need to install the =autoconf= package to make it work.

To use it with Erlang, install the [[https://github.com/asdf-vm/asdf-erlang][Erlang plugin]] by calling =asdf plugin-add erlang https://github.com/asdf-vm/asdf-erlang.git=. This plugin wraps =kerl= and reuses all of its options, but transfers the builds under =asdf='s control. As such, the previous configuration instructions remain the same. You just have to change the sequence of calls for:

#+NAME: asdf_linux
#+BEGIN_SRC sh
# asdf install erlang <version>
$ asdf install erlang 21.3
...
# asdf global <name> <version> [<version>...]
# asdf local <name> <version> [<version>...]
# export ASDF_ERLANG_VERSION=<version>
#+END_SRC

The main difference between =kerl= and =asdf= from there on is that =kerl= will use environment variables to know which version to run, and =asdf= will optionally use a =.tool-versions= file to trigger the change on a per-directory basis.


**** Linux

Linux distributions pretty much all have package managers that let you install pre-built copies of Erlang, or you can still use [[https://www.erlang-solutions.com/resources/download.html][Erlang Solutions Ltd. packages]]. Much like with OSX though, you should only do so if you're trying things out the first time around. If you're planning on doing actual development for the longer haul, you'll instead want to be able to handle multiple versions at once.

The most commonly supported tool for this is [[https://github.com/kerl/kerl][kerl]]. Kerl is a wrapper around downloading, compiling, and loading various Erlang/OTP versions on a single system, and will abstract away most annoying operations.

You can install kerl by calling:

#+NAME: linux_kerl
#+BEGIN_SRC sh
$ curl -O https://raw.githubusercontent.com/kerl/kerl/master/kerl
$ chmod a+x kerl
#+END_SRC

And then moving kerl to your path. Kerl will automatically check and warn you about missing dependencies you might be needing when building libraries, so you can just go ahead and run the following commands, and listen to its directions as you go.

First, you can set options as follows in your environment:

#+NAME: kerlcfg_linux
#+BEGIN_SRC sh
export KERL_BUILD_BACKEND="git"
export KERL_CONFIGURE_OPTIONS="--without-javac \
                               --with-dynamic-trace=systemtap"
#+END_SRC

And ensure it's active (for example, call =source ~/.bashrc=). These options specify what is accepted or expected from the build tool. The one here disables Java bindings, but they would be skipped automatically anyway. You can look at the [[https://github.com/erlang/otp/blob/master/HOWTO/INSTALL.md#configuring-1][Build Instructions]] for more configuration options.

If you want to add more content, such as =Wx= (which lets you use and build GUIs), the [[https://github.com/erlang/otp/blob/master/HOWTO/INSTALL.md#building-with-wxerlang][Build instructions for Wx]] contain further details to guide you.

From that point on, you can download and install your own Erlang/OTP versions:

#+NAME: kerl_linux
#+BEGIN_SRC sh
$ kerl update releases
...
# kerl build <release> <build name>
$ kerl build 21.3 21.3
...
# kerl install <build name> <target path>
$ kerl install 21.3 ~/bin/erls/21.3/
...
# make that version active
$ . ~/bin/erls/21.3/activate
# or alternatively
$ source ~/bin/erls/21.3/activate
#+END_SRC

Any installed version can then be activated on-demand. If you want to set a default version, you can put the activation command in your =.bashrc= configuration file (or any shell profile you might have).

If you are planning on using both Erlang and Elixir on your development machine, you might want to take a look at [[https://asdf-vm.com/#/core-manage-asdf-vm][=asdf=]]. It is a plugin-based installer for multiple programming languages, and can handle both Elixir and Erlang at once. You may need to install the =autoconf= package to make it work.

To use it with Erlang, install the [[https://github.com/asdf-vm/asdf-erlang][Erlang plugin]] by calling =asdf plugin-add erlang https://github.com/asdf-vm/asdf-erlang.git=. This plugin wraps =kerl= and reuses all of its options, but transfers the builds under =asdf='s control. As such, the previous configuration instructions remain the same. You just have to change the sequence of calls for:

#+NAME: asdf_osx
#+BEGIN_SRC sh
# asdf install erlang <version>
$ asdf install erlang 21.3
...
# asdf global <name> <version> [<version>...]
# asdf local <name> <version> [<version>...]
# export ASDF_ERLANG_VERSION=<version>
#+END_SRC

The main difference between =kerl= and =asdf= from there on is that =kerl= will use environment variables to know which version to run, and =asdf= will optionally use a =.tool-versions= file to trigger the change on a per-directory basis.

**** FreeBSD

On FreeBSD, the experience with =kerl= (as reported in other sections) has been hit and miss. Sometimes, some patches are required to make things work as smoothly as on other platforms. The good news is that if you use either the BSD [[https://www.freebsd.org/doc/en/books/handbook/ports-using.html][ports]] or [[https://www.freebsd.org/doc/en/books/handbook/pkgng-intro.html][packages]], it will all work fine out of the box.

This is the easiest way forwards, but makes switching across versions a bit trickier since you don't get an Erlang version manager for free. However, BSD ports and packages do let you build any version supported at your liking.

For example you can call any of the following:

#+NAME: bsd_install
#+BEGIN_SRC sh
# pkg install erlang # default copy
# pkg install erlang-runtime20  # OTP-20.x
# ls /usr/ports/lang/erlang* # source install: pick the version directory
erlang/
...
erlang-runtime20/
erlang-runtime21/
erlang-wx/
# cd /usr/ports/lang/erlang-runtime21/
# make config-recursive     # configure all the deps
# make install
#+END_SRC

FreeBSD maintainers are generally good about ensuring things keep working fine on the main supported architectures, so if you're sticking to x86 and avoid ARM, you should have no major issues.

**** Making things Nice

Before you're done, you should go to your shell or terminal profile, and add a few environment variables. Specifically, you can use =ERL_AFLAGS= or =ERL_ZFLAGS= to add configuration switches to the =erl= executable at all times.

We'll use =ERL_AFLAGS= to turn on two neat features: outputting strings with Unicode support by default, and enabling shell history so that the Erlang shell remembers your commands between invocations. Add the following to your environment:

#+NAME: erl_aflags
#+BEGIN_SRC sh
export ERL_AFLAGS="+pc unicode -kernel shell_history enabled"
#+END_SRC

Things will feel a bit more modern that way.

*** Installing Rebar3

Rebar3 is the standard build tool within the Erlang community. It essentially bundles all of the other tools shipping with Erlang along with a few open-source ones, and makes them all work under a unified project structure.

There are a few ways to install Rebar3: from a pre-built binary, or from source, and then a last variant for a faster-running local install. Do note that in all cases, you need Erlang to have been installed already.

**** Pre-Built Binaries

Pre-built binaries can be found at [[https://www.rebar3.org/][www.rebar3.org]]. There's a big "Download" button with the latest stable version, but if you like to live more dangerously, you can grab [[https://s3.amazonaws.com/rebar3-nightly/rebar3][the latest _nightly_ build]] as well.

It is common to create a directory =~/bin/= to place commands line utilities like =rebar3=, which is where you might want to put the version you just downloaded. Call =chmod +x rebar3= on it to make sure it can run, and add it to your path with =export PATH=~/bin/:$PATH= in your =~/.bashrc=, =~/.zshrc= or equivalent.

Windows users who want to use the code from PowerShell or cmd.exe (rather than a terminal emulator) must ensure that a =rebar3.cmd= file is added:

#+NAME: rebar.cmd
#+BEGIN_SRC sh
@echo off
setlocal
set rebarscript=%~f0
escript.exe "%rebarscript:.cmd=%" %*
#+END_SRC

**** Building From Source

First make sure that you have git installed, and checkout the repository to build it:

#+NAME: rebar_bootstrap
#+BEGIN_SRC sh
$ git clone https://github.com/erlang/rebar3.git
$ cd rebar3
$ ./bootstrap
#+END_SRC

This will create a =rebar3= script file (along with a =rebar3.cmd= file on Windows).

**** Local Install

The local install form will let you take any of the previously built Rebar3 versions, and unpack them to a local directory from which the tool will be able to self-update at a later time:

#+NAME: rebar_local
#+BEGIN_SRC sh
$ ./rebar3 local install  # starting from a rebar3 not in PATH
===> Extracting rebar3 libs to ~/.cache/rebar3/lib...
===> Writing rebar3 run script ~/.cache/rebar3/bin/rebar3...
===> Add to $PATH for use: export PATH=$PATH:~/.cache/rebar3/bin
$ export PATH=$PATH:~/.cache/rebar3/bin
$ rebar3 local upgrade # this can be used to update to the latest stable copy
...
#+END_SRC


*** Configuring Editors

**** Editor-agnostic (via a Language Server)

A [[https://langserver.org][language server]] is an editor-agnostic solution which provides language features such as code completion, jump to definition and inline diagnostics. The [[https://erlang-ls.github.io/][Erlang LS]] language server implements those features for the Erlang programming language. It integrates with Emacs, VS Code, Sublime Text 3, Vim and probably many more text editors and IDEs which adhere to the [[https://microsoft.github.io/language-server-protocol/][LSP protocol]].

To get started with Erlang LS with a specific text editor, please refer to the "editors" section of the [[https://erlang-ls.github.io/][documentation]].

**** Visual Studio Code

The [[https://marketplace.visualstudio.com/items?itemName=pgourlain.erlang][Erlang extension]] by Pierrick Gourlain is recommended.

To configure the extension, go to the =Preferences= and then =Settings= menu. Within the VS Code window, unroll the =Extensions= menu until the =erlang configuration= section. Make sure that all the values are right, particularly the Erlang path and the Rebar3 path. With this in place, you can mix and match all the other extensions you'd like and things should be ready to go.

The code formatter may feel a bit janky; it respects the official Erlang repository's old rules of mixing tabs and spaces, and expects each tab is 8 spaces wide. This is not really use anywhere else, and if your Visual Studio Code is not configured that way (using 4 spaces for example), it will just look off.

Otherwise, that extension covers all the major features: jumping around code definitions, build tool support (although only =compile=, =eunit=, and =dialyzer= are supported in the command palette, you can still call =rebar3= directly from the terminal), intellisense, warnings as you type, and CodeLens features. If you look at the extension's documentation, you'll also find debugger support instructions.

All you've got to do then is configure themes and more general extensions to your liking.

**** Emacs

Erlang/OTP comes with an Emacs mode in the =tools= application, =lib/tools/emacs/=. The authors of this book who use Emacs stick to using this mode by having Emacs load directly from the latest Erlang version installed. There are a number of options for alternative modes and addons to use for fancier support in =erlang-mode=, here we will only discuss [[https://oremacs.com/swiper/][Ivy]] completions and [[https://www.flycheck.org/en/latest/][Flycheck]] syntax checking. But first we need [[https://jwiegley.github.io/use-package/][use-package]] which is a tool for isolating package configuration. Code for automatically installing =use-package= can be [[https://github.com/CachesToCaches/getting_started_with_use_package/blob/7d260ddf7b15160c027915340ff0c70ce05ea315/init-use-package.el][found here]]. Include that code in your =~/.emacs.d/init.el= so that on startup =use-package= is installed. Or use the [[https://jwiegley.github.io/use-package/installation/][installation instructions]] found on the =use-package= website.

This following bit of =elisp= code can be used to setup just =erlang-mode=, no Ivy or Flycheck involved, and have it load for Rebar3, Relx and other Erlang configuration files:

#+BEGIN_SRC elisp
(use-package erlang
  :load-path ("<PATH TO OTP>/lib/erlang/lib/tools-3.0/emacs/")
  :mode (("\\.erl?$" . erlang-mode)
         ("rebar\\.config$" . erlang-mode)
         ("relx\\.config$" . erlang-mode)
         ("sys\\.config\\.src$" . erlang-mode)
         ("sys\\.config$" . erlang-mode)
         ("\\.config\\.src?$" . erlang-mode)
         ("\\.config\\.script?$" . erlang-mode)
         ("\\.hrl?$" . erlang-mode)
         ("\\.app?$" . erlang-mode)
         ("\\.app.src?$" . erlang-mode)
         ("\\Emakefile" . erlang-mode)))
#+END_SRC

For using Ivy to get completion support add the =ivy-erlang-complete= package, set a custom Erlang root for it to use and run its =init= when the Erlang mode is configured:

#+BEGIN_SRC elisp
(use-package ivy-erlang-complete
  :ensure t)

(use-package erlang
  :load-path ("<PATH TO OTP>/lib/erlang/lib/tools-3.0/emacs/")
  :hook (after-save . ivy-erlang-complete-reparse)
  :custom (ivy-erlang-complete-erlang-root "<PATH TO OTP>/lib/erlang/")
  :config (ivy-erlang-complete-init)
  :mode (("\\.erl?$" . erlang-mode)
         ("rebar\\.config$" . erlang-mode)
         ("relx\\.config$" . erlang-mode)
         ("sys\\.config\\.src$" . erlang-mode)
         ("sys\\.config$" . erlang-mode)
         ("\\.config\\.src?$" . erlang-mode)
         ("\\.config\\.script?$" . erlang-mode)
         ("\\.hrl?$" . erlang-mode)
         ("\\.app?$" . erlang-mode)
         ("\\.app.src?$" . erlang-mode)
         ("\\Emakefile" . erlang-mode)))
#+END_SRC

Flycheck comes with Rebar3 support and can automatically detect a Rebar3 project, so all that is needed is the =flycheck= package:

#+BEGIN_SRC elisp
(use-package delight
  :ensure t)

(use-package flycheck
  :ensure t
  :delight
  :config (global-flycheck-mode))
#+END_SRC

The =:config (global-flycheck-mode)= argument to =use-package= will enable Flycheck for all code you edit in Emacs, the expressions given with =:config= are run after the package has been loaded. The =:delight= argument tells =use-package= to use the =delight= utility to disable showing Flycheck in the mode line. Keeping it out of the mode line saves space and especially since it is enabled globally we don't need it being called out as currently enabled in the mode line.

If you like using Flycheck then [[https://github.com/abo-abo/hydra][hydra]] is worth checking out for stepping through and viewing the full list of errors. The Hydra macro sets up short keybindings that work only when the initial Hydra binding has been run. The following code will setup basic bindings for viewing Flycheck errors when =C-c f= is called:

#+BEGIN_SRC elisp
(use-package hydra
  :defer 2
  :bind ("C-c f" . hydra-flycheck/body))

(defhydra hydra-flycheck (:color blue)
  "
  ^
  ^Errors^
  ^──────^
  _<_ previous
  _>_ next
  _l_ list
  _q_ quit
  ^^
  "
  ("q" nil)
  ("<" flycheck-previous-error :color pink)
  (">" flycheck-next-error :color pink)
  ("l" flycheck-list-errors))
#+END_SRC

Lastly, a couple packages that are not Erlang specific but are worth calling out as very useful when developing on a project:

- [[https://magit.vc/][magit]]: An Emacs interface for [[https://git-scm.com/][Git]]. Magit does not just allow for calling Git from Emacs but provides a streamlined interface for everything from staging changes to interactive rebases.
- =counsel-rg=: [[https://github.com/abo-abo/swiper/][counsel]] is a collection of commands that utilize Ivy, the package we used earlier for Erlang completions. =counsel-rg= uses  [[https://github.com/BurntSushi/ripgrep][ripgrep]] to search for strings across the files in a project -- when in a git project they act like =git grep=, only searching in files in the git repo and honoring =.gitignore=. Since ripgrep is an external commands it must be installed separately, for example on Ubuntu or Debian run =sudo apt-get install ripgrep=. When =ripgrep= is installed it will also be used by =ivy-erlang-complete= for faster searches.
- [[https://github.com/abo-abo/swiper/][swiper]]: An alternative to =isearch= for searching in a buffer that uses Ivy.
- [[https://company-mode.github.io/][company-mode]]: When combined with =ivy-erlang-complete= through [[https://github.com/s-kostyaev/company-erlang][company-erlang]] this mode will provide a popup of completions automatically, rather than requiring =C-:= to bring up completions in the minibuffer.
- [[https://github.com/flycheck/flycheck-inline][flycheck-inline]], [[https://github.com/flycheck/flycheck-pos-tip][flycheck-pos-tip]] or [[https://github.com/flycheck/flycheck-popup-tip][flycheck-popup-tip]]: These packages offer different options for displaying Flycheck errors at the position of the error instead of in the minibuffer.

**** Vim

Although absolutely fancy support for Erlang is possible in Vim—as the [[https://github.com/vim-erlang][vim-erlang group on Github]] allows—the authors of this book who use it tends to stick with the most minimal configuration possible.

Simply stick with the default syntax highlighting in your =.vimrc= file, and make sure it's used in all the right file types:

#+BEGIN_SRC vim
"also erlang
autocmd BufRead,BufNewFile *.erl,*.es.*.hrl,*.xrl,*.config setlocal expandtab noautoindent
au BufNewFile,BufRead *.erl,*.es,*.hrl,*.xrl,*.config setf erlang
#+END_SRC

This is the very basic stuff, obviously. Fancier integration is possible, but the one author who uses vim mostly uses only this, and relies on Rebar3 in a terminal to deal with the rest of the language.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development">← Prev</a></div>
  <div><a href="/docs/development/otp_high_level">Next →</a></div>
</div>
#+END_EXPORT

** DONE OTP at a High Level
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: otp_high_level
:EXPORT_HUGO_MENU: :menu main :parent development
:END:


Erlang/OTP is different from most programming environments out there, even those that also use a virtual machine. Erlang has a strong opinion about how your applications should be structured, the level of isolation they should have, and a separation between what Erlang's VM can do, and what your software can do. It's not just a programming language, it's a whole framework for building systems. Understanding its core principles is the key to getting started fast without having to rewrite everything later: it ensures that all applications can fit well together, that updates can be done live, and that your code is easy to instrument and make observable.

In this chapter, we'll cover the Erlang virtual machine and the core concepts of OTP at the highest level.

*** The Erlang Run-Time System

The foundational block for everything is the Erlang virtual machine itself, called BEAM. BEAM is technically a single implementation of the Erlang virtual machine, as there could be others. For example, Erllvm is an implementation over LLVM (using some custom patches to make everything possible), and an older implementation in the 90s was called JAM. The Erlang VM is implemented in C, and contains a lot of fancy stuff: schedulers to run processes, garbage collection, memory allocators, a timer wheel for events, a bunch of smart switches to abstract over operating system features and provide unified interfaces (such as over time management, file-handling drivers, and so on), a few built-in functions that go faster than what Erlang can do on its own (BIFs) and an interface for functions implemented natively in other languages (NIFs) along with special schedulers for them. There's obviously a lot more, but you can think of all that stuff the way you would with the kernel in BSD or Linux: low level stuff that you need in order to build fancier stuff.

If all you have is the virtual machine with nothing else, you can't run Erlang code. You don't have a standard library, you don't have libraries to even load code. To get it all going, there's some tricky bootstrapping going on that we don't need to understand. Just know that there's a limited set of pre-loaded Erlang modules that ship with the virtual machine, and those can be used to set up networking and file-handling stuff, which is in turn used to further load and run modules. If you're interested in knowing more though, please consult [[https://happi.github.io/theBeamBook/][The BEAM Book]] or [[http://beam-wisdoms.clau.se/][BEAM Wisdoms]].

If you take the virtual machine and the pre-loaded stuff, along with all the little utilities that make code-loading possible, you have what is essentially called the _Erlang Run-Time System_ (ERTS). The Run-Time System, when starting, follows the instructions of a thing called a _boot script_ (which nobody writes by hand) that specifies what to start.

Erlang, by default, provides boot scripts that load a minimal amount of code required to start a shell and write your own applications. Once this is done, we can start thinking about Erlang, and not just the virtual machine.

*** Erlang/OTP

What we have described so far is equivalent to an operating system's kernel. We now need the foundational blocks for the userspace components. In Erlang, this is essentially what OTP is about. OTP specifies how "components" that run on the virtual machine should be structured. There's more to the language than just "processes and messages": there's one well-defined way to structure your code.

#+attr_shortcode: note
#+begin_admonition
#+begin_export html
OTP stands for _Open Telecom Platform_, which is literally a meaningless name that was used to get the stuff open-sourced back in the old days of Erlang at Ericsson.
#+end_export
#+end_admonition

Erlang/OTP systems are structured through components named _OTP Applications_. Every Erlang version you have installed or system built with it that you use ships with a few OTP Applications. There are basically two variants of OTP applications: _Library Applications_, which are just collections of modules, and _Runnable Applications_, which contain a collection of modules, but also specify a stateful process structure stored under a supervision tree. For the sake of clarity, we're going to use the following terminology for OTP Applications for this entire book:

- _Library Applications_: stateless collections of modules
- _Runnable Applications_: OTP applications that start stateful supervision tree structures with processes running in them
- _OTP Applications_: either _Library_ or _Runnable Applications_, interchangeably

By default, the two OTP applications everyone includes are called =stdlib=, which is a library application that contains the core standard library modules such as =list= or =maps=, and =kernel=, which is a runnable application and sets up the core structure for an Erlang system that relies on OTP applications to work.

When a node boots, the modules from all required OTP applications are loaded in memory. Then =kernel= is started. =kernel= manages the lifecycle of the system from this point on. All other OTP applications and their configuration are handled through it, and so are unique features like distribution and hot code updates. If we go back to the operating system comparison, you can think of the =kernel= OTP application a bit like you could think of =systemd= for the Linux kernel (or =init= if you hate =systemd= or use a BSD -- Windows users can think of it as the service that runs other services)

In fact, =kernel= and =stdlib= are the only two applications you need for a basic working Erlang shell. When you type in =erl= (or start =werl= on Windows), this boots up the VM, along with kernel, with =stdlib= pre-loaded. Everything else is optional and can be loaded at a later time.

The standard Erlang distribution contains applications such as:

- kernel
- stdlib
- crypto (cryptographic primitives)
- ssl (TLS termination library)
- inets (network services such as FTP or HTTP clients)
- ct (Common Test framework)
- wx (graphic toolkit)
- observer (a control panel to manage your Erlang node, building on =wx=)
- compiler (the Erlang compiler to build your own project)
- and so on

All of these are put together into what is called an Erlang _release_. A release is a collection of OTP applications, possibly bundled together with a full copy of the virtual machine. As such, when you download and install Erlang, you just get a release whose name is something like _Erlang/OTP-21.3.4_. You're free to build your own releases, which will take some of the OTP applications in the standard distribution, and then bundle them with some of your own apps.

So if we were to write an app named =proxy= that relies on =ssh= and =ssl= (which themselves depend on =public_key=, =crypto=, =stdlib=, and =kernel=), we would make a release with all of these components in it:

- ERTS
- kernel
- stdlib
- crypto
- public_key
- ssl
- ssh
- proxy

A visual representation of this can be seen in Figure [[fig:proxy_release]].

#+CAPTION: Visual representation of building the =proxy= release
#+NAME:   fig:proxy_release
[[./static/img/proxy_release_draft.png]]

Essentially, building an Erlang system is re-bundling the VM, along with some standard applications provided with the default distribution, together with your own apps and libraries.

*** Living in Erlang/OTP

Standard tools developed and used by the community such as Rebar3 operate on the idea that what you write and publish are OTP applications, and as such contain all the functionality required to deal with them. That's a big shift from a lot of programming languages that only ask of you to have a function named =main()= somewhere in one of your files. This is why the programming language is often called =Erlang/OTP= rather than just 'Erlang': it's not just a programming language, it's a general development framework that mandates some basic structure for everything you do.

And everyone follows it, whether they are writing embedded software, blockchain systems, or distributed databases. It's OTP or nothing. Whereas other languages usually mandate nothing specific to get started, but then add some requirements later on (such as when integrating with a package manager), Erlang--and its entire community--expects you to just write OTP applications, which the rest of the tools can handle.

So the key to getting started fast in Erlang is to know the framework, which is often kept as more advanced material. Here we're going to do things upside down and start from a fully functional release, and then dig down into its structure. The next chapters will be dedicated to understanding how to work within these requirements.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/setup">← Prev</a></div>
  <div><a href="/docs/development/otp_applications">Next →</a></div>
</div>
#+END_EXPORT

** DONE OTP Applications
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: otp_applications
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

Since every component to be shipped in an Erlang/OTP release needs to be an OTP Application, it will do you a great good to understand what they are and how they work. In this chapter, we'll go over the basic structure of an OTP application, and what that means for your project.

*** Project Structure

We'll start by using the Rebar3 templates, since they will allow us to create brand new projects that properly respect the directory structures expected by Erlang/OTP. Let's see which templates are available:

#+NAME: rebar3_new
#+BEGIN_SRC sh
$ rebar3 new
app (built-in): Complete OTP Application structure.
cmake (built-in): Standalone Makefile for building C/C++ in c_src
escript (built-in): Complete escriptized application structure
lib (built-in): Complete OTP Library application (no processes) structure
plugin (built-in): Rebar3 plugin project structure
release (built-in): OTP Release structure for executable programs
umbrella (built-in): OTP structure for executable programs
                     (alias of 'release' template)
#+END_SRC

Here's a table showing when they might be used:

| Type of Project                    | Template to use | Comments                                                            |
|------------------------------------+-----------------+---------------------------------------------------------------------|
| script or command line tool        | escript         | Requires Erlang to be installed by the user                         |
| a library (collection of modules)  | lib             | Can be used as a dependency                                         |
| a library (stateful processes)     | app             | Can be used as a dependency                                         |
| full executable program            | umbrella or app | Can be turned into a full release, the recommended deploy mechanism |
| a collection of multiple libraries | umbrella        | Cannot be used as a git dependency but each individual app could be published to hex |
| Rebar3 extension                   | plugin          |                                                                     |
| compiling C code                   | cmake           | Also see the "pc" plugin for a portable way to compile C/C++        |

You can see the details of a given template by calling =rebar3 new help <template>=. See for example:

#+NAME: rebar3_new_lib
#+BEGIN_SRC sh
$ rebar3 new help lib
lib:
  built-in template
  Description: Complete OTP Library application (no processes) structure
  Variables:
    name="mylib" (Name of the OTP library application)
    desc="An OTP library" (Short description of the app)
    date="2019-03-15"
    datetime="2019-03-15T19:52:31+00:00"
    author_name="Fred Hebert"
    author_email="mononcqc@ferd.ca"
    copyright_year="2019"
    apps_dir="apps" (Directory where applications will be created if needed)
#+END_SRC

The values can be modified as desired on the command line, but those are the default variables. Let's see what we get by writing our own:

#+NAME: rebar3_new_mylib
#+BEGIN_SRC sh
$ rebar3 new lib mylib desc="Checking out OTP libs"
===> Writing mylib/src/mylib.erl
===> Writing mylib/src/mylib.app.src
===> Writing mylib/rebar.config
===> Writing mylib/.gitignore
===> Writing mylib/LICENSE
===> Writing mylib/README.md
#+END_SRC

Go to the =mylib= directory, and call =rebar3 compile= right away:

#+NAME: rebar3_mylib_compile
#+BEGIN_SRC sh
$ rebar3 compile
===> Verifying dependencies...
===> Compiling mylib
#+END_SRC

If you look at your directory structure, you should now have something like this in your project:


#+NAME: lib_structure
#+BEGIN_SRC sh
mylib/
├─ _build/
│  └─ default/
│     └─ lib/
│        └─ mylib/
│           ├─ ebin/
│           │  ├─ mylib.app
│           │  └─ mylib.beam
│           ├─ include/
│           ├─ priv/
│           └─ src/
│              └─ ...
├─ .gitignore
├─ LICENSE
├─ README.md
├─ rebar.config
├─ rebar.lock
└─ src/
   ├─ mylib.app.src
   └─ mylib.erl
#+END_SRC

The =_build/= directory is the build tool's playground, where it can stash all the artifacts it needs. You should never have to touch what is in there by hand, but should feel free to blow it away when you want. This directory is nonetheless interesting because it shows how Rebar3 structures things.

Everything in =_build/= is split by [[https://www.rebar3.org/docs/configuration/profiles/][profile]], which lets Rebar3 build things differently (with different sets of dependencies and compiler options) whether they are built in the =default=, =test=, or =prod= profile—in fact, you can define as many profiles as you want, and compose them together. The Rebar3 documentation explains how this works.

Within each profile, the =lib/= directory contains all the OTP applications that your project may use, outside of the standard distribution's libraries. You can see our =mylib= library replicated right there, but its directory structure is a bit different from what's directly at the project root:

- compiled =.erl= files are moved to the =ebin/= directory and now have the =.beam= extension
- there is a =mylib.app= file created, whereas the source application had =mylib.app.src=
- two symlinks have been added to =include/= and =priv/=. These will refer to matching directories at the root of the project, if they exist. The =include/= directory is meant for [[http://erlang.org/doc/reference_manual/macros.html#file-inclusion][header files]] (=.hrl=), and the =priv/= directory for any file that must be copied over and made available in production
- All other files at the root of the project have been discarded

If we had any dependencies (see [[/docs/development/dependencies][The Dependencies chapter]]), they would also be placed in the =_build/<profile>/lib/= directory.

In general, you will want to ignore the =_build/= directory entirely and avoid tracking it in your source control: if you look at the =.gitignore= file, you will see that it automatically ignores =_build/= for you.

Rebar3 chooses a license for you by default (because you should always choose a license if you plan on doing open source work), going for the [[https://en.wikipedia.org/wiki/Apache_License#Version_2.0][Apache 2.0]] license that Erlang ships with. Feel free to replace it as required. Rebar3 also sets up a =README= file that you might want to fix up and update with all the relevant contents. Don't be a jerk, write documentation!

Then we get to two interesting files, =rebar.config= and =rebar.lock=. The lock file is used by Rebar3 to track which versions you were using for any dependency in the project, and should therefore be checked into source control. The [[/docs/development/dependencies][Dependencies chapter]] contains more details.

The =rebar.config= file is a complete declarative configuration file that exposes options for all the Erlang tools that Rebar3 integrates with. [[https://www.rebar3.org/docs/configuration][The official documentation]] explains all the values possible, but by default it is quite empty. In fact, if you only want default values with no dependencies, you can just delete the file. As long as your project is structured like an OTP application, Rebar3 will figure out what needs to be done.

Let's see what the standards are for that to happen.

*** What Makes a Lib an App

As with any other framework, there are some things you have to do to conform to its expectations. You've possibly guessed it, but the directory structure is one of the basic requirements of a framework like OTP. As long as your library has an =ebin/= directory once compiled with an =<appname>.app= file in it, the Erlang runtime system will be able to load your modules and run your code.

This basic requirement guides the project structure of the entire Erlang ecosystem. Let's look at what a built =.app= file looks like:

#+NAME: mylib.app
#+BEGIN_SRC erlang
$ cat _build/default/lib/mylib/ebin/mylib.app
{application, mylib, [
  {description, "Checking out OTP libs"},
  {vsn, "0.1.0"},     % version number (string)
  {registered, []},   % name of registered processes, if any
  {applications, [    % List of OTP application names on which
    kernel, stdlib    % yours depends at run-time. kernel and
  ]},                 % stdlib are ALWAYS needed
  {env, []},          % default configuration values ({Key, Val} pairs)
  {modules, [mylib]}, % list of all the modules in the application
  %% content below is optional, and for package publication only
  {licenses, ["Apache 2.0"]},
  {links, []}         % relevant URLs
]}.
#+END_SRC

This is essentially a metadata file that describes everything about the application. We've taken the time to annotate it for you, so check it out. A lot of the content in there is annoying to write by hand so if you look at the source file (=src/mylib.app.src=), you'll see that the fields are mostly pre-populated when you apply the Rebar3 template. You may also notice that =modules= is empty. That's on purpose: Rebar3 will populate the list for you when compiling your code.

By far, the most critical field to keep up to date in there is the =applications= tuple. It lets Erlang libraries know the order in which OTP applications must be started to work, and also allows build tools to build a dependency graph between all available OTP applications to know which to keep and which to remove from the distribution when building a release.

A more subtle thing to notice is that even if what we have here is a _library_, and it therefore has no processes to run, we still have the ability to define some configuration values (to be covered in the not yet written Configuration chapter), and dependencies must be respected. It is possible, for example, that our library is stateless, but uses a stateful HTTP client: the Erlang VM will then need to know when your code may or may not be safe to call.

For now, let's focus on what exactly is the difference between a stateless and a stateful application.

*** What Makes a Runnable App an App

To make a runnable application, we're going to use the "app" template in Rebar3, and see what are the differences with a stateless application.

So let's grab your command line tool and run the following:

#+NAME: rebar3_new_myapp
#+BEGIN_SRC sh
$ rebar3 new app myapp
===> Writing myapp/src/myapp_app.erl
===> Writing myapp/src/myapp_sup.erl
===> Writing myapp/src/myapp.app.src
===> Writing myapp/rebar.config
===> Writing myapp/.gitignore
===> Writing myapp/LICENSE
===> Writing myapp/README.md
$ cd myapp
#+END_SRC

If you're careful, you'll see that we now have two modules instead of =<appname>.erl=: we have =<appname>_app.erl= and =<appname>_sup.erl=. We'll study them real soon, but first, let's focus on the top-level metadata file for the application, the =myapp.app.src= file:

#+NAME: myapp.app.src
#+BEGIN_SRC erlang
$ cat src/myapp.app.src
{application, myapp,
 [{description, "An OTP application"},
  {vsn, "0.1.0"},
  {registered, []},
  {mod, {myapp_app, []}},               % this is new!
  {applications, [kernel, stdlib]},
  {env,[]},
  {modules, []},

  {licenses, ["Apache 2.0"]},
  {links, []}
 ]}.
#+END_SRC

The only new line here is the ={mod, {<appname>_app, []}}= tuple. This tuple specifies a special module that can be called (=<appname>_app=) with some specific arguments (=[]=). When called, it is expected that this module will return the _process identifier_ (the _pid_) of a [[/docs/development/supervision_trees][supervision tree]].

If you go visit the =myapp_app= module, you will see what these callbacks are:

#+NAME: myapp_app.erl
#+BEGIN_SRC erlang
%%%-------------------------------------------------------------------
%% @doc myapp public API
%% @end
%%%-------------------------------------------------------------------

-module(myapp_app).
-behaviour(application).
%% Application callbacks
-export([start/2, stop/1]).

%%====================================================================
%% API
%%====================================================================

start(_StartType, _StartArgs) ->
    myapp_sup:start_link().

stop(_State) ->
    ok.
#+END_SRC

The =start/2= callback is called when the application is booted by the Erlang runtime system, at which point all of its dependencies—as defined in the =applications= tuple in the .app file—have already been started. This is where you can do one-time bits of initialization. In the template application, the only thing done is starting the root supervisor for the application.

The =stop/1= callback is called _after_ the whole supervision tree has been taken down once someone, somewhere, has decided to shut down the OTP application.

But all in all, this little additional =mod= line in the app file and the presence of a supervision structure are what differentiates a runnable application from a library application.

#+attr_shortcode: tip
#+begin_admonition
#+begin_export html
If you do not want to care for a supervision tree and are only interested in getting a <code>main()</code> function to get going like you would in most other programming languages, <code>escript</code> might be a good option for you.

<code>escript</code> is a special C program that wraps the Erlang virtual machine. In wrapping it, it also introduces a small shim that retrofits the idea of a <code>main()</code> function to the release structure, by calling your code into the root Erlang process of the virtual machine.

The net result is that you can run interpreted code without having to bother about releases, OTP Applications, or supervision tree. You can read more about escripts in <a href='http://erlang.org/doc/man/escript.html'>the official Erlang documentation</a>. Rebar3 also <a href='https://www.rebar3.org/docs/commands#escriptize'>has a command to create complex escript bundles</a>.
#+end_export
#+end_admonition

You now understand most of the weird stuff about Erlang/OTP's project structure and everything that has to do with these mysterious "OTP Applications". Starting with next chapter, we'll start digging a bit in supervision trees, so that you know how to set things up in a stateful runnable application.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/otp_high_level">← Prev</a></div>
  <div><a href="/docs/development/supervision_trees">Next →</a></div>
</div>
#+END_EXPORT

** DONE Supervision Trees
CLOSED: [2019-09-18 Wed 09:02]
:PROPERTIES:
:EXPORT_FILE_NAME: supervision_trees
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

The biggest difference between Erlang and every other programming language out there is not in its concurrency, but rather in its fault tolerance. Almost everything in the language [[https://ferd.ca/the-zen-of-erlang.html][was designed for fault tolerance]], and supervisors are one of the core parts of this design. In this chapter, we'll cover the basics of supervision trees, what is in a supervisor, and how to structure supervision trees in your own system. When we're done, you'll be able to set up and manage most of the state your system will need.

*** Basics

Erlang is a kind of two-tiered language. At the lowest level, you have a functional subset. All you get is a bunch of data structures, functions, and pattern matching to modify and transform them. The data is immutable, local, and side-effects are rather limited if not unnecessary. At the higher level, you have the concurrent subset, where long-lived state is managed, and where it gets communicated from process to process.

The functional subset is rather straightforward and easy to learn with any of the resources mentioned in the book's [[/docs/introduction][Introduction]]: you get a data structure, you change it, and return a new one. All program transformations are handled as pipelines of functions applied to a piece of data to get a new one. It's a solid foundation on which to build.

The challenge of functional languages comes when you have to handle side-effects. How will you take something like a program's configuration and pass it through the entire stack? Where is it going to be stored and modified? How do you take something inherently mutable and stateful, like a network stream, and embed it in an immutable and stateless application?

In most programming languages, this is all done in a rather informal and ad-hoc matter. In object-oriented languages, for example, we would tend to pick where side-effects live according to the [[https://en.wikipedia.org/wiki/Domain-driven_design][boundaries of their domain]], possibly while trying to respect principles such as [[https://deviq.com/persistence-ignorance/][persistence ignorance]] or [[https://fideloper.com/hexagonal-architecture][hexagonal architecture]].

In the end what you may end up with is a kind of layered system where you hopefully have a bunch of pure domain-specific entities in the core, a bunch of interaction mechanisms on the outer edge, and a few layers in-between whose role is to coordinate and wrap all activities between entities:

#+CAPTION: Hexagonal architecture as frequently recommended in OO design
#+NAME: fig:hexagonal
[[./static/img/hexagonal.png]]

The kind of structure this gives is usually very explicit regarding the domain modelling, but rather ambiguous with regards to how interactions and side-effects should be structured. How should failures contacting an external service bubble up? How would the inability to save a transformation on a core domain entity impact the outer-edge interactions when it comes from some event-driven mechanism?

The domain modelling you'd do in object-oriented systems can still be done in Erlang. We'd stick all of that in the functional subset of the language, usually in a library application that regroups all the relevant modules that do the changes and transformation you need, or in some cases within specific modules of a runnable application.

The richness of failure and fault handling, however, will be explicitly encoded within a supervision structure. Because the stateful parts of the system are encoded using processes, the structure of dependencies and their respective instantialization is all laid out for everyone to see:

#+CAPTION: Sample supervision tree
#+NAME: fig:suptree
[[./static/img/suptree.png]]

In such a tree, all the processes are started depth-first, from left to right. This means that before the cache process and [[http://erlang.org/doc/man/ets.html][ETS table]] can be started, the database's supervision structure (and all its workers), must first be started. Similarly, before the HTTP server and its handler can be started, the whole business domain subtree will have been to be created, and since it depends on the cache table, we will similarly ensure that this one (and the database workers) will be ready to go.

This supervision structure defines how the release starts, but it will also define how it shuts down: depth-first, from right to left. And even more than this, each supervisor can set its own policy and tolerance for child failures. This means that they also define which kind of _partial failures_ are allowed or not in the system. Should the node still run if it can't talk to the database? Maybe it should, but it can't stay up if the cache becomes unavailable.

In a nutshell, where functional stuff can be used for the domain-specific handling, the flow of state, events, and interactions with the outside world is codified and explicit in stateful components, giving us a whole new way to handle errors and initialization.

We'll see how to do this in a moment, but first, let's review how supervisors work.

*** What's in a Supervisor

Supervisors are one of the simplest behaviours in all of Erlang/OTP on the surface. They take a single =init/1= callback, and that's about it. The callback is used to define the children of each supervisor, and to configure some basic policies in how they react to failures of many kind.

There are 3 types of policies to handle:

- The supervisor type
- The restart policy of a child
- The frequency of failures to be accepted

Each of them is simple enough in isolation, but choosing good ones can become a bit tricky. Let's start with the supervisor type:

#+CAPTION: The three supervisor types
#+NAME: fig:suptypes
[[./static/img/suptypes.png]]

There are three types of strategies:

1. =one_for_one=, which states that each child is independent from the other children. If one dies, the others don't need any replacing or modification.
  - =simple_one_for_one= is a specialization of =one_for_one= in the case where all the children are of the same type (say a worker pool), which is faster and lighter weight
2. =rest_for_one= encodes a linear dependency between the supervisor's children. If one of them dies, all those started _after_ it must be restarted, but not those started before. In a scenario where process =C= depends on process =B=, and =B= depends on process =A=, the =rest_for_one= strategy allows efficient encoding of their dependency structure.
3. =one_for_all= is a strategy by which if any child dies, then all of them must also be restarted. This is the type of supervisor you'd want to use when there is a strong interdependency between all children; if any one of them restarts, there's no easy way for the other ones to recuperate, and so they should also be restarted.

These strategies are effectively all about how errors should propagate between children of the supervisor. The next bit to consider is how each of the processes that fail should, on their own, be handled by the supervisor after the signals have been propagated.

| Restart Policy | On Normal Exit | On Abnormal Exit |
|----------------+----------------+------------------|
| permanent      | restart        | restart          |
| transient      | stay dead      | restart          |
| temporary      | stay dead      | stay dead        |

This lets you say that some processes are expected to never stop (permanent), some are expected to stop (transient), and some are expected to fail (temporary).

The final bit of configuration we can set is the frequency at which restarts are allowed. This is done through two parameters, =intensity= and =period=, which respectively stand for how many crashes have been spotted and during how many seconds they took place. We can then specify that a supervisor should tolerate only one crash per hour, or a dozen per second if we want it so.

This is how you declare a supervisor:

#+NAME: myapp_sup
#+BEGIN_SRC erlang
-module(myapp_sup).
-behaviour(supervisor).

%% API
-export([start_link/0]).

%% Supervisor callbacks
-export([init/1]).

-define(SERVER, ?MODULE).

start_link() ->
    supervisor:start_link({local, ?SERVER}, ?MODULE, []).

init([]) ->
    {ok, {{one_for_all, 1, 10}, [ % one failure per 10 seconds
        #{id => internal_name,                % mandatory
          start => {mymod, function, [args]}. % mandatory
          restart => permanent,               % optional
          shutdown => 5000,                   % optional
          type => worker,                     % optional
          modules => [mymod]}                 % optional
    ]}}.
#+END_SRC

You can define as many children as you want at once in there (except for =simple_one_for_one=, which [[http://erlang.org/doc/man/supervisor.html#Module:init-1][expects a description template]]). Other arguments you can specify for a child include =shutdown=, which gives a number of milliseconds to wait for the proper termination of the child (or =brutal_kill= to kill it right away), and a definition on whether a process is a =worker= or a =supervisor=. This =type= field, along with =modules=, are only used when doing live code upgrades with releases, and the latter can generally be ignored and be left to its default value.

That's all that's needed. Let's see how we can put that in practice.

*** Trying Your Own Supervisor

Let's try to add a supervisor with a worker to an application, running what is essentially the heaviest _Hello World_ on the planet. We'll build a whole release for that:

#+NAME: hello_world
#+BEGIN_SRC sh
$ rebar3 new release hello_world
===> Writing hello_world/apps/hello_world/src/hello_world_app.erl
===> Writing hello_world/apps/hello_world/src/hello_world_sup.erl
===> Writing hello_world/apps/hello_world/src/hello_world.app.src
===> Writing hello_world/rebar.config
===> Writing hello_world/config/sys.config
===> Writing hello_world/config/vm.args
===> Writing hello_world/.gitignore
===> Writing hello_world/LICENSE
===> Writing hello_world/README.md
$ cd hello_world
#+END_SRC

You should recognize the release structure, where all OTP applications are in the =apps/= subdirectory.

Open up =hello_world_sup= module, and make sure it looks like this:


#+NAME: hello_world_sup.erl
#+BEGIN_SRC erlang
%%%----------------------------------------------------------
%% @doc hello_world top level supervisor.
%% @end
%%%----------------------------------------------------------

-module(hello_world_sup).
-behaviour(supervisor).

-export([start_link/0]).
-export([init/1]).

-define(SERVER, ?MODULE).

start_link() ->
    supervisor:start_link({local, ?SERVER}, ?MODULE, []).

init([]) ->
    SupFlags = #{strategy => one_for_all,
                 intensity => 0,
                 period => 1},
    ChildSpecs = [
        #{id => main,
          start => {hello_world_serv, start_link, []}}
    ],
    {ok, {SupFlags, ChildSpecs}}.
#+END_SRC

This establishes a single child, which will be in the module =hello_world_serv=. This will be a straightforward =gen_server= that does nothing, using only its =init= function:

#+NAME: hello_world_serv
#+BEGIN_SRC erlang
-module(hello_world_serv).
-export([start_link/0, init/1]).

start_link() ->
    gen_server:start_link(?MODULE, [], []).

init([]) ->
    %% Here we ignore what OTP asks of us and just do
    %% however we please.
    io:format("Hello, heavy world!~n"),
    halt(0). % shut down the VM without error
#+END_SRC

This file just starts an OTP process, outputs =Hello, heavy world!=, and then shuts the whole virtual machine down.

Let's build a release and see what happens:

#+BEGIN_SRC sh
$ rebar3 release
===> Verifying dependencies...
===> Compiling hello_world
===> Starting relx build process ...
===> Resolving OTP Applications from directories:
          /Users/ferd/code/self/adoptingerlang/hello_world/_build/default/lib
          /Users/ferd/code/self/adoptingerlang/hello_world/apps
          /Users/ferd/bin/erls/21.1.3/lib
===> Resolved hello_world-0.1.0
===> Dev mode enabled, release will be symlinked
===> release successfully created!
#+END_SRC

And now we can start it. We'll use the =foreground= argument, which means we'll boot the release to see all of its output, but do so in a non-interactive mode (without a shell):

#+BEGIN_SRC sh
$ ./_build/default/rel/hello_world/bin/hello_world foreground
<debug output provided by wrappers bundled with Rebar3>
Hello, heavy world!
#+END_SRC

What happens here is that the tools generate a script at =/_build/default/rel/hello_world/bin/hello_world=. This script puts a bunch of stuff together to make sure your release boots with all the right configuration and environment values.

Everything starts with the virtual machine starting, and eventually spawning the root process in Erlang itself. The =kernel= OTP application boots, and then sees in the config data that it has to start the =hello_world= application.

This is done by calling =hello_world_app:start/2=, which in turn calls =hello_world_sup=, which starts the =hello_world_serv= process, which outputs text and then makes a hard call to the VM telling it to shut down.

And that's what we just did. Supervisors only start and restart processes; they're simple, but their power comes from how they can be composed and used to structure a system.

*** Structuring Supervision Trees

The most complex part of supervisors isn't their declaration, it's their composition. They're a simple tool up to a complex task. In this section, we'll cover why supervision trees can work, and how to best structure them to gain the most out of them.

**** What Makes Supervisors Work

Everyone has heard "have you tried turning it off and on again?" as a general bug-fixing approach. It works surprisingly often, and Erlang supervision trees operate under that principle. Of course, restarting can't solve all bugs, but it can cover a lot.

The reason restarting works is due to the nature of bugs encountered in production systems. To discuss this, we have to refer to the terms _Bohrbug_ and _Heisenbug_ coined by [[https://www.hpl.hp.com/techreports/tandem/TR-85.7.pdf][Jim Gray in 1985]]. Basically, a bohrbug is a bug that is solid, observable, and easily repeatable. They tend to be fairly simple to reason about. Heisenbugs by contrast, have unreliable behaviour that manifests itself under certain conditions, and which may possibly be hidden by the simple act of trying to observe them. For example, concurrency bugs are notorious for disappearing when using a debugger that may force every operation in the system to be serialised.

Heisenbugs are these nasty bugs that happen once in a thousand, million, billion, or trillion times. You know someone's been working on figuring one out for a while once you see them print out pages of code and go to town on them with a bunch of markers.

With these terms defined, let's look at how easy it should be to find bugs in production:

| Type of Feature | Repeatable              | Transient |
|-----------------+-------------------------+-----------|
| Core            | Easy                    | Hard      |
| Secondary       | Easy (often overlooked) | Hard      |

If you have bohrbugs in your system's core features, they should usually be very easy to find before reaching production. By virtue of being repeatable, and often on a critical path, you should encounter them sooner or later, and fix them before shipping.

Those that happen in secondary, less used features, are far more of a hit and miss affair. Everyone admits that fixing all bugs in a piece of software is an uphill battle with diminishing returns; weeding out all the little imperfections takes proportionally more time as you go on. Usually, these secondary features will tend to gather less attention because either fewer customers will use them, or their impact on their satisfaction will be less important. Or maybe they're just scheduled later and slipping timelines end up deprioritising their work.

Heisenbugs are pretty much impossible to find in development. Fancy techniques like formal proofs, model checking, exhaustive testing or property-based testing may increase the likelihood of uncovering some or all of them (depending on the means used), but frankly, few of us use any of these unless the task at hand is extremely critical. A once in a billion issue requires quite a lot of tests and validation to uncover, and chances are that if you've seen it, you won't be able to generate it again just by luck.

So let's take a look at the previous table of bug types, but let's focus on how often they _will_ happen in production:

| Type of Feature | Repeatable | Transient |
|--------+------------+-----------|
| Core | Should Never | All the time |
| Secondary | Pretty often | All the time |


First of all, easy repeatable bugs in core features should just not make it to production. If they do, you have essentially shipped a broken product and no amount of restarting or support will help your users. Those require modifying the code, and may be the result of some deeply entrenched issues within the organisation that produced them.

Repeatable bugs in side-features will pretty often make it to production. This is often a result of not taking or having the time to test them properly, but there's also a strong possibility that secondary features often get left behind when it comes to partial refactorings, or that the people behind their design do not fully consider whether the feature will coherently fit with the rest of the system.

On the other hand, transient bugs will show up all the damn time. Jim Gray, who coined these terms, reported that on 132 bugs noted at a given set of customer sites, only one was a Bohrbug. 131/132 of errors encountered in production tended to be heisenbugs. They're hard to catch, and if they're truly statistical bugs that may show once in a million times, it just takes some load on your system to trigger them all the time; a once in a billion bug will show up every 3 hours in a system doing 100,000 requests a second, and a once in a million bug could similarly show up once every 10 seconds on such a system, but their occurrence would still be rare in tests.

That's a lot of bugs, and a lot of failures if they are not handled properly. Let's rework the table, but now we're considering whether restarts can handle these faults:

| Type of Feature | Repeatable | Transient |
|--------+------------+-----------|
| Core | No | Yes |
| Secondary | It depends | Yes |

For repeatable bugs on core features, restarting is useless. For repeatable bugs in less frequently used code paths, it depends; if the feature is a thing very important to a very small amount of users, restarting won't do much. If it's a side-feature used by everyone, but to a degree they don't care much about, then restarting or ignoring the failure altogether can work well.

For transient bugs though, restarting is extremely effective, and they tend to be the majority of bugs you'll meet live. Because they are hard to reproduce, that their showing up is often dependent on very specific circumstances or interleavings of bits of state in the system, and that their appearance tends to be in a very small fraction of all operations, restarting tends to make them disappear altogether.

Supervisors allows a part of your system hit by such a bug to roll back to a known stable state. Once you've rolled back to that state, trying again is unlikely to hit the same weird context that caused the first bug. And just like that, what could have been a catastrophe has become little more than a hiccup for the system, something users quickly learn to live with.

**** It's About the Guarantees

One very important part of Erlang supervisors and their supervision trees is that their start phase is synchronous. Each OTP Process started has a period during which it can do its own thing, preventing the entire boot sequence of its siblings and cousins to come. If the process dies there, it's retried again, and again, until it works, or fails too often.

That's where people make a very common mistake. There isn't a backoff or cooldown period before a supervisor restarts a crashed child. When people write a network-based application and try to set up a connection in this initialization phase, and that the remote service is down, the application fails to boot after too many fruitless restarts. Then the system may shut down.

Many Erlang developers end up arguing in favor of a supervisor that has a cooldown period. This sentiment is wrong for one simple reason: it's all about the guarantees.

Restarting a process is about bringing it back to a stable, known state. From there, things can be retried. When the initialization isn't stable, supervision is worth very little. An initialized process should be stable no matter what happens. That way, when its siblings and cousins get started later on, they can be booted fully knowing that the rest of the system that came up before them is healthy.

If you don't provide that stable state, or if you were to start the entire system asynchronously, you get very little benefit from this structure that a =try ... catch= in a loop wouldn't provide.

Supervised processes provide guarantees in their initialization phase, not a best effort. This means that when you're writing a client for a database or service, you shouldn't need a connection to be established as part of the initialization phase unless you're ready to say it will always be available no matter what happens.

You could force a connection during initialization if you know the database is on the same host and should be booted before your Erlang system, for example. Then a restart should work. In case of something incomprehensible and unexpected that breaks these guarantees, the node will end up crashing, which is desirable: a pre-condition to starting your system hasn't been met. It's a system-wide assertion that failed.

If, on the other hand, your database is on a remote host, you should expect the connection to fail. In this case, the only guarantee you can make in the client process is that your client will be able to handle requests, but not that it will communicate to the database. It could return ={error, not_connected}= on all calls during a net split, for example.

The reconnection to the database can then be done using whatever cooldown or backoff strategy you believe is optimal, without impacting the stability of the system. It can be attempted in the initialization phase as an optimization, but the process should be able to reconnect later on if anything ever disconnects.

If you expect failure to happen on an external service, do not make its presence a guarantee of your system. We're dealing with the real world here, and failure of external dependencies is always an option.

Of course, the libraries and processes that call the client will then error out if they didn't expect to work without a database. That's an entirely different issue in a different problem space, but not one that is always impossible to work around. For example, let's pretend the client is to a statistics service for Ops people—then the code that calls that client could very well ignore the errors without adverse effects to the system as a whole. In other cases, an event queue could be added in front of the client to avoid losing state when things go sour.

The difference in both initialization and supervision approaches is that the client's callers make the decision about how much failure they can tolerate, not the client itself. That's a very important distinction when it comes to designing fault-tolerant systems. Yes, supervisors are about restarts, but they should be about restarts to a stable known state.

**** Growing Trees

When you structure Erlang programs, everything you feel is fragile and should be allowed to fail has to move deeper into the hierarchy, and what is stable and critical needs to be reliable is higher up. Supervision structures allow the encoding of partial failures and fault propagation, so we must think properly about all of these things. Let's take a look at our sample supervision tree again:

#+NAME: fig:suptree_repeat
[[./static/img/suptree.png]]

If a worker in the =DB= subtree dies, and =DB= is a supervisor with a =one_for_one= strategy, then we are encoding that each worker is allowed to fail independently from each other. On the other hand, if =event_sup= has a =rest_for_one= strategy, we are encoding in our system that the worker handling subscriptions _must_ restart if the event listener dies; we say that there is a direct dependency.

Implicitly, there is also a statement that the event handling subtree is not directly impacted by the database, as long as it has managed to successfully boot at some point.

This supervision tree can be read like a schedule and a map to the system faults. The HTTP server won't start unless the domain-specific workers are available, and the HTTP handler failing will not do anything that might compromise the database cache. Of course, if the HTTP handler relies on the domain worker, which relies on the cache's ETS table and that table vanishes, then all these processes may die together.

What's truly interesting here is that we can at a glance know how even unknown failures may impact our system. I don't need to know _why_ a database worker may fail, whether it's due to a disconnection, a dying database, or a bug in the protocol implementation; I know that no matter what, this should leave the cache in place, and possibly let me do stale reads until the database sub-tree becomes available again.

This is where a child's restart policy, combined with the supervisor's accepted failure frequency comes in play. If all the workers and their supervisor are marked =permanent=, then there is a possibility that frequent crashes will take down the whole node. However, you can do a  special little trick:

#+CAPTION: The manager worker pattern
#+NAME: fig:manager
[[./static/img/manager.png]]

1. mark the workers as =permanent= or =transient=, so that if they fail they get restarted
2. mark the workers' direct supervisor (in the square) as =temporary=, so that if it fails, it gives up and does not get restarted
3. add a new supervisor above it (this one can use any policy you want), but make it =one_for_one=.
4. add a new process under the new supervisor (a sibling of the =temporary= supervisor). Have this process _linked_ to the old supervisor. This is the _manager_.

The temporary supervisor can be used to tolerate an acceptable restart frequency. It could say that it's normal for a worker to die every two minutes, for example. However, if we exceed this threshold, something bad is happening, and we want to stop retrying without risking to destabilize the node. The supervisor shuts down, and since it's temporary, the super-supervisor (above the square) will just stay there doing nothing.

The manager can then look at whether the =sup= supervisor is alive or dead, and apply any policy it want when it comes to restarting it: use an exponential backoff, wait until a [[https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern][circuit breaker]] untrips, wait for an external service registry to say the dependent service is healthy again, or ask for user input on what to do. The manager can then ask its own parent supervisor to restart the temporary supervisor, which will then restart its own workers.

This is one of the few _design patterns_ in Erlang. Because smart systems make stupid mistakes, we want to keep the supervisors as simple and predictable as they can be. The manager is us grafting a brain onto the supervisor, and making fancier decisions. This lets us separate policies for transient errors from policies for more major or persistent faults that can't just be dealt with through restarting.

#+attr_shortcode: note
#+begin_admonition
#+begin_export html
This pattern can be adapted however you want. The authors have, for example, used these two other variants for restart management:

<ul>
<li> Multiple supervision trees are nested into each other, each representing a worker pool for a physical device. Each physical device is allowed to fail, go offline, or lose power. The managing process would routinely compare the running subtrees to a configuration service that was off-site. It would then start all the subtrees that were missing, and shut down all those that no longer existed. This let the manager handle run-time configuration synchronisation while also handling restarts for tricky hardware failure scenarios.</li>
<li>The manager does nothing. However, the Erlang release shipped with a script that could be called by an operator. The script sent a message to the manager, asking it to restart the missing subtree. This variant was used because the subtree died only in the rarest of occasions (a whole region's storage going down) without wanting to kill the rest of the system, but upon recovery, operators could re-enable traffic that way.</li>
</ul>

While it would be difficult to bake that kind of functionality in a generic
supervisor, a manager can easily provide the flexibility required for
tailor-made solutions of this kind.
#+end_export
#+end_admonition

One exercise we would recommend you do is to take your system, and then draw its supervision tree on a white board. Go through all the workers and supervisors, and ask the following questions:

- Is it okay if this dies?
- Should other processes be taken down with this one?
- Does this process depend on anything else that will be weird once it restarts?
- How many crashes are too many for this supervisor?
- If this part of the system is flat out broken, should the rest of it keep working or should we give up?

Discuss these with your team. Shuffle supervisors around, adjust the strategies and policies. Add or remove supervision layers, and in some cases, add managers.

Repeat this exercise from time to time, and eventually do some [[https://principlesofchaos.org/][chaos engineering]] by killing Erlang processes on a running node to see if it behaves and recovers the way you think it should (you can of course do chaos engineering on entire nodes, as is more common). What you'll end up with is a fault tolerant piece of code. You will also build a team with a strong understanding of the failure semantics that are baked in their system, with a good mental model of how things should break down when they inevitably do. This is worth its weight in gold.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/otp_applications/">← Prev</a></div>
  <div><a href="/docs/development/dependencies/">Next →</a></div>
</div>
#+END_EXPORT

** DONE Dependencies
CLOSED: [2019-10-26 Sat 15:03]
:PROPERTIES:
:EXPORT_FILE_NAME: dependencies
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

There have been large shifts in how people approached programming over the last 20 years. Whereas material of yesterday focused a lot on writing reusable and extendable components within each project, the current trend is to make small, isolated projects that each individually ends up being easy to throw away and replace. Current microservice implementations and Javascript dependency trees may lead us to believe no project is too small to be discardable.

Ironically, the shift towards strong isolation and clearly-defined boundaries—whether they be interfaces, network APIs, or protocols—that was necessary for small single-purpose components has given rise to what is possibly the greatest amount of code reuse we've ever seen. Reuse materialized from shared needs across various people and projects, not from the mythical ability of providing a perfectly extendable class hierarchy. There is so much reuse today that we have reached the point where folks are starting to ask if maybe, just maybe, we aren't reusing more code than we should: each library comes with risks and liabilities, and we're exposing ourselves to a lot of it for the sake of moving fast.

Erlang, for its own part, has always lived and breathed strong isolation with clearly defined message-passing protocols wrapped in functional interfaces. Its small community size of, generally, experienced developers probably made it lag behind other communities when it comes to libraries and packaging. Despite prior efforts on package managers like [[https://web.archive.org/web/20111026040038/http://cean.process-one.net/][CEAN]] by ProcessOne and Faxien, both of which installed OTP applications globally, it was not until 2009 that it really became easy to install libraries from other people. This was made possible through the first version of =rebar=, which favored per-project dependency installation, and was the first to come as an [[http://erlang.org/doc/man/escript.html][escript]], a portable script that required no installation by the developer.

Erlang libraries proliferated all over GitHub and similar hosted version control services haphazardly, and at some point, it became possible to find over twelve different versions of the same PostgreSQL driver, which all had the same name and similar versions, but all did a few things differently.

It took a decent push in the back from Elixir, which was bringing in newer perspectives and tools such as =mix= and hex.pm, for the Erlang community to collect themselves and get to a more understandable ecosystem. In modern days, Erlang's community is still small, but it has adopted better practices, and now shares its package and library infrastructure with Elixir and half a dozen smaller languages on the same virtual machine.

In this chapter, we'll see how modern library usage and community integration is done within the Erlang world by answering questions like:

- What is a library?
- How can I use one as a dependency?
- What is a dependency's lifecycle?
- How can I use Elixir dependencies?
- What do I do if my work uses a monorepo?

*** Using Open Source Libraries

Erlang's open source dependencies are just OTP applications, like every other library in a release. As such, all that's required to use an open source library is to have that OTP application visible to the Erlang toolchain. This is conceptually simple, but every language and community takes slightly different approaches here. We have choices to make between installing libraries globally on a computer, within shared environments, or locally within each project. Then, there are norms regarding versioning and publishing, which also must be adhered to. This section will show how that's all done, but first we'll take a detour through the expectations of Rebar3 regarding your project's lifecycle.

**** Rebar3 Expectations

Some of the tricky decisions around open source work have been enshrined in Rebar3, and it's often easier to go with the flow than fight it, especially when you're first starting. Rebar3 has initially been built in a mid-size corporation writing services in a semi-private mode: open-source dependencies are both used and published, but some of the code is to remain private forever. Multiple unrelated services are developed at the same time, and not all of them will necessarily be on the same Erlang version with the same libraries. Some programs will only be deployed through rolling restarts (as is usual everywhere in the cloud) but some systems will absolutely require hot code loading. Rebar3 was also developed at a time where versioning practices were downright messy in the community: many libraries had different versions in their =.app= files than their git tag on GitHub, and documentation yet again mentioned another version. Back then, roughly 4 out of 5 libraries on hex.pm had not even reached version 1.0.0 either.

As such, Rebar3 has the following properties:

- Rebar3 is a declarative build tool. You provide a configuration, and it executes code to satisfy that configuration. If you want to run custom scripts and extend the build, it specifically provides [[https://www.rebar3.org/docs/configuration/configuration/#hooks][hooks]] or [[https://www.rebar3.org/docs/configuration/plugins/][plugins]] interfaces to do so. There are also ways to gain [[https://www.rebar3.org/docs/configuration/config_script/][Dynamic Configuration]] to more flexibly populate config files.
- As part of that declarative approach, commands in Rebar3 are defined with dependency sequences. For example, the =rebar3 compile= task relies on =rebar3 get-deps= (or rather, a private form of it that locks dependencies), and will run it for you. The command =rebar3 tar= will implicitly call a sequence that includes =get-deps -> compile -> release -> tar= for you. Rebar3 therefore knows that to compile your project, its dependencies are needed.
- Rebar3 defines its own working area within the =_build= directory; it expects to have control over what goes in there, and for you not to track that directory in source control nor rely on its internal structure. When it creates an artifact the user will want to use directly, it outputs the path to that artifact in the terminal. For example =rebar3 escriptize= will print the path to the generated escript in =_build/default/bin/= after each run, =rebar3 ct= prints the path to the Common Test HTML output if tests fail and =rebar3 tar= prints the path to the release tarball.
- All projects are built locally within their own directory based on the Erlang runtime currently loaded in your environment, i.e. the =erl= found in =$PATH=.
- All dependencies are fetched to the project's =_build= directory.
- All dependencies can define their own dependencies, and Rebar3 will recognize this and fetch them to the root project's =_build= directory as well.
- Since version numbers are unreliable (even with semantic versioning), we consider versions as information for humans, not build tools. In case of libraries that end up declared multiple times, the one closest to the project root is chosen, while [[https://www.rebar3.org/docs/configuration/dependencies][warnings]] are emitted during the first build (it knows it is the "first" because there is no lock for the dependency) to let the user know versions not being used, with the assumption that libraries declared closer to the root project are used more thoroughly.
- Circular dependencies are forbidden.
- Rebar3 supports composable per-project [[https://www.rebar3.org/docs/configuration/profiles][_profiles_]] that let you segment or combine configuration settings, such as dependencies that are to only be used for tests or only in a specific environment (say on a specific target OS). There are no restrictions on the number or names of profiles, but there are four profiles, =default=, =test=, =docs=, and =prod=, which are used by Rebar3 automatically for specific tasks.
- Builds are meant to be repeatable. Dependencies are locked, and only upgraded when specifically asked to. The version in =rebar.lock= takes precedence of whatever version is declared in =rebar.config= until =rebar3 upgrade= is called to update the lock file.
- Dependencies are always built with the =prod= profile applied. Rebar3 will always check that they match the lock file and that build artifacts are present, but will not detect code changes done by hand within a dependency.
- Rebar3 assumes that you'll sometimes need to tweak the configuration of libraries you do not control and supports [[https://www.rebar3.org/docs/configuration#section-overrides][overrides]] for this purpose.
- The tool assumes you're developing with a source control mechanism such as =git= or =hg= (mercurial), which means that switching branches may switch dependency versions in the lock file. Since Rebar3 verifies dependencies before each build, it will automatically re-fetch libraries to get the locked version for the current branch if there is a change when switching branches.
- To ease contribution and publishing, Rebar3 does not natively support ways to use relative paths to declare libraries, since this could make builds brittle, non-repeatable, and non-portable when publishing code.
- Knowing relative paths are very common when making changes within the dependencies of a project, [[https://www.rebar3.org/docs/configuration/dependencies/#checkout-dependencies][=_checkouts=]] allow for an automatic way to temporary override of a dependency with a local copy. For other use cases [[https://www.rebar3.org/docs/configuration/plugins/#recommended-plugins][plugins]] allow the creation of [[https://www.rebar3.org/docs/extending/custom_dep_resources/][custom resource types]].
- Rebar3 is not an installer or runner of end-user applications and does not support anything related to that; it aims to generate build artifacts that you can then install through the correct dedicated channels. Rebar3 does not expect or need to ever be on a production device or server.
- Rebar3 is not a sandbox. While it will make sure all dependencies you download match the right signatures and provides repeatable builds, it cannot guarantee that script files invoked during builds or [[https://stackoverflow.com/questions/2416192/is-there-a-good-complete-tutorial-on-erlang-parse-transforms-available][parse transforms]] run during compilation are ever going to be safe, and has no intention of taking on that responsibility. Plugins are also not locked automatically, and it is up to library authors to pin versions in cases where they impact compilation.

That's a lot of information, but we find that it's useful to know about it before getting in too deep with dependencies. If you operate under assumptions that Rebar3 works like Javascript's =npm=, Elixir's =mix=, Go's toolchain, or even the original =rebar=, you might find some behaviours confusing. With that being said, let's use dependencies.

**** Declaring Dependencies

Since dependencies are all project-local, they have to be declared in the =rebar.config= file of your project. This will let Rebar3 know it needs to fetch them, build them, and make them available to your project. All dependencies must be individual OTP applications, so that they can be versioned and handled independently from each other.

The following formats are valid:

#+NAME: dep-types
#+BEGIN_SRC erlang
{deps, [
    %% git dependencies
    {AppName, {git, "https://host.tld/path/to/app", {tag, "1.2.0"}}},
    {AppName, {git, "https://host.tld/path/to/app", {branch, "master"}}},
    {AppName, {git, "https://host.tld/path/to/app", {ref, "aed12f..."}}},
    %% similar format for mercurial deps
    {AppName, {hg, "https://host.tld/path/to/app", {RefType, Ref}}}
    %% hex packages
    AppName, % latest known version (as per `rebar3 update`)
    {AppName, "1.2.0"},
    {AppName, "~> 1.2.0"}, % latest version at 1.2.0 or above, and below 1.3.0
    {AppName, "1.2.0", {pkg, PkgName}}, % when application AppName is published with package name PkgName
]}.
#+END_SRC

Additionally, plugins allow to define [[https://www.rebar3.org/docs/extending/custom_dep_resources/][custom resource definitions]] that let you add new types of dependencies to projects.

Let's see how this would work with a project we've created specifically for this book, [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]]. Open up the =rebar.config= file and you'll see:

#+NAME: service_discovery.deps
#+BEGIN_SRC erlang
...

{deps, [
    {erldns,
     {git, "https://github.com/tsloughter/erldns.git",
     {branch, "revamp"}}},
    {dns,
     {git, "https://github.com/tsloughter/dns_erlang.git",
     {branch, "hex-deps"}}},

    recon,
    eql,
    jsx,
    {uuid, "1.7.5", {pkg, uuid_erl}},
    {elli, "~> 3.2.0"},
    {grpcbox, "~> 0.11.0"},
    {pgo,
     {git, "https://github.com/tsloughter/pgo.git",
     {branch, "master"}}}
]}.

...
#+END_SRC

Both git and hex dependencies can work together for most projects. The only exception is hex packages, which can only depend on other hex packages. Let's compile the whole project and step through what is going on:

#+NAME: service_discovery.compile
#+BEGIN_SRC sh
$ rebar3 compile
===> Fetching covertool v2.0.1
===> Downloaded package, caching at /Users/ferd/.cache/rebar3/hex/hexpm/packages/covertool-2.0.1.tar
===> Compiling covertool
...
===> Verifying dependencies...
===> Fetching dns (from {git,"https://github.com/tsloughter/dns_erlang.git",
               {ref,"abc562548e8a232289eec06cf96ce7066261cc9d"}})
===> Fetching provider_asn1 v0.2.3
===> Downloaded package, caching at /Users/ferd/.cache/rebar3/hex/hexpm/packages/provider_asn1-0.2.3.tar
===> Compiling provider_asn1
===> Fetching elli v3.2.0
...
===> Fetching rfc3339 v0.9.0
===> Version cached at /Users/ferd/.cache/rebar3/hex/hexpm/packages/rfc3339-0.9.0.tar is up to date, reusing it
===> Compiling quickrand
===> Compiling uuid
===> Compiling recon
...
===> Compiling service_discovery_storage
===> Compiling service_discovery
===> Compiling service_discovery_http
===> Compiling service_discovery_grpc
===> Compiling service_discovery_postgres
#+END_SRC

Running this build, you can see multiple things going on:

1. Plugins (such as =covertool=) are fetched and compiled before anything else goes on
2. Actual dependencies of the project (such as =dns= and =elli=) are fetched
3. Dependencies are compiled (=quickrand= and others)
4. The main applications are compiled

If you were to try again and run things from scratch while deleting the =rebar.lock= file, things would be a bit different. You might see something looking like this as part of the output:

#+NAME: service_discovery.resolve
#+BEGIN_SRC
...
===> Fetching dns (from {git,"https://github.com/tsloughter/dns_erlang.git",
               {branch,"hex-deps"}})
...
===> Skipping dns (from {git,"git://github.com/dnsimple/dns_erlang.git",
               {ref,"b9ee5b306acca34b3d866d183c475d5f12b313a5"}}) as an app of the same name has already been fetched
...
===> Skipping jsx v2.9.0 as an app of the same name has already been fetched
===> Skipping recon v2.4.0 as an app of the same name has already been fetched
...
#+END_SRC

Those are little notices and warnings that happen during dependency resolution, which is not necessary when a =rebar.lock= file is available. They inform the library maintainer that a conflict was detected, and a certain version of a library was skipped. To finish an audit of your build, you can inspect the final dependency resolving by calling =rebar3 tree=:

#+NAME: service_discovery.deps-tree
#+BEGIN_SRC sh
$ rebar3 tree
===> Verifying dependencies...
├─ service_discovery─e4b7061 (project app)
├─ service_discovery_grpc─e4b7061 (project app)
├─ service_discovery_http─e4b7061 (project app)
├─ service_discovery_postgres─e4b7061 (project app)
└─ service_discovery_storage─e4b7061 (project app)
   ├─ dns─0.1.0 (git repo)
   │  └─ base32─0.1.0 (hex package)
   ├─ elli─3.2.0 (hex package)
   ├─ eql─0.2.0 (hex package)
   ├─ erldns─1.0.0 (git repo)
   │  ├─ iso8601─1.3.1 (hex package)
   │  ├─ opencensus─0.9.2 (hex package)
   │  │  ├─ counters─0.2.1 (hex package)
   │  │  └─ wts─0.3.0 (hex package)
   │  │     └─ rfc3339─0.9.0 (hex package)
   │  └─ telemetry─0.4.0 (hex package)
   ├─ grpcbox─0.11.0 (hex package)
   │  ├─ acceptor_pool─1.0.0 (hex package)
   │  ├─ chatterbox─0.9.1 (hex package)
   │  │  └─ hpack─0.2.3 (hex package)
   │  ├─ ctx─0.5.0 (hex package)
   │  └─ gproc─0.8.0 (hex package)
   ├─ jsx─2.10.0 (hex package)
   ├─ pgo─0.8.0+build.91.refaf02392 (git repo)
   │  ├─ backoff─1.1.6 (hex package)
   │  └─ pg_types─0.0.0+build.24.ref32ed140 (git repo)
   ├─ recon─2.4.0 (hex package)
   └─ uuid─1.7.5 (hex package)
      └─ quickrand─1.7.5 (hex package)
#+END_SRC

This listing starts with the =Verifying dependencies ...= line, which is Rebar3 validating that all dependencies are resolved before printing the tree. Following it are all the top-level applications (those we write in the repository), and the dependencies we just fetched are all below them. You can see the entire resolution tree that way, find which versions have been fetched and which application brought them in. This can prove useful to understand why a version was selected when a transitive dependency is included by two or more applications.

If you take a look within =_build/default/lib=, you will see all these applications within their own directory:

#+NAME: service_discovery.deps-ls
#+BEGIN_SRC sh
$ ls _build/default/lib
acceptor_pool  gproc       rfc3339
backoff        grpcbox     service_discovery
base32         hpack       service_discovery_grpc
chatterbox     iso8601     service_discovery_http
counters       jsx         service_discovery_postgres
ctx            opencensus  service_discovery_storage
dns            pgo         telemetry
elli           pg_types    uuid
eql            quickrand   wts
erldns         recon
#+END_SRC

Each of these is an OTP application with similar directory structures. This layout is rather similar to the project structure described for releases in [[OTP at a High Level]], but this is still just a staging area.

**** Building a Project with Dependencies

Building the OTP applications in a project requires more than just fetching its dependencies and compiling them. As mentioned in [[What Makes a Lib an App]], the Erlang run-time system expects to find run-time definitions of dependencies within the =.app= file. Not putting them there tells Rebar3 they are build-time dependencies, not runtime ones. This means they will not be included in some tasks and environments: releases will ignore them, and =rebar3 dialyzer= will avoid including them in its analysis, for example.

Open up =apps/service_discovery/src/service_discovery.app.src= and look at the values in the =applications= tuple:

#+NAME: service_discovery.app-src
#+BEGIN_SRC erlang
{application, service_discovery,
 [{description, "Core functionality for service discovery service"},
  {vsn, {git, short}},
  {registered, []},
  {mod, {service_discovery_app, []}},
  {applications,
   [kernel,
    stdlib,
    erldns,
    service_discovery_storage
   ]},
  {env,[]},
  {modules, []},

  {licenses, ["Apache 2.0"]},
  {links, []}
 ]}.
#+END_SRC

You can see =erldns= and =service_discovery_storage= have been added. Specifying these dependencies ensures they are available at runtime and in releases. Not putting them there can result in broken builds.

If you have ever worked with other build tools in the Erlang ecosystem, you likely never had to do this. These tools (erlang.mk or Mix in elixir) end up copying dependencies in the project configuration into the =applications= tuple for you. It sounds like a huge hindrance to have to do this by hand instead, but it ends up following OTP standards to support build-time dependencies. Other tools reach similar results through additional options within configuration files. Let's see a few scenarios where Rebar3's approach can give a critical bit of control.

The first case where this is important is downloading a dependency that you want to include in a release to help you debug it, but on which none of your OTP applications depend. Examples of this include =recon=, =redbug= or custom [[https://ferd.ca/erlang-otp-21-s-new-logger.html][=logger=]] [[http://erlang.org/doc/apps/kernel/logger_chapter.html#handlers][handlers]]. You would want these applications to be available in a release, but since the =applications= tuple lets releases know in which order applications must be booted or started, you do not necessarily want these to be part of your dependency chain. Why should a debugging tool installed just in case be up and running for a website to work? It's not necessary at all. You wouldn't want a malfunctioning debug tool to prevent your actual application from booting.

In such cases you'd want to have a project configuration that looks like this part of =service_discovery=:

#+NAME: service_discovery.relx-cfg
#+BEGIN_SRC erlang
{relx, [
    {release, {service_discovery, {git, long}},
     [service_discovery_postgres,
      service_discovery,
      service_discovery_http,
      service_discovery_grpc,
      recon]},
    ...
]}.
#+END_SRC

You can see that, along with our applications, the =recon= debugging tool is explicitly included in the release's list of applications. All their transitive dependencies will be included (according to the =applications= tuple in the =.app= file), but the various OTP applications are handled in a disjoint manner.

Let's focus on these 4 =service_discovery= applications just a bit. These represent the second type of situation where we want to split the declaration of dependencies for builds in =rebar.config=, and the declaration of dependencies for runtime in =.app.src= files.

You can see that at the top-level, all the dependencies for all the libraries are declared in a single =rebar.config= file. That makes it easy for developers to handle and update all versions required. However, if you go look into the =.app.src= files of both =service_discovery= and =service_discovery_http=, you will find this:

#+NAME: service_discovery.app-src-compare
#+BEGIN_SRC erlang
%% service_discovery.app.src
...
  {applications,
   [kernel,
    stdlib,
    erldns,
    service_discovery_storage
   ]},
...
%% service_discovery_http.app.src
...
  {applications,
   [kernel,
    stdlib,
    service_discovery,
    jsx,
    elli
   ]},
...
#+END_SRC

Here, =service_discovery_http= depends on a web server (=elli=), but =service_discovery= does not. This allows for cleaner boot and shutdown scenarios, where you don't actually need the HTTP server to be up and running to actually start booting the back-end of the system.

For a third scenario, you might also imagine a small app =service_discovery_mgmt= that is used only to generate an escript that lets you do system administration tasks to interact with the system and send commands around.

If the run-time dependencies were shared across all applications depending on the same =rebar.config= file, then even if =service_discovery_mgmt= were to _not_ be included in the release (it'd just be a script on the side), its dependencies would still risk being pushed to production in it through other apps getting them automatically inserted in there. Possibly worse, all the dependencies of the =service_discovery= release would risk being bundled with the script as well! We could end up with a small admin tool that contains web servers and database drivers because the build tool tried to be nice.

The Rebar3 maintainers therefore just decided to keep a clear distinction between the applications that need fetching for the project to build or run (in =rebar.config=), and the run-time dependencies of each OTP application (in the =.app= file) which may be part of the default OTP install, and would therefore not be included in =rebar.config=. Other build tools in the ecosystem let you achieve similar results, but they default to including everything at run-time whereas Rebar3 asks of developers to always be specific in their intent.

With this all in place, all we have to do is groom and clean our sets of dependencies.

*** Dependency Lifecycles

When it comes to initianalizing a project, the dependency resolution and fetching is where the heaviest work is done. The result is stored in a lock file, and all the results after that are handled through shorter partial changes done through Rebar3. This first phase is important to understand, even if you need to do it rather infrequently.

The Rebar3 lock file is created at the root of your project, the directory from which you called Rebar3. It is saved as =rebar.lock= and you should track it in your source control of choice. You can open the file and look at its content, but there shouldn't be any need to ever edit it by hand. You'll find that it mostly contains version numbers, application names, and various hashes. It might be interesting to audit it from time to time, but you'll end up doing that indirectly as you maintain your dependency tree.

The lock file represents the flattened tree of all dependencies as desired at the time of the build. It will not be modified unless you ask for it to be changed, or unless you delete it, forcing a new resolution to be done from scratch. This strictness is on purpose, and is part of how Rebar3 can guarantee repeatable builds under all circumstances.

The hashes in the file mean that even if the dependencies are fetched by multiple layered [[https://www.rebar3.org/docs/configuration/configuration/#hex-repos-and-indexes][mirrors]], and some malevolent person alters the packages in one or more of the various hex indices or git sources you might use, Rebar3 will be able to find that the information is not as expected, and error out because of it.

You should, therefore, only update the lock file as required. You can do so by using the following operations:

- =rebar3 unlock <appname>= removes an unused dependency from the lockfile. You will usually want to call this _after_ you have already removed it from your =rebar.config= file, to tell Rebar3 that it's really gone or has been downgraded to a transitive dependency.
- =rebar3 upgrade <appname>= tells Rebar3 to disregard the locked version for that application, and re-build the dependency tree from the version currently specified in =rebar.config= (if any). This will generate a new lock file and re-resolve all transitive dependencies that might have changed.
- =rebar3 update=, while not strictly about the lock file, updates the local _snapshot_ of remote hex packages, a kind of cache that prevents each build from pinging the package server. If you find yourself calling =rebar3 upgrade= on an app and it isn't upgrading to the latest version you know is available in Hex, you will want to =update= first. This is because Rebar3 limits the use of the network by trying to resolve dependencies with the local index cache. =rebar3 update= will fetch the latest index entries for each package already in the index and then another run of =upgrade= will see the latest versions. Note that if you specify an exact version to upgrade to Rebar3 will automatically fetch the updated index because it is unable to satisfy the dependency locally.
- =rebar3 tree= prints out the dependency tree that was built and is represented by the lock file.
- =rebar3 deps= lists out dependencies and annotate those that could be updated. Do note that there are strong limitations in what it considers worth updating: a branch in git, a tag whose reference shifted, or hex versions that weren't specified. But if you specified a package to have version ="1.2.3"= and =1.2.4= is available, it won't tell you anything.

There are other commands that are a bit more drastic, namely =rebar3 unlock= and =rebar3 upgrade= (without any arguments). These will just get rid of the lock file for the next build of the project. But overall, all these commands will do everything you need to manage most dependencies.

In general your workflow might look like this:

1. set up the initial project, compile once, and track the lock file in source control
2. find out you have a top-level dependency you want to change
3. change the dependency definition in the rebar.config file (or optionally if using a non-versioned hex package, call =rebar3 update=)
4. call =rebar3 upgrade <app>= to update the application and its transitive dependencies

And that's it, you're done.

*** Checkout Dependencies

Rebar3 wants to make the developers life easier, while also staying safe and repeatable. =_checkouts= are a feature that goes _against_ repeatability and concept such as lock files, but that provides quick feedback and a better experience around local changes to dependencies.

If you're just adopting Erlang, whether for fun or at work, chances are you'll have few projects with dependencies that live in separate repositories. Working with them won't be too hard. But sooner or later, if you have to start patching dependencies or if you're working in a corporate environment with dozens and dozens of repositories, working with Rebar3 might become frustrating.

The problem would be that every time you want to try a modification of a dependency you will have to commit and publish the change somewhere Rebar3 can fetch it, because it will not build changes made to source files under =_build= if there is already a corresponding =.beam= file for the module. This can become annoying really fast when you have to work across repository boundaries and just want to test a change.

So there's a little trickshot of a feature called _checkout dependencies_. Checkout dependencies work as follows:

- you have your main project somewhere on your file system
- the dependency is declared in the rebar.config file of the main project
- you also have that dependency somewhere on your file system, as a standalone project
- you add a =_checkouts/= directory to the main project
- you either copy or symlink the dependency's directory in the =_checkouts/= directory

From that point on, every time the main app is built, it will add an =ebin/= directory to the dependency's directory in checkouts, and re-compile it as if it were a top-level application within the main project.

You can then test the changes to your dependency within the main project until they're ready. Once you're done, remove the =ebin/= directory from the dependency, commit and publish your code in the dependency, remove it from the =_checkouts= directory, and =rebar3 upgrade= it.

This lets you do a lot of small iterative changes locally on a dependency, within the context of the main project, without having to push the dependency's changes nor changing configuration files to point a dependency to some local directory. It massively reduces the overhead of per-application repositories as an overall development strategy.

*** Using Elixir Dependencies

For many years, the Elixir and Erlang road was a one way street. You could include Erlang dependencies in your Elixir project, but the opposite wasn't true. Since then, and thanks to the support of the Erlang Ecosystem Foundation, changes have been made to Rebar3 to give it a brand new compiler management structure. This structure has made it possible to write compiler plugins that can let Erlang users use Elixir code.

The way to do it is to first install Elixir. For this you might want to follow the steps [[https://elixir-lang.org/install.html][on the official Elixir website]]. Most Elixir developers use =asdf= as a tool to manage versions. As described in [[Setup]], =kerl= options for Erlang can be used with the Erlang plugin for =asdf=, so that can give you a complete setup.

With Elixir in place, add the [[https://www.rebar3.org/docs/configuration/plugins/#elixir-dependencies][rebar_mix]] plugin to your library or project:

#+NAME: rebar_mix.config
#+BEGIN_SRC erlang
{plugins, [rebar_mix]}.
{provider_hooks, [{post, [{compile, {mix, consolidate_protocols}}]}]}.

{relx, [
    ...
    {overlay, [
        {copy, "{{base_dir}}/consolidated", "releases/{{release_version}}/consolidated"}
    ]
}.
#+END_SRC

And the following line to your =vm.args.src= file if you have one:

#+NAME: rebar_mix.args
#+BEGIN_SRC sh
-pa releases/${REL_VSN}/consolidated
#+END_SRC

From that point on, you will be able to install any hex dependency containing Elixir code without a problem. For now, the plugin only supports hex dependencies that also only rely on other hex dependencies; support for transitive dependencies using git should be coming soon, however.

Do note that mixed Rebar3 projects using both Erlang and Elixir within the same library are not currently supported since more work both on the Rebar3 and Elixir side would need to be done to make this possible. On the other hand, Mix does support this pattern if you need it.

*** Corporate Environments

Corporate environments tend to have all kinds of weird restrictions regarding what can or cannot be done, and development tools can be very idiosyncratic. Rebar3 was mostly developed to fit an open-sourced world and mainly focuses on enforcing project structure, fetching dependencies, and using both together as a great pretext to wrap a bunch of standard tools.

As such, it is perhaps unsurprising that fitting a corporate environment with the tool can prove to be a bit challenging. In this section, we'll cover some standard tools that are common to corporate environments and may make your life easier when adopting Erlang.

**** Proxy Support

Many workplaces enforce very strict firewall rules, to the point where all incoming and outgoing data must be intercepted and monitored. Generally, these places will not be totally isolated from the public Internet, but will require the use of proxy servers to make outgoing connections.

It is rather standard for programs to respect both the =HTTP_PROXY= and =HTTPS_PROXY= environment variable. When those are set in your development environment Rebar3 will make sure that all the communications it makes talking to the outside world uses these proxies.

This should let you properly work in line with your IT department's policies. In some cases, that won't even be enough.

**** Private Hex Mirrors

Some corporations go a step further and segment their internal network from the public Internet. All data that comes on site has to be inspected and hosted independently, without a chance to talk to a common code repository like github, gitlab, or hex. Another interesting case is build servers, where you might want to prevent all connectivity to the outside world both for safety and for repeatability reasons.

For such setups, two approaches tend to be used: vendoring in a monorepo, which will be covered in the next section, and through privately hosted package indexes, which we'll cover here.

The idea behind a privately hosted index is that all packages and dependencies to be used in a project need to be fully vetted. You might want to give it a technical review for code quality, a security assessment, or have corporate lawyers look at code for licensing or patent issues. Then, only the packages of the right version can be used. This kind of index is frequently accepted for hermetic builds with the assumption that it either runs locally on each build server, or within a private network that is as tightly monitored as build servers.

Rebar3 supports this use case. If you want to enable it, you will need to first set up a private hex instance. This can be done through the [[http://blog.plataformatec.com.br/2019/07/announcing-minirepo-a-minimal-hex-server/][minirepo]] project. By following the instructions [[https://github.com/wojtekmach/mini_repo][on the project page]], you will end up running your own private hex server, with either a local filesystem or S3-based storage, and the ability to both mirror other indexes and publish your own packages privately.

You will need to [[https://github.com/wojtekmach/mini_repo#usage-with-rebar3][tweak your global Rebar3 config]] to make use of it, but once that's done, you're good to go.

**** About Monorepos

If you work in a corporate environment with a monorepo where all private libraries, OTP applications, and dependencies are treated on equal footing (it's not just an umbrella release), Rebar3 is not the best tool for the job. Mostly, this comes down to the fact that most companies using a monorepo have a lot of custom tooling with very custom workflows, large codebases, and a very strong propension to never share access with maintainers of Rebar3. Until it becomes possible for maintainers to get access, little will be possible to do on that front.

Various commercial users who rely on Rebar3 despite using monorepos have reported obtaining successful builds with a combination of using =_checkouts= dependencies, along with re-configuring the =_build= directory. However, we cannot recommend this approach at this time, and no official support is provided for monorepos.

Another option is to vendor dependencies, which can be done through plugins such as [[https://www.rebar3.org/docs/configuration/plugins/#vendoring-dependencies][=rebar3_path_deps=]].

With all this in place, you should be set to manage the lifecycle of your project's dependencies.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/supervision_trees/">← Prev</a></div>
  <div><a href="/docs/development/umbrella_projects/">Next →</a></div>
</div>
#+END_EXPORT

** DONE Multi-App Projects
CLOSED: [2020-01-15 Wed 20:00]
:PROPERTIES:
:EXPORT_FILE_NAME: umbrella_projects
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

Multi-application projects, more frequently called _umbrella projects_, are how most projects are structured in a business environment, mostly because they make it easier to maintain multiple OTP Applications within a single repository. In this chapter, we'll cover their structure, when they prove most and least useful, some subtleties of this new structure, and finally, give tips to help properly split up a monolithic library into multiple OTP Applications.

*** Organizing Multi-App Projects

One example of an umbrella project is the [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]] repository we have been using before. The tell-tale sign that lets you know right away it's an umbrella is visible right in the directory listing:

#+BEGIN_SRC shell
$ ls
apps  cloudbuild.yaml  deployment          Dockerfile  README.md     rebar.lock  Tiltfile
ci    config           docker-compose.yml  LICENSE     rebar.config  test
#+END_SRC

Multi-app projects require a directory in which all the top-level applications' source code is located and this is in the =apps/= directory (the =libs/= directory is also supported). Whenever you see =apps/= or =libs/= with a =rebar.config= file, you can be pretty certain this is an umbrella project.

Take a look at that directory and you can get a pretty good idea about what the project is all about:

#+BEGIN_SRC shell
$ ls apps
service_discovery       service_discovery_http      service_discovery_storage
service_discovery_grpc  service_discovery_postgres
#+END_SRC

There are no hard rules about how to name the OTP applications in your project. The rules we have found to work well with experience were to always use some sort of namespacing, since the VM doesn't support anything like that. In the previous listing, we can see that we have one =service_discovery= app, and then a bunch of =service_discovery_<thing>= applications.

This tells us that all these applications are related. The main one is probably =service_discovery= and the others are helpers: the =_grpc= and =_http= apps are likely front-ends or client libraries (spoiler: they're front-ends), the =_storage= one should clearly handle storage, and the =_postgres= one probably also handles some sort of storage.

If you go dig in the code you'll find that =_storage= is a kind of generic storage API (see its =.app.src= file's =description= field) while =_postgres= is one specific implementation: it has been written for extensibility.

It's possible for applications to have wildly different names in that directory. For example, we could decide to write or vendor in some sort of authentication library, and so we could also have something like =authlib= and =authlib_http= in terms of applications in there.

This type of pattern is why namespacing can be useful in larger projects. As projects grow, they tend to gain more and more APIs, endpoints, clients, and ways to interact with them, and while cumbersone during writing, such manual namespaces allow very clear separations when necessary.

In all cases, this multi-app directory is the biggest structural difference between single-app and multi-app projects. There's another small variation though: you can have multiple rebar.config and test directories. In the case of =service_discovery= in particular, you can see that it has a top-level =rebar.config= file and a =test/= directory. But if you look into all individual applications, there's more:

#+BEGIN_SRC shell
$ ls apps/service_discovery*
apps/service_discovery:
src

apps/service_discovery_grpc:
proto  rebar.config  src

apps/service_discovery_http:
src

apps/service_discovery_postgres:
priv  src

apps/service_discovery_storage:
src
#+END_SRC

All apps maintain the basic need for OTP applications of having a =src/= directory, but they're free to add their own test directories, =priv/= directories, or any other they need, along with new =rebar.config= files.

Let's take a look at the =service_discovery_grpc='s config:

#+NAME: service_discovery_grpc_config
#+BEGIN_SRC erlang
{grpc, [{protos, "proto"},
        {gpb_opts, [{descriptor, true},
                    {module_name_prefix, "sdg_"},
                    {module_name_suffix, "_pb"}]}]}.
#+END_SRC

This configuration is meant specifically for the =grpcbox_plugin= that is declared in the top-level's =rebar.config=, but allows the plugin to only run for the OTP applications that do require it.

In short, this creates a multi-tiered dynamic to building and structuring applications:

- everything at the top level is shared across all top-level applications (even tests)
- each application is allowed to be more specific by creating a local version of directories or test files

There are a few exceptions to these. For example, dependencies are shared for the project: while each top-level app can specify its own dependencies, Erlang and Rebar3 only allow one version of a library to be loaded at a time (live code upgrades aside). This means that dependency resolution will pick a single winning application for all conflicting versions, and considering them shared therefore makes sense. At the opposite, directories such as =priv/= are meant for a single app's private files, and while anyone can read their content, the same directory cannot be owned by multiple applications.

Another small subtlety is around hooks; some hooks can be defined both for a single OTP application and for a whole project. For example, a =compile= hook defined at the top level will run before or after _all_ applications are built, and the same hook defined for a single application in =apps/= will run only before or after that one application is compiled.

*** Should You Migrate

The main benefit of an umbrella project compared to dozens of single-app repositories is that most of your code development gets centered in one place where you can easily have one big shared configuration for a lot of tools. They also make it easy to track everything in one repository in terms of reviews, migrations, and history. It sounds like a pretty straightforward decision to make, but it's not always that easy.

There are two big caveats to switching to multi-app projects. The first one is that the only dependencies that an Erlang project can have with Rebar3 need to be single-app repositories, at least until new features are added to permit it. If you intend to write libraries that are to be used across multiple projects within your workplace, doing it in a multi-app project is not going to work unless all development also moves into the multi-app project as well.

The second caveat is that moving all your development into the big multi-app project is not that simple either. Most of the tooling assumes you might be building one (or not more than a few) releases per project, and as such will not hesitate to run code analysis or to rebuild on all the top-level applications at once.

This means that if instead of having a few moderately sized repositories you have a gigantic one, you might see common commands take a lot more time, simply because they expected gigantic projects to be rarer.

Until Rebar3 (and other tools) can manage to catch-up to monorepos, you might want to structure things as follows:

- Make one multi-app repository per larger project, such as a service or micro-service
- All your common libraries that are shared across larger projects are maintained and published individually, and pulled in when required by specific projects
- Use [[https://www.rebar3.org/docs/tutorials/templates/#section-plugin-templates][templates]] stored in some general [[https://www.rebar3.org/docs/configuration/plugins/][plugin]] that your teammates will install globally to automate the layout and specification of services, web APIs, CI configurations, and so on

If at some point a library from a multi-app project becomes useful to other users within your organization, it becomes easy to just take it out into its own repository, publish it, and reimport it as a dependency. Similarly, orphaned libraries or forked libraries can then be maintain locally within each project.

This structure is also helpful when your organization intends to patch or develop, and then publish open source code, and makes it somewhat less costly to make changes in a library without having to synchronize all of its users at once.

It also keeps things somewhat straightforward when it comes to developing specific scripts around deployments and CI for each project; open source tools tend to keep working well. However, it has a higher cost when you're in an organization that already has monorepos and tooling developed for these. Another common roadblock is that it requires that CI and build servers for one project have read-access to those of dependencies, which isn't necessarily in place in all organizations.

*** Cutting Up Apps

Regardless of the approach you prefer—single-app, multi-app, or monorepos—you have to figure out how to best cut code up into manageable chunks.

This has always been challenging, regardless of what you're writing. The same way there are countless blog posts and articles on the perfect size of a function, how much a module or a class should contain and expose, and exactly how small or large should a micro-service be, there is no single canonical reference on what's the best OTP application size.

Rather than providing hard and fast rules, we tend to structure them according to some sort of gut feeling of what good isolation feels like. This is often the result of experience that is difficult to teach, but here are a few questions we like to ask that simplifies decision-making:

- Is the specific piece of functionality something other projects could eventually need? If so, it may help to give it its own OTP application so that it is easy to extract and share
- Does it contain code that is very specific to the concerns of the project? For service discovery, does it have to do with tracking services, or it's something general like "storing data"? The more specific, the closer it should be to the main app; the more general, the easier it is to imagine as a stand-alone OTP application
- Does it require some peculiar configuration values or domain knowledge? If so, it might be a good idea to bundle all the calls into a restricted set of modules in their own OTP application so that others can use them as a good source of abstraction (for example, handling authentication or specific protocols)
- Can you imagine it booting only in some specific contexts or builds? If so, making it a distinct app will make things easier down the road. A good example for this could be healthcheck or monitoring endpoints that could depend on your main app, but wouldn't be needed for their tests or for specific builds

All of these questions are proxies that should lead you to more easily gauge just how much coupling exists between your business logic (which tends to always live in the top-level repo and apps) and the rest of the code you're currently writing.

Tracing such a line in the sand is often a useful exercise to figure out how to structure things.

To use =service_discovery= as an example, there would technically be no strong requirement to make =service_discovery_storage= a distinct application from the main =service_discovery= application. However, we felt that it would make sense that the main application does not worry about specifics of the storage layer, whether it's generic, backed to disk, on another service, or in memory. It does not care. All that complexity and indirection can be handled in a very narrowly-defined application (=service_discovery_storage=), which can be configured with specific plugins (such as the =service_discovery_postgres= library).

We simply felt that this isolation could be beneficial in more clearly ensuring that the main app never gets to be concerned about storage-specific woes outside of calling them as a general abstraction. The complexity of switchable backends still exists, but has been isolated in one clearly-labelled area of the code, which we hoped would make maintenance simpler.

In the end, the best way to structure and organize code and repositories is to pick the one you and your team finds the most effective given your current organisational context. The way we (the authors) prefer things is one way we've found to be a reasonable compromise across multiple organizations we have worked for over the last decade or so, but it might not be as effective as fully embracing what is currently provided to you.

Solve one problem at a time; if your team is learning Erlang for the first time, it may make sense to start with everything on the path of least resistance given your organization's existing deployment and build systems, knowing fully you'll rework them later to be more comfortable. Trying to tackle all the problems at once and trying to solve them all perfectly the first time around can be counterproductive when you have limited energy, time, or resources.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/dependencies">← Prev</a></div>
  <div><a href="/docs/development/hard_to_get_right/">Next →</a></div>
</div>

#+END_EXPORT
** TODO Rebar3 Shell
:PROPERTIES:
:EXPORT_FILE_NAME: rebar3_shell
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

** TODO Configuration
:PROPERTIES:
:EXPORT_FILE_NAME: configuration
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

** TODO Documentation
:PROPERTIES:
:EXPORT_FILE_NAME: documentation
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

** TODO Testing
:PROPERTIES:
:EXPORT_FILE_NAME: testing
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

*** Common Test
*** Coverage
*** Dialyzer

Example from =service_discovery=: adding =port_name= to the endpoints:

#+BEGIN_SRC erlang
-spec endpoint_from_json(unicode:unicode_binary(), map()) -> {ok, service_discovery:endpoint()} |
                                                             {error, term()}.
endpoint_from_json(ServiceName, #{<<"ip">> := IPString,
                                  <<"port">> := Port,
                                  <<"tags">> := Tags}) ->
    case inet:parse_address(binary_to_list(IPString)) of
        {ok, IP} ->
            {ok, #{service_name => ServiceName,
                   ip => IP,
                   port => Port,
                   tags => Tags}};
        {error, einval}=Error ->
            Error
    end;
endpoint_from_json(_, _) ->
    {error, bad_endpoint_json}.
#+END_SRC

Missing =port_name= in return results in map that does not match the `service_discovery:endpoint()` spec:

#+BEGIN_SRC erlang
apps/service_discovery_http/src/sdh_handler.erl
  42: The pattern 'ok' can never match the type {'error','bad_endpoint_json' | 'einval'}
 130: The pattern {'ok', Endpoint} can never match the type {'error','bad_endpoint_json' | 'einval'}
#+END_SRC

*** XRef
*** Continuous Integration
**** Docker Compose
**** CircleCI

CircleCI has a very flexible CI offering that integrates well with services like Github. One of this author's favorite features of CircleCI when working with Erlang is the built in support for serving up test artifacts after a test run. This is especially useful because Common Test will output details about test runs as HTML. Plus CircleCI supports displaying information about a test run based on JUnit structured XML which CT can output using the =surefire= hook.

The =surefire= hook can be enabled in =rebar.config= with the following =ct_opts=:

#+BEGIN_SRC erlang
%% generate junit xml report from test results
{ct_opts, [{ct_hooks, [cth_surefire]}]}.
#+END_SRC

CircleCI has no official support for Rebar3 projects, but they do host Erlang docker images which contain Rebar3 and tools necessary for interacting with CircleCI's cache and artifact store. But thanks to [[https://circleci.com/orbs/][CircleCI Orbs]] it is easy to get started with Rebar3 on CircleCI and take advantage of its unique features. CircleCI Orbs are reusable commands, executors and jobs that make writing workflows simpler. The [[https://circleci.com/orbs/registry/orb/tsloughter/rebar3][Rebar3 Orb]] takes care of caching built dependencies and moving artifacts, like CT's HTML output, with CircleCI's artifacts commands.

CircleCI runs Docker images on machines with many more CPU's than are allocated for your job. Because Erlang can not limit the number of schedulers it starts to the number of CPU's allocated to the container, Erlang will start as many schedulers as CPU's it sees on the host. This results in wasted resources because the VM is busy scheduling something like 32 schedulers when it only has access to 1 or 2 processors. To configure the number of schedulers that Erlang will start when running =rebar3= use the environment variable =ERL_FLAGS= which takes the arguments that need to be passed to =erl= in this case ="+S 2"=.

#+BEGIN_SRC yaml
version: 2.1

orbs:
  rebar3: tsloughter/rebar3@0.7.0

workflows:
  version: 2.1
  build_and_test:
    jobs:
    - rebar3/compile

    - rebar3/xref:
        requires:
        - rebar3/compile
    - rebar3/dialyzer:
        requires:
        - rebar3/compile
    - rebar3/ct:
        requires:
        - rebar3/compile
#+END_SRC

The following screenshot show's CircleCI's workflow page for a project that runs Common Test, Dialyzer and xref in parallel followed by cover which is reported to [[https://codecov.io][codecov.io]]:

[[./static/img/circleci_ct_success_workflow.png]]

Clicking on the =rebar3/ct= job in the workflow graph goes to the job page which has the output from the steps in the job, a test summary based on the JUnit XML and a tab with artifacts kept after the job completed:

[[./static/img/circleci_ct_junit_success.png]]

Going to the artifacts tab we see the contents from =_build/test/logs=:

[[./static/img/circleci_ct_artifacts.png]]

Following the link for =index.html= will open a new browser tab for a url like https://498-59958463-gh.circle-artifacts.com/0/common_test/index.html where you can browse all Common Test test result pages:

[[./static/img/circleci_ct_html.png]]

Lastly, this image shows an example of the test summary, from the =cth_surefire= output, for a failed test case in a suite:

[[./static/img/circleci_ct_junit_failure.png]]


**** Microsoft's visual studio output
 https://github.com/ferd/trx

**** CirrusCI
**** Google Cloud Build

#+BEGIN_SRC yaml
steps:
- name: 'docker/compose:1.24.0'
  args: ['-f', 'docker-compose.yml', 'up', '-d']

- name: 'gcr.io/kaniko-project/executor:latest'
  args:
  - --target=releaser
  - --dockerfile=./Dockerfile.cb
  - --build-arg=BASE_IMAGE=$_BASE_IMAGE
  - --build-arg=RUNNER_IMAGE=$_RUNNER_IMAGE
  - --destination=gcr.io/$PROJECT_ID/service_discovery:tester-$BUILD_ID
  - --cache=true
  - --cache-ttl=48h

- name: 'gcr.io/cloud-builders/docker'
  args: ['run', '--network', 'workspace_sd_net', '-e', 'BUILD_ID=$BUILD_ID', '--entrypoint', 'rebar3', 'gcr.io/$PROJECT_ID/service_discovery:tester-$BUILD_ID', 'ct']
#+END_SRC

** DONE Hard Things to Get Right
CLOSED: [2019-09-18 Wed 16:39]
:PROPERTIES:
:EXPORT_FILE_NAME: hard_to_get_right
:EXPORT_HUGO_MENU: :menu main :parent development
:END:

You probably have a fairly decent understanding of how an Erlang project should be structured by now. Along with any guide about the language basics, you should be mostly good to get started. However, there are a few complex topics that are currently not covered well in any of the Erlang documentation out there. In this chapter, we'll go through the task of providing guidance around handling Unicode, time, and SSL/TLS configurations.

Do note that those are three complex topics on their own. While we're going to provide some background information on each of them, you are not going to be an expert at handling them right away—it just helps to know how much complexity exists to avoid huge mistakes.

*** Handling Unicode

Erlang's got quite a bad reputation for string handling. A lot of this comes from not having a dedicated string type, and for years, not having decent unicode support outside of community libraries. While the former has not changed, there are some strengths to that approach, and the latter has finally been addressed in recent Erlang releases.

**** Background Information on Unicode

Unicode, in a nutshell, is a set of standards about how text should be handled in computers, regardless of the user's language (real languages, not programming languages). It has become a huge specification with a lot of exceedingly complex considerations about all kinds of details, and developers are often reasonably getting lost in it.

Even without knowing all about Unicode, you can know _enough_ to be effective and avoiding all of the most glaring mistakes. To get started, we'll introduce a bit of terminology:

- _character_: the word "character" is defined kind of vaguely in Unicode. Every time you see the word "character", imagine that the person talking to you is using a very abstract term that can mean anything from a letter in a given alphabet, some drawings (like emojis), accents or letter modifiers (like =¸= and =c=, which becomes =ç=), control sequences (like "backspace"), and so on. It's mostly a common but inaccurate way to refer to bits of text, and you must not attach too much meaning to it. Unicode has better and exact definitions of its own.
- _code point_: the Unicode standard defines all the possible basic fundamental "characters" you can have in a big list (and then some), each of which has a unique identifier. That identifier is the _code point_, often denoted =U+<hexadecimal number>=. For example, "M" has the code point =U+004D=, and ♻ has the code point =U+267B=. You can see [[https://unicode-table.com][the Unicode Table]] for the full list.
- _encoding_: While code points are just integers that represent an index by which you can look up, this is not sufficient to represent text in programming languages. Historically, a lot of systems and programming languages used bytes (=0..255=) to represent all valid characters in a language. If you needed more characters, you had to switch languages. To be compatible with all kinds of systems, Unicode defines _encodings_, which allows people to represent sequences of code points under various schemes. _UTF-8_ is the most common one, using bytes for everything. Its representation shares the same basic structure as [[https://en.wikipedia.org/wiki/ASCII][ASCII]] or [[https://en.wikipedia.org/wiki/ISO/IEC_8859-1][Latin-1]] did, and so it became extremely popular in Latin and Germanic languages. _UTF-16_ and _UTF-32_ are two alternatives that represent on wider sequences (16 or 32 bits).
- _code unit_: A code unit specifies the way a given code point is encoded in a given encoding. Each code point takes from 1 to 4 code units for UTF-8. For example, =F= takes only =46= as a code unit in UTF-8, =0046= in UTF-16, and =00000046= in UTF-32. By comparison, =©= has the code point =U+00A9=, but is representable as _two_ code units in UTF-8 (=C2= and =A9=), and one code unit in UTF-16 and UTF-32 (=00A9= and =000000A9= respectively).
- _glyph_: the graphic representation of a character. For example, =U+2126= is "Ohm sign", represented as =Ω=, and =U+03A9= is "Greek Capital Letter Omega", also represented by a similar-looking =Ω=. In some [[https://en.wikipedia.org/wiki/Typeface][Typefaces]] they will be the same, in some not. Similarly, the letter "a" is possibly representable by glyphs looking like "а" or "α".  Some code points have no associated glyphs ("backspace", for example), and some glyphs can be used for _ligatures_ representing multiple codepoints at once (such as =æ= for =ae=).
- _grapheme cluster_: all of the terms mentioned so far have to do with very abstract concepts. Unicode has funky stuff like _combining marks_ and ways to join multiple code points into one "character". This can become super confusing because what a user considers a character and what a programmer considers a character are not the same thing. A _grapheme cluster_ is a term meaning "a unit of text the user perceives as being a single character". For example, the letter "ï" is composed of two code-points: the latin small letter =i= (=U+0069=), and a =combining= [[https://en.wikipedia.org/wiki/Diaeresis_(diacritic)][diaeresis]] (=¨= as =U+0308=). So for a programmer, this will look like two  code points, encoded with 3 code units in UTF-8. For a user though, they will expect that pressing "backspace" will remove both the diaeresis _and_ the letter "i".

That's a lot of stuff, but those are important to know about. There is no direct relationship between how a programmer writes a character and how it ends up displayed to a user.

One particularly fun example is the _ARABIC LIGATURE BISMILLAH AR-RAHMAN AR-RAHEEM_, which is a single code point (=U+FDFD=), but represented graphically as "﷽". This is currently the widest "character" in the Unicode standard. This represents an entire arabic sentence, and was added to the standard because it turns out to be a legal requirement in multiple Urdu documents, without their keyboard layouts having the ability to type arabic. It's a great bit of unicode to mess with UI folks.

Most languages have problems with the fact that graphical (and logical) representations are not equal to the underlying codes creating the final character. Those exist for all kinds of possible ligatures and assemblies of "character parts" in various languages, but for Emojis, you can also make a family by combining individual people: 👩‍👩‍👦‍👦 is a family composed of 4 components with combining marks: 👩 + 👩 + 👦 + 👦, where =+= is a special combining mark (a [[https://www.fileformat.info/info/unicode/char/200d/index.htm][zero width joiner]]) between two women and two boys (if you are viewing this document on an older browser, with an older font, or are checking out the PDF version of this book, then you might just see four people instead of a family.) If you were to go and consume that sequence byte by byte or codepoint by codepoint, you would break the family apart and change the semantic meaning of the text.

If you edit the text in a text editor that traditionally has good support for locales and all kinds of per-language rules, such as Microsoft Word (one of the few we know to do a great job of automatically handling half-width spaces and non-breakable spaces when languages ask for it), pressing backspace on 👩‍👩‍👦‍👦 will remove the whole family as one unit. If you do it in FireFox or Chrome, deleting that one 'character' will take you 7 backstrokes: one for each 'person' and one for each zero-width joining character. Slack will consider them to be a single character and visual studio code behaves like the browsers (even if both are electron apps), and notepad.exe or many terminal emulators will instead expand them as 4 people and implicitly drop the zero-width joining marks.

This means that no matter which programming language you are using, if strings look like arrays where you can grab "characters" by position or through some index, you are likely to have serious problems.

Worse than this, some "characters" have more than one acceptable encoding in Unicode. The character =é= can be created by encoding a single code point (=U+00E9=), or as the letter =e= (=U+0065=) followed by =´= (=U+0301=). This will logically be the same letter =é= in French, but two strings using the two different forms will not compare equal. Unicode therefore introduces concepts such as [[http://unicode.org/reports/tr15/][Normalization]], which specifies how to coerce the representation of strings according to four possible standards: NFC, NFD, NFKC, and NFKD (if you don't know which one to use, stick to NFC).

Sorting strings also introduces concepts such as [[http://unicode.org/reports/tr10/][Collations]], which require knowing the current language being used when sorting.

In short, to support Unicode well in your programs, no matter in which programming language you work, you must treat strings as a kind of opaque data type that you manipulate exclusively through Unicode-aware libraries. Anything else and you are manipulating _byte sequences_ or _code point sequences_ and may end up breaking things unexpectedly at the human-readable level.


**** Handling Strings in Erlang

Erlang's support for strings initially looks a bit funky: there is no dedicated string type. When considering all the complexity of Unicode though, it's not actually all that bad. It's usually as tricky to work with just _one_ string type as it would be to work with _no_ string types at all, because of all the possible alternative representations.

Folks using programming languages with variable string types that reflect multiple encodings may feel good about themselves right now, but you'll see that Erlang has pretty decent Unicode support all things considered—only collations appear to be missing.

***** Data Types

In Erlang, you have to be aware of the following possible encodings for strings:

- ="abcdef"=: a string, which is directly made up of Unicode code points in a list. This means that if you write =[16#1f914]= in your Erlang shell, you'll quite literally get ="🤔"= as a string, with no regards to encoding. This is a singly linked-list.
- =<<"abcdef">>= as a binary string, which is shorthand for =<<$a, $b, $c, $d, $e, $f>>=. This is an old standard list of Latin1 integers transformed as a binary. By default this literal format does _not_ support Unicode encodings, and if you put a value that is too large in there (such as =16#1f914=) by declaring a binary like =<<"🤔">>= in your source file, you will instead find yourself with an overflow, and the final binary =<<20>>=. This is implemented with an Erlang binary (what is essentially an immutable byte array), and is meant to handle any kind of binary data content, even if it's not text.
- =<<"abcdef"/utf8>>= as a binary Unicode string that is encoded as UTF-8. This one would work to support emojis. It is still implemented as an Erlang binary, but the =/utf8= constructor ensures proper Unicode encoding. =<<"🤔"/utf8>>= returns =<<240,159,164,148>>=, which is the proper sequence to represent the thinking emoji in UTF-8.
- =<<"abcdef"/utf16>>= as a binary string that is Unicode encoded as UTF-16. =<<"🤔"/utf16>>= returns =<<216,62,221,20>>=
- =<<"abcdef"/utf32>>= as a binary string that is Unicode encoded as UTF-32. =<<"🤔"/utf32>>= returns =<<0,1,249,20>>=
- =["abcdef", <<"abcdef"/utf8>>]=: This is a special list dubbed "IoData" that can support multiple string formats. Your list can be codepoints as usual, but you'll want all the binaries to all be the same encoding (ideally UTF-8) to prevent issues where encodings get mixed.

If you want to work with Unicode content, you will want to use the various string-related modules in Erlang.

The first one is [[http://erlang.org/doc/man/string.html][string]], which contains functions such as =equal/2-4= to handle string comparison while dealing with case sensitivity and normalization, =find/2-3= to look for substrings, =length/1= to get the number of grapheme clusters, =lexemes/2= to split a string on some pattern, =next_codepoint/1= and =next_grapheme/1= to consume bits of a string, =replace/3-4= for substitutions, =to_graphemes/1= to turn a string into lists of grapheme clusters, and finally functions like =lowercase/1=, =uppercase/1=, and =titlecase/1= to play with casing. The module contains more content still, but that should be representative.

You will also want to use the [[http://erlang.org/doc/man/unicode.html][unicode]] module to handle all kinds of conversions across string formats, encodings, and normalization forms. The regular expression module [[http://erlang.org/doc/man/re.html][re]] handles unicode fine (just pass in the =unicode= atom to its options lists), and lets you use [[http://erlang.org/doc/man/re.html#generic_character_types][Generic Character Types]] if you pass in the =ucp= option. Finally, the =file= and =io= modules all support specific options to make unicode work fine.

All of these modules work on any form of string: binaries, lists of integers, or mixed representations. As long as you stick with these modules for string handling, you'll be in a good position.

The one tricky thing you have to remember is that the encoding of a string is implicit. You have to know what it is when a string enters your system: an HTTP request often specifies it in headers, and so does XML. JSON and YAML mandate using UTF-8, for example. When dealing with SQL databases, each table may specify its own encoding, but so does the connection to the database itself! If any one of them disagrees, you're going to corrupt data.

So you will want to know and identify your encoding very early on, and track it well. It's not just a question of which data type in your language exists, it's a question of how you design your entire system and handle exchanging data over the network.

There's one more thing we can cover about strings: how to transform them effectively.

***** IoData

So which string type should you use? There are plenty of options, but picking one is not too simple.

The quick guidelines are:

- Binaries for UTF-8, which should represent the majority of your usage
- Binaries for UTF-16 and UTF-32, should you use them
- Lists as strings are rarely used in practice, but can be very effective if you want to work at the codepoint level
- Use IoData for everything else, particularly building strings.

One advantage of the binary data types is their ability to create subslices efficiently. So for example, I could have a binary blob with content such as =<<"hello there, Erlang!">>= and if I pattern match a subslice such as =<<Txt:11/binary, _/binary>>=, then =Txt= now refers to =<<"hello there">>= at the same memory location as the original one, but with no way to obtain the parent context programmatically. It's a bounded reference to a subset of the original content. The same would not be true with lists, since they're defined recursively.

On top of that, binaries larger than 64 bytes can be shared across process heaps, so you can cheaply move string content around the virtual machine without paying the same copying cost as you would with other data structure.

#+attr_shortcode: warning
#+begin_admonition
#+begin_export html
Binary sharing is often a great way to gain performance in a program. However, there exist some pathological usage patterns where binary sharing can lead to memory leaks. If you want to know more, take a look at <a href='https://www.erlang-in-anger.com/'>Erlang In Anger's</a> chapter on memory leaks, particularly section 7.2
#+end_export
#+end_admonition

The real cool thing though comes from the IoData representation where you combine the list approach with binaries. It's how you get really cheap composition of immutable strings:

#+NAME: greetings
#+BEGIN_SRC erlang
Greetings = <<"Good Morning">>,
Name = "James",
[Greetings, ", ", Name, $!]
#+END_SRC

The final data structure here looks like =[<<"Good Morning">>, ", ", "James", 33]= which is a mixed list containing binary subsections, literal codepoints, strings, or other IoData structures. But the VM mechanisms all support handling it as if it were a flat binary string: The IO systems (both network and disk access), and the modules named in the previous section all seamlessly handle this string as =Good Morning, James!= with full Unicode support.


So while you can't mutate strings, you can append and match a bunch of them in constant time, no matter the type they initially had. This has interesting implications if you're writing libraries that do string handling. For example, if I want to replace all instances of =&= by =&amp;=, and I started with =<<"https://example.org/?abc=def&ghi=jkl"/utf8>>=, I might instead just return the following linked list:

#+NAME: url_sublists
#+BEGIN_SRC erlang
% a list
[%% a slice of the original unmutated URL
 <<"https://example.org/?abc=def"/utf8>>,
 %% a literal list with the replacement content
 "&amp;amp",
 %% the remaining sub-slice
 <<"ghi=jkl"/utf8>>
]
#+END_SRC

What you have then is a string that is in fact a linked list of 3 elements: a slice of the original string, the replaced subset, the rest of the original string. If you're replacing on a document that's taking 150MB in RAM and you have somewhat sparse replacements, you can build the entire thing and edit it with essentially no overhead. That's pretty great.

So why else are IoData strings kind of cool? Well the unicode representation is one fun thing. As mentioned earlier, grapheme clusters are a crucial aspect of Unicode strings when you want to operate on them as a human would (rather than as binary sequences that only programmers would care about). Whereas most programming languages that use a flat array of bytes to represent strings have no great way to iterate over strings, Erlang's =string= module lets you call =string:to_graphemes(String)= to play with them:


#+NAME: graphemes
#+BEGIN_SRC erlang
erl +pc latin1 # disable unicode interpretation
1> [Grapheme | Rest] = string:next_grapheme(<<"ß↑õ"/utf8>>),
[223 | <<226,134,145,111,204,131>>]
2> string:to_graphemes("ß↑õ"),
[223,8593,[111,771]]
3> string:to_graphemes(<<"ß↑õ"/utf8>>),
[223,8593,[111,771]]
#+END_SRC

This lets you take any unicode string, and turn it into a list that is safe to iterate using calls such as =lists:map/2=, lists comprehensions, or pattern matching. This can only be done through IoData, and this might even be a better format than what you'd get with just default UTF-8 binary strings.

Do note that pattern matching is still risky there. Ideally you'd want to do a round of normalization first, so that characters that can be encoded in more than one way are forced into a uniform representation.

This should hopefully demistify Erlang's strings.

*** Handling Time

Time is something very simple to live, but absurdly difficult to describe. It has taken philosophers and scientists centuries of debate to kind of get to a general agreement, and we software folks decided that counting seconds since January 1st 1970 ought to be good enough. It's harder than that. We won't get into all the details about calendaring rules and conversions, timezones, concepts of leap seconds and so on; that's too general a topic. However, we'll cover some important distinctions between _wall clocks_ and _monotonic time_, and how the Erlang virtual machine can help us deal with this stuff.

**** Background Information on Time

If we want to be very reductionist, there are a few specific use cases for time measurements, and they are often rather distinct:

1. Knowing the duration between two given events, or "how long does something take?", which requires having a single value in a unit such as microseconds, hours, or years
2. Placing an event on the timeline in a way we can pinpoint it and understand when it happened, or "when does something happen?", usually based on a [[https://en.wikipedia.org/wiki/ISO_8601][datetime]].

Essentially those both have to do with time, but are not measured the same way.

The short answer for Erlang is that you should use [[http://erlang.org/doc/man/erlang.html#monotonic_time-0][=erlang:monotonic_time/0-1=]] to calculate durations, and [[http://erlang.org/doc/man/erlang.html#system_time-0][=erlang:system_time/0-1=]] for the system time (such as a [[https://www.unixtimestamp.com/][UNIX timestamp]]). If you want to understand _why_, then you will want to read the rest of this chapter.

A datetime is usually based on a calendar date (most of readers are likely using the [[https://en.wikipedia.org/wiki/Gregorian_calendar][Gregorian Calendar]]) along with a given hour, minute, and seconds value, along with a timezone and optionally, a higher precision value for milliseconds or microseconds. This value needs to be understandable by humans, and is entirely rooted into a social system that people agree means something for a given period of time: we all know that the year 1000 refers to a specific part of time even if our current Gregorian calendar was introduced in 1582 and technically none of the dates before have happened under that system -- they may have been lived under the Julian calendar or the mayan calendar, but not the gregorian one. The whole concept is eventually attached to astronomical phenomena such as Earth's orbit, or [[https://en.wikipedia.org/wiki/Solar_time#Mean_solar_time][mean solar days]].

On the other hand, a time interval, particularly for computers, is based on some cyclical event which we count. [[https://en.wikipedia.org/wiki/Water_clock][Water clocks]] or hourglasses would have a given rate at which drops or grains of sands would fall, and when it was empty, a given period of time would have gone through. Modern computers use various mechanisms like [[https://en.wikipedia.org/wiki/Clock_signal][clock signals]], [[https://en.wikipedia.org/wiki/Crystal_oscillator][crystal oscillators]], or if you're very fancy, [[https://en.wikipedia.org/wiki/Atomic_clock][atomic clocks]]. By synchronizing these cyclical countable events up with concepts such as solar days, we can adjust the measured cycles to a broader point of reference, and join our two time accounting systems: durations and pinpointing in time can be reconciliated.

We as humans are used to considering these two concepts as two facets of the same "time" value, but when it comes to computers, they're really not that good at doing both at the same time. It really helps a lot if we keep both concepts distinct: durations are not the same thing as absolute points in times, and should be handled differently.

To give an example, many programmers are aware of [[https://en.wikipedia.org/wiki/Unix_time][Unix Time]] (a duration in seconds since January 1st 1970), and many programmers are aware of [[https://en.wikipedia.org/wiki/Coordinated_Universal_Time][UTC]] as a standard. However, few developers are aware that both are actually tricky to convert because UTC handles [[https://en.wikipedia.org/wiki/Leap_second][leap seconds]], but unix timestamps do not, and it can cause all kinds of weird, funny issues. Using them interchangeably can introduce subtle bugs in software.

One particular challenge is that computer clocks are not particularly good at being accurate over long periods of time, a concept known as [[https://en.wikipedia.org/wiki/Clock_drift][clock drift]]. One result of this is that while we might have a very decent resolution over short intervals (a few minutes or hours), over weeks, months, or years, clocks drift _a lot_. The frequency of the computer clock varies here and there, but overall doesn't change too much. All the short-time calculations you'll make will be fine, but they won't be good enough to keep track of most longer spans of time. Instead, protocols such as [[https://en.wikipedia.org/wiki/Network_Time_Protocol][NTP]] are required to re-synchronize computer clocks with far more accurate (and more expensive) ones, over the network.

This means that it is to be expected that the time on your computer might just jump around for fun. Even more so if the operator just plays with things like changing timezones or system time.

**** Handling Time in Erlang

The way your computer handles time is based on the previously mentioned clocks that simply increment by counting microseconds or milliseconds (depending on hardware and operating systems). To represent time in units that makes sense to us humans, a conversion is done from some _epoch_ (an arbitrary starting point) to some time standard (UTC). Because computer clocks drift and shift with time, an _offset_ value is kept that enables correcting the local ever-increasing clock so that it makes sense to people. This is usually all hidden, and unless you know where to look, you won't know it happens.

Erlang's runtime system uses the same kind of mechanisms, but makes them explicit. It exposes two clocks:

1. A _monotonic_ clock, which means a clock that is just a counter that always returns increasing values (or the same number as before, if you call it during the same microsecond). It can have a high precision and is useful to calculate intervals.
2. A _system_ clock, which exposes the time the user usually cares about as a human being. This tends to be done by using the unix POSIX time (seconds since January 1st 1970), which is widely used by computers everywhere, and plenty of conversion libraries exist for all other kinds of time formats. It presents a kind of lowest common denominator for human time.

Overall, all the clocks on your system may end up looking like Figure [[fig:clocks]]:

#+NAME: fig:clocks
[[./static/img/clock-compare.png]]

There is some real perceived time (which we'll assume is rather constant, if we ignore [[https://en.wikipedia.org/wiki/Time_dilation][relativistic effects]]), which the computer's clock more or less matches. Voltage, temperature, humidity, and hardware quality may all impact its reliability. The Erlang VM provides its own monotonic clock, which is synchronized on the hardware clock (if any), and allows some additional control which we'll describe soon.

The system time, for its own part, is always calculated as a given offset from its underlying monotonic clock. The objective is to take the arbitrary number of clock ticks of the hardware clock, and turn them into seconds from 1970, which can then be converted to other formats.

If the offset is a constant 0, then the VM's monotonic and system times will be the same. If the offset is modified positively or negatively, the Erlang system time may be made to match the OS system time while the Erlang monotonic time is left independent. In practice, it is possible for the monotonic clock to be some large negative number, and the system clock to be modified by the offset to represent the positive POSIX timestamp.

This means that in Erlang, you'll want to use the following functions for specific use cases:

- [[http://erlang.org/doc/man/erlang.html#monotonic_time-0][=erlang:monotonic_time/0-1=]] for the Erlang monotonic time. It may return very low negative numbers, but they'll never get more negative. You can use something like =T0=erlang:monotonic_time(millisecond), do_something(), T1=erlang:monotonic_time(millisecond)= and get the total duration of the operation by calculating =T1 - T0=. Note that the time units should be the same across comparisons (see notice).
- [[http://erlang.org/doc/man/erlang.html#system_time-0][=erlang:system_time/0-1=]] for the Erlang system time (after the offset has been applied) when you need a UNIX timestamp
- [[http://erlang.org/doc/man/erlang.html#time_offset-0][=erlang:time_offset/0-1=]] to figure out the difference between the Erlang monotonic and Erlang system clocks
- [[http://erlang.org/doc/man/calendar.html#local_time-0][=calendar:local_time/0=]] to return the system time converted to the operating system's current clock (meaning in the user's current timezone and daylight saving times) in a ={{Year, Month, Day}, {Hour, Minute, Second}}= format
- [[http://erlang.org/doc/man/calendar.html#universal_time-0][=calendar:universal_time/0=]] to return the system time converted to the current time in UTC in a ={{Year, Month, Day}, {Hour, Minute, Second}}= format.

#+attr_shortcode: tip
#+begin_admonition
#+begin_export html
The functions handling time in the <code>erlang</code> module almost all take a <code>Unit</code> argument, which can be one of <code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>, or <code>native</code>. By default, the type of timestamp returned is in the native format. The unit is determined at run time, and <code>erlang:convert&#95;time&#95;unit(Time, FromUnit, ToUnit)</code> may be used to convert between time units. For example, <code>erlang:convert&#95;time&#95;unit(1, seconds, native)</code> returns <code>1000000000</code>.
#+end_export
#+end_admonition

The [[http://erlang.org/doc/man/calendar.html][=calendar=]] module also contains more utility functions, such as [[http://erlang.org/doc/man/calendar.html#valid_date-1][date validation]], conversion [[http://erlang.org/doc/man/calendar.html#system_time_to_rfc3339-1][to]] and [[http://erlang.org/doc/man/calendar.html#rfc3339_to_system_time-1][from]] RFC3339 datetime strings (=2018-02-01T16:17:58+01:00=), time differences, and conversions to days, weeks, or detecting leap years.

The last tool in the arsenal is a new type of monitor, usable to detect when the time offset jumps. It can be called as =erlang:monitor(time_offset, clock_service)=. It returns a reference and when time drifts, the message received will be ={'CHANGE', MonitorRef, time_offset, clock_service, NewTimeOffset}=.

**** Time Warping

If you use the previous functions in the right context, you'll almost never have a problem handling time. The only thing you have to care about now is how to handle weird cases such as the host computer going to sleep and waking up with a brand new clock, dealing with a system administrator playing with the time, having NTP force a clock forwards and backwards in time, and so on. Not caring for them can make your system behave in very weird ways.

Fortunately, the Erlang VM lets you pick and choose from pre-established strategies, and as long as you stick with using the right functions at the right time (monotonic clocks for intervals and benchmarks, system time for pinpointing events in time), you can just choose whichever option you feel is more appropriate. Picking the right functions for the right use cases ensures that your code is [[http://erlang.org/doc/apps/erts/time_correction.html#Dos_and_Donts][time warp safe]].

These options can be passed by passing the =+C= (warp mode) and =+c= (time correction) switches to the =erl= executable. The warp mode (=+C=) defines how the offset between monotonic and system time is handled, and the time correction (=+c=) defines how the VM will adjust the monotonic clock it exposes when the system clock changes.

- =+C multi_time_warp +c true=: The time offset can change at any time without any limitations to provide good system time, and the Erlang monotonic clock frequency can be adjusted by the VM to be as accurate as possible. This is what you want to specify on any modern platform, and tends to have better performance, scale better, and behave better.
- =+C no_time_warp +c true=: The time offset is chosen at VM start time, and then is never modified. Instead, the monotonic clock is sped up or slowed down to slowly correct time drift. This is the default mode for backwards compatibility reasons, but you might want to pick a different one that is more in line with proper time usage.
- =+C multi_time_warp +c false=: The time offset can change at any time, but the Erlang monotonic clock frequency may not be reliable. If the OS system time leaps forwards, the monotonic clock will also leap forward. If the OS system time leaps backwards, the Erlang monotonic clock may pause briefly.
- =+C no_time_warp +c false=: The time offset is chosen at VM start time, and then is never modified. The monotonic clock is allowed to stall or jump forwards in large leaps. You generally do not want this mode.
- =+C single_time_warp +c true=: This is a special hybrid mode to be used on embedded hardware when you know Erlang boots before the OS clock is synchronized (for example, you boot your software before NTP synchronization can take place). When the VM boots, the Erlang monotonic clock is kept as stable as possible, but no system time adjustments are made. Once time synchronization is done at the OS level, the user calls =erlang:system_flag(time_offset, finalize)=, the Erlang system time warps once to match the OS system time, and then the clocks become equivalent to those under =no_time_warp=.
- =+C single_time_warp +c false=: This is a special hybrid mode to be used on embedded hardware when you know Erlang boots before the OS clock is synchronized (for example, you boot your software before NTP synchronization can take place). No attempts are made to synchronize the Erlang system time with the OS system time, and any changes in the OS system times may have impacts on the Erlang monotonic clock. Once time synchronization is done at the OS level, the user calls =erlang:system_flag(time_offset, finalize)=, the Erlang system time warps once to match the OS system time, and then the clocks become equivalent to those under =no_time_warp=

You generally want to always have =+c true= as an option (it's the default), and to force =+C multi_time_warp= (which is not default). If you want to emulate old Erlang systems where clock frequency is adjusted, pick =+C no_time_warp=, and if you work in embedded systems where the first clock synchronization can jump really far in time and after that you expect it to be more stable _and_ you don't want =+C multi_time_warp= (you should want it!), then look for =single_time_warp=.

In short, if you can, pick =+C multi_time_warp +c true=. It's the best option for accurate time handling out there.


*** SSL Configurations
**** Background Information on TLS

Coming Soon...

**** Handling TLS in Erlang

Coming Soon...

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/umbrella_projects/">← Prev</a></div>
  <div><a href="/docs/production/">Next →</a></div>
</div>
#+END_EXPORT

* Production
:PROPERTIES:
:EXPORT_HUGO_SECTION: docs/production
:END:

** DONE Index
CLOSED: [2019-08-08 Thu 08:05]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_FRONT_MATTER_KEY_REPLACE: title>label
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :title "Production"
:EXPORT_HUGO_MENU: :menu main :weight 3001  :identifier production
:END:

#+BEGIN_EXPORT html
<header>
  <h1>Production</h1>
  <h5>
    <strong>August 3, 2019</strong>
  </h5>
</header>
#+END_EXPORT

Erlang/OTP is certainly a unique language and runtime, but it is not as different as even some proponents would have you believe. In this part we will see how to build artifacts for deployment and how to operate the deployment in the same environment you would run any other service. In this part we'll build a deployable artifact (a release), create a docker image of the release and deploy to a Kubernetes cluster.

A common claim heard on forums and comment sections of popular tech news sites is, you have OTP, you don't need Kubernetes. Or the opposite, that having Kubernetes replaces OTP. Erlang does not replace Kubernetes, nor does Kubernetes replace Erlang. If an Erlang system didn't need to be monitored and restarted like any other runtime because it has supervision trees then it wouldn't come with [[http://erlang.org/doc/man/heart.html][heart]]. Similarly, if restarting the entire program with =heart= was adequate there wouldn't be supervision trees.

[[https://kubernetes.io/][Kubernetes]] provides a scheduler for efficiently packing your programs, as containers, on physical or virtual machines. Using containers eases deployment by bundling all dependencies (think OpenSSL in an Erlang release's case) into a single image and isolation that protects against dependency conflicts between services running on a machine. Additionally, deployment and management becomes consistent across the services in your production environment no matter the language or runtime they are built with. When you integrate your Erlang service with common monitoring and tracing services, you will also ensure it's no longer the oddball your coworkers dread having to deal with.

However, containers and Kubernetes are not appropriate in all cases. Kubernetes can be overkill, particularly if using a hosted solution isn't an option, or your product could be an embedded device.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/development/hard_to_get_right">← Prev</a></div>
  <div><a href="/docs/production/releases">Next →</a></div>
</div>
#+END_EXPORT

** DONE Releases
CLOSED: [2019-09-24 Tue 07:43]
:PROPERTIES:
:EXPORT_FILE_NAME: releases
:EXPORT_HUGO_MENU: :menu main :parent production
:END:

In [[/docs/development/otp_high_level/][OTP at a High Level]] we took a bird's-eye view of how applications are combined into a release and in the following chapters of [[/docs/development/][Development]] we built and tested OTP applications. We have discussed how an Erlang system is setup differently from many other languages which have a single entry point the project's code, like a =main()= function that is called when starting the program. Ultimately starting an Erlang node that runs your code is similar enough to other languages and runtimes, and can be packaged up in a Docker container to run like any other container, and does not appear so different once it is being run by booting a container. But getting to the point that you have a bundled up release with an easy to use shell script, or a Docker image ready to run, can be confusing at first and give the impression Erlang is too different or too difficult to be running in your environment.

This chapter will give some low level details about how a release is constructed, but the main purpose is to use the [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]] project to show how to use =rebar3= for building releases suited for local development, then a production ready release, and finally how to use the tools provided by =rebar3= to configure and operate a release. We will show that the advantages from the flexibility of a system based on booting applications which run concurrently does not have to come with the cost of esoteric operations.

*** The Nitty Gritty

Going back to the operating system comparison, starting an Erlang release is similar to an operating system boot sequence that is followed by an init system starting services. An Erlang node starts by running instructions found in a [[http://erlang.org/doc/man/script.html][boot file]]. The instructions load modules and start applications. Erlang provides functions for generating a boot script from a list of all required applications, defined in a release resource file with extension ".rel", and the corresponding application resource file, the ".app" file each application has. The application resource file defines the modules the boot script must load and the dependencies of each application so that the boot script will start applications in the correct order. When only the applications used in a release are bundled together with the boot script, to be copied and installed on a target, it is called a *target system*.

In the earlier days of Erlang/OTP, there was only =systools= and its functions for generating boot scripts from release resource files. Back then, release handling was a manual process around which users built their own tooling. Then came =reltool=, a release management tool that ships with Erlang/OTP and was meant to ease the creation of releases—it even has a GUI. While creating and installing target systems has never been provided outside of an example module found in the =sasl= application, =sasl/examples/src/target_system.erl=.

Releases continued to be mysterious and difficult to build to many users. relx was created with the goal of making release creation and management so simple that users no longer felt it was a burden best not undertaken -- this is done in part through requiring minimal configuration to get started and by including tools for runtime management in the generated release. When Rebar3 was started, it bundled relx to provide its release building functionality.

Along with building and packaging =relx= comes with a shell script for starting a release and interacting with the running release. While running a release can be as simple as =erl= (which itself runs a boot script built from =start.script= you can find in the =bin/= directory of your Erlang install) the provided script handles setting appropriate arguments to point to configuration files, attaching a remote console to a running node, running functions on a running node and more.

In the following sections we will dissect release building for the [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]] project. The focus is on real world usage of the tools and not the low level details of constructing and running a release.

*** Building a Development Release

The first thing to look at in [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]] is the =relx= section of =rebar.config=:

#+NAME: rebar.config
#+BEGIN_SRC erlang
{relx, [{release, {service_discovery, {git, long}},
         [service_discovery_postgres,
          service_discovery,
          service_discovery_http,
          service_discovery_grpc,
          recon]},

        {sys_config, "./config/dev_sys.config"},
        {vm_args, "./config/dev_vm.args"},

        {dev_mode, true},
        {include_erts, false},

        {extended_start_script, true},

        {overlay, [{copy, "apps/service_discovery_postgres/priv/migrations/*", "sql/"}]}]}.
#+END_SRC

The =release= tuple in the =relx= configuration list defines a release's name, version and the applications
included in the release. The version here is ={git, long}= which tells =relx= to use the full git sha reference of the current commit as the version of the release. The applications the release will boot includes the Postgres storage backend, the application that is the main interface to the services and the DNS setup, HTTP and grpc frontends and a tool useful for inspecting production nodes, =recon=.

The order the applications are listed is important here. When constructing the boot script a stable sort on all applications based on their dependencies is done to decide the order to start them. However, when the dependency order is not specific enough to make a decision, the order defined in the list is kept. Each of =service_discovery_postgres=, =service_discovery_http= and =service_discovery_grpc= depend on =service_discovery=, resulting in the latter being the first to start. The next one will then be =service_discovery_postgres= because it is listed first. This is important because we need the storage backend available before the HTTP and grpc services are usable.

=rebar3 release= runs =relx= with a configuration based on the =relx= section of =rebar.config= and the Rebar3 project structure, allowing =relx= to find the necessary applications for building the release.

#+BEGIN_SRC shell
$ rebar3 release
===> Verifying dependencies...
===> Compiling service_discovery_storage
===> Compiling service_discovery
===> Compiling service_discovery_http
===> Compiling service_discovery_grpc
===> Compiling service_discovery_postgres
===> Starting relx build process ...
===> Resolving OTP Applications from directories:
          /app/src/_build/default/lib
          /app/src/apps
          /root/.cache/erls/otps/OTP-22.1/dist/lib/erlang/lib
          /app/src/_build/default/rel
===> Resolved service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70
===> Dev mode enabled, release will be symlinked
===> release successfully created!
#+END_SRC

The first =relx= step seen in the output is "Resolving OTP Applications" followed by a list of directories it will search for built applications. For each application in the =relx= release's configuration, in this case =service_discovery_postgres=, =service_discovery=, =service_discovery_http=, =service_discovery_grpc=, and =recon=, =relx= will find the directory of the built application and then do the same for any application listed in its =.app= file.

#+attr_shortcode: note
#+begin_admonition
#+begin_export html
The application <code>sasl</code> is not included in the <code>relx</code> config's list of applications. By default in the Rebar3 template it is in the list. This is because <code>sasl</code> is required for certain release operations. The <code>sasl</code> application includes <code>release&#95;handler</code> which provides functionality for performing release upgrades and downgrades. Since we are focused here on creating containers which are replaced instead of live upgraded <code>sasl</code> does not need to be included.
#+end_export
#+end_admonition

Because this release is built with ={dev_mode, true}=, symlinks are created in the release's lib directory that point to each application instead of being copied:

#+BEGIN_SRC shell
$ ls -l _build/default/rel/service_discovery/lib
lrwxrwxrwx ... service_discovery-c9e1c80 -> .../_build/default/lib/service_discovery
#+END_SRC

The same is also done for the runtime configuration files =dev_sys.config= and =dev_vm.args=:

#+BEGIN_SRC shell
$ ls -l _build/default/rel/service_discovery/releases/c9e1c805d57a78d9eb18af1124962960abe38e70
lrwxrwxrwx [...] sys.config -> [...]/config/dev_sys.config
lrwxrwxrwx [...] vm.args -> [...]/config/dev_vm.args
#+END_SRC

This allows for a faster feedback loop when running our release for local testing. Simply stopping and starting the release again will pick up any changes to beam files or configuration, no need to run =rebar3 release=.

#+attr_shortcode: warning
#+begin_admonition
#+begin_export html
On Windows the <code>dev&#95;mode</code> of relx won't necessary work but it will fallback to copying.
#+end_export
#+end_admonition

The full filesystem tree of the development release now looks like:

#+BEGIN_SRC shell
_build/default/rel/service_discovery/
├── bin/
│   ├── install_upgrade.escript
│   ├── nodetool
│   ├── no_dot_erlang.boot
│   ├── service_discovery
│   ├── service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70
│   └── start_clean.boot
├── lib/
│   ├── acceptor_pool-1.0.0 -> /app/src/_build/default/lib/acceptor_pool
│   ├── base32-0.1.0 -> /app/src/_build/default/lib/base32
│   ├── chatterbox-0.9.1 -> /app/src/_build/default/lib/chatterbox
│   ├── dns-0.1.0 -> /app/src/_build/default/lib/dns
│   ├── elli-3.2.0 -> /app/src/_build/default/lib/elli
│   ├── erldns-1.0.0 -> /app/src/_build/default/lib/erldns
│   ├── gproc-0.8.0 -> /app/src/_build/default/lib/gproc
│   ├── grpcbox-0.11.0 -> /app/src/_build/default/lib/grpcbox
│   ├── hpack-0.2.3 -> /app/src/_build/default/lib/hpack
│   ├── iso8601-1.3.1 -> /app/src/_build/default/lib/iso8601
│   ├── jsx-2.10.0 -> /app/src/_build/default/lib/jsx
│   ├── recon-2.4.0 -> /app/src/_build/default/lib/recon
│   ├── service_discovery-c9e1c80 -> /app/src/_build/default/lib/service_discovery
│   ├── service_discovery_grpc-c9e1c80 -> /app/src/_build/default/lib/service_discovery_grpc
│   ├── service_discovery_http-c9e1c80 -> /app/src/_build/default/lib/service_discovery_http
│   └── service_discovery_storage-c9e1c80 -> /app/src/_build/default/lib/service_discovery_storage
├── releases/
│   ├── c9e1c805d57a78d9eb18af1124962960abe38e70
│   │   ├── no_dot_erlang.boot
│   │   ├── service_discovery.boot
│   │   ├── service_discovery.rel
│   │   ├── service_discovery.script
│   │   ├── start_clean.boot
│   │   ├── sys.config.src -> /app/src/config/sys.config.src
│   │   └── vm.args.src -> /app/src/config/vm.args.src
│   ├── RELEASES
│   └── start_erl.data
└── sql
    ├── V1__Create_services_endpoints_table.sql
    └── V2__Add_updated_at_trigger.sql
#+END_SRC

The last directory, =sql=, is created by an =overlay=, ={copy, "apps/service_discovery_postgres/priv/migrations/*", "sql/"}=. An =overlay= tells =relx= about files to include that are outside of the regular release structure of applications and boot files. It supports making directories, basic templating with =mustache= and copying files. In this case we only need to copy the =service_discovery_postgres= application's migrations to a top level directory of the release =sql=. The migrations would be included anyway because they are in the =priv= of an application that is part of the release, but since we intend to only have one release available at a time (see notice box for more information) it simplifies the migration scripts we will see later to keep them in a known top level directory.

#+attr_shortcode: info
#+begin_admonition
#+begin_export html
The OTP release structure supports having multiple versions of the same release. They share the same <code>lib</code> directory but could have different versions of each application and even different <code>erts</code>. This is why there is a <code>releases</code> directory with a version number directory under it and the file <code>RELEASES</code>. Having the SQL files in a directory shared by all versions of the release could be problematic in such an environment. But since we are focused on building self contained releases that will run separately from any other in a container we can assume there is only ever one version.
#+end_export
#+end_admonition

Since the release uses the Postgres storage backend,a database needs to be running and accessible before starting the release. Running Docker Compose at the top level of the project will bring up a database and run the migrations:

#+BEGIN_SRC shell
$ docker-compose up
#+END_SRC

#+attr_shortcode: tip
#+begin_admonition
#+begin_export html
You don't have to start everything to get an Erlang shell from your release. Every release built with Rebar3 comes with a boot script called <code>start&#95;clean</code> which only starts the <code>kernel</code> and <code>stdlib</code> applications. This can be run with the command <code>console&#95;clean</code> and can be useful for debugging purposes.
#+end_export
#+end_admonition

To boot the development release to an interactive Erlang shell run the extended start script with command =console=:

#+BEGIN_SRC shell
$ _build/default/rel/service_discovery/bin/service_discovery console
Erlang/OTP 22 [erts-10.5] [source] [64-bit] [smp:1:1] [ds:1:1:10] [async-threads:30] [hipe]

(service_discovery@localhost)1>
#+END_SRC

With =service_discovery= running, =curl= can be used to access the HTTP interface. The following commands create a service =service1=, verify it was created by listing all services, registers an endpoint with the service at IP =127.0.0.3= and  adds a named port =http= on port 8000.

#+BEGIN_SRC shell
$ curl -v -XPUT http://localhost:3000/service \
    -d '{"name": "service1", "attributes": {"attr-1": "value-1"}}'
$ curl -v -XGET http://localhost:3000/services
[{"attributes":{"attr-1":"value-1"},"name":"service1"}]
$ curl -v -XPUT http://localhost:3000/service/service1/register \
    -d '{"ip": "127.0.0.3", "port": 8000, "port_name": "http", "tags": []}'
$ curl -v -XPUT http://localhost:3000/service/service1/ports \
    -d '{"http": {"protocol": "tcp", "port": 8000}}'
#+END_SRC

=service_discovery= DNS server is running on port 8053, use =dig= to see that =service1= is a registered endpoint at the correct IP and that a service (SRV) DNS query returns the port and DNS name for the service.

#+BEGIN_SRC shell
$ dig -p8053 @127.0.0.1 A service1
;; ANSWER SECTION:
service1.		3600	IN	A	127.0.0.3
$ dig -p8053 @127.0.0.1 SRV _http._tcp.service1.svc.cluster.local
;; ANSWER SECTION:
_http._tcp.service1.svc.cluster.local. 3600 IN SRV 1 1 8000 service1.svc.cluster.local.
#+END_SRC

*** Building a Production Release

Preparing a release to be deployed to production requires different options than what is best used during local development. Rebar3 profiles allow us to override and add to the =relx= configuration. This profile is commonly named =prod=:

#+NAME: rebar.config
#+BEGIN_SRC erlang
{profiles, [{prod, [{relx, [{sys_config_src, "./config/sys.config.src"},
                            {vm_args_src, "./config/vm.args.src"},
                            {dev_mode, false},
                            {include_erts, true},
                            {include_src, false},
                            {debug_info, strip}]}]
            }]}.
#+END_SRC

We have two overridden configuration values in the =prod= profile. =dev_mode= is set to =false= so all content is copied into the release directory, we can't utilize symlinks to the =_build= directory from another machine and a production release should be an immutable snapshot of the project. =include_erts= copies the Erlang runtime and the Erlang/OTP applications depended on by the release into the release directory and configures the boot script to point to this copy of the runtime.

The entries added to the configuration are setting =include_src= to =false=, =debug_info= to =strip=, and =_src= versions of the configuration files. Running the release in production doesn't require the source code, so we set =include_src= to false in order to drop it from the final release to save on space. Additional space is saved by stripping debug information from the beam files with =debug_info= set to =strip=. Debug information is used by tools like the debugger, =xref= and =cover= but in a release those tools won't be used and, unless explicitly included, won't even be available.

=sys_config_src= and =vm_args_src= take precedence over the entries =sys_config= and =vm_args= we had in the default profile, these two will be discussed more in the following section [[Runtime Configuration]]. There will be a warning printed when the production release is built in case the user did not intend for this, but we are doing it on purpose so the warning can be ignored.

Building with the production profile enabled results in artifacts being written to the profile directory =_build/prod/=:

#+BEGIN_SRC shell
$ rebar3 as prod release
===> Verifying dependencies...
===> Compiling service_discovery_storage
===> Compiling service_discovery
===> Compiling service_discovery_http
===> Compiling service_discovery_grpc
===> Compiling service_discovery_postgres
===> Starting relx build process ...
===> Resolving OTP Applications from directories:
          /app/src/_build/prod/lib
          /app/src/apps
          /root/.cache/erls/otps/OTP-22.1/dist/lib/erlang/lib
===> Resolved service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70
===> Both vm_args_src and vm_args are set, vm_args will be ignored
===> Both sys_config_src and sys_config are set, sys_config will be ignored
===> Including Erts from /root/.cache/erls/otps/OTP-22.1/dist/lib/erlang
===> release successfully created!
#+END_SRC

Viewing the tree of the new =prod= profile's release directory we see:

#+NAME: prod_rel_structure
#+BEGIN_SRC sh
_build/prod/rel/service_discovery
├── bin
│   ├── install_upgrade.escript
│   ├── nodetool
│   ├── no_dot_erlang.boot
│   ├── service_discovery
│   ├── service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70
│   └── start_clean.boot
├── erts-10.5
│   ├── bin
│   ├── doc
│   ├── include
│   ├── lib
│   └── man
├── lib
│   ├── acceptor_pool-1.0.0
│   ├── asn1-5.0.9
│   ├── base32-0.1.0
│   ├── chatterbox-0.9.1
│   ├── crypto-4.6
│   ├── dns-0.1.0
│   ├── elli-3.2.0
│   ├── erldns-1.0.0
│   ├── gproc-0.8.0
│   ├── grpcbox-0.11.0
│   ├── hpack-0.2.3
│   ├── inets-7.0.8
│   ├── iso8601-1.3.1
│   ├── jsx-2.10.0
│   ├── kernel-6.5
│   ├── mnesia-4.16
│   ├── public_key-1.6.7
│   ├── recon-2.4.0
│   ├── service_discovery-c9e1c80
│   ├── service_discovery_grpc-c9e1c80
│   ├── service_discovery_http-c9e1c80
│   ├── service_discovery_storage-c9e1c80
│   ├── ssl-9.3.1
│   └── stdlib-3.10
├── releases
│   ├── c9e1c805d57a78d9eb18af1124962960abe38e70
│   │   ├── no_dot_erlang.boot
│   │   ├── service_discovery.boot
│   │   ├── service_discovery.rel
│   │   ├── service_discovery.script
│   │   ├── start_clean.boot
│   │   ├── sys.config.src
│   │   └── vm.args.src
│   ├── RELEASES
│   └── start_erl.data
└── sql
    ├── V1__Create_services_endpoints_table.sql
    └── V2__Add_updated_at_trigger.sql
#+END_SRC

There are no symlinks under =lib= and OTP applications like =stdlib-3.10= are included. At the top of the tree is =erts-10.5= which contains the Erlang runtime, =bin/beam.smp=, along with executables like =erlexec=, =erl= and =escript= required for running and interacting with a release.

To build the target system of the release we run the =tar= command in the =prod= profile:

#+BEGIN_SRC shell
$ rebar3 as prod tar
#+END_SRC

Now we have a tarball =_build/prod/rel/service_discovery/service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70.tar.gz= that can be copied to any compatible host, unpacked and run. But before we do that we need to know how to configure =sys.config.src= and =vm.args.src= when the release is booted.

*** Runtime Configuration

In the =service_discovery= project we have two files for configuration under the =config/= directory that are included in the production release: =vm.args.src= and =sys.config.src=. These files act as templates to be filled in at runtime based on environment variables. There are two separate files because running a release involves two levels of configuration. First, there is the underlying Erlang virtual machine's settings. These values have to be set before the VM has started, so they cannot be part of a term file like =sys.config= which requires a running VM to read and parse the Erlang term file. Instead, the VM arguments are passed directly to =erl= -- =erl= being the command used to boot a release. To simplify this, the =erl= command has an =-args_file= argument to allow command-line arguments to be read from a plain text file. This file is commonly named =vm.args=.

The second level is the configuration for the Erlang applications that make up the release. This is done with a file passed to =erl= through the =-config= argument. The file is a list of 2-tuples where the first element is the name of the application to set the environment of and the second element is a list of key-value pairs to set in the environment.

Of course static files can be pretty limiting and it has become common to want to set configuration through OS environment variables. To offer flexibility and support for environment variable configuration the release start script generated when using Rebar3 can replace variables of the form =${FOO}= with the value found in the current environment. This is done automatically if the files end with extension =.src=.

In the =relx= configuration we use =vm_args_src= and =sys_config_src= to include the files and signal that they are templates -- this is necessary so that the release building does not attempt to verify that =sys.config= is a proper list of Erlang terms, which, for example, ={port, ${PORT}}= is not:

#+BEGIN_SRC erlang
{relx, [...
        {sys_config_src, "config/sys.config.src"},
        {vm_args_src, "config/vm.args.src"},
        ...
       ]}.
#+END_SRC

In the Docker and Kubernetes chapters we will discuss why we need to set =sbwt=, but for now we just care about how it would be done in =vm.args.src=:

#+BEGIN_SRC shell
+sbwt ${SBWT}
#+END_SRC

In =sys.config.src= we will make the =logger= level a variable as well, so we could, for example, turn on =debug= or =info= level logging to get more details when investigating the deployed service:

#+BEGIN_SRC erlang
{kernel, [{logger_level, ${LOGGER_LEVEL}}]}
#+END_SRC

Now when running the release we must set these variables or the release will fail to start.

#+BEGIN_SRC shell
$ DB_HOST=localhost LOGGER_LEVEL=debug SBWT=none \
  _build/prod/rel/service_discovery/bin/service_discovery console
Erlang/OTP 22 [erts-10.5] [source] [64-bit] [smp:1:1] [ds:1:1:10] [async-threads:30] [hipe]

(service_discovery@localhost)1>
#+END_SRC

The errors produced when running without the necessary environment variables set can be confusing. There is currently no validation done to check that all environment variables used in the configuration files are set and then print out which are missing. Instead the missing variables are replaced with empty strings. If the variable is used in a string, like ="${DB_HOST}"= the applications will start but be unable to connect to the database. When the missing variable creates a =sys.config= that is not able to be parsed, like in the case of =${LOGGER_LEVEL}=, there will be a syntax error:

#+BEGIN_SRC shell
$ DB_HOST=localhost SBWT=none \
  _build/prod/rel/service_discovery/bin/service_discovery console
{"could not start kernel pid",application_controller,"error in config file \"/app/src/_build/prod/rel/service_discovery/releases/71e109d8f34ef5e5ccfcd666e0d9e544836044f1/sys.config\" (48): syntax error before: ','"}
could not start kernel pid (application_controller) (error in config file "/home/tristan/Devel/service_discovery/_build/prod/rel/service_discovery/releases/71e109d8f34ef5e5ccfcd666e0d9e544836044f1/sys

Crash dump is being written to: erl_crash.dump...done
#+END_SRC

When a value in =vm.args= is missing you will likely see an error about the argument given to a flag. For =+sbwt ${SBWT}= it results in attempting to use the next line as the argument to =+sbwt=, in this case it is =+C multi_time_warp= producing a boot error that =+C= is not a valid :

#+BEGIN_SRC shell
$ DB_HOST=localhost LOGGER_LEVEL=debug \
  _build/prod/rel/service_discovery/bin/service_discovery console
bad scheduler busy wait threshold: +C
Usage: service_discovery [flags] [ -- [init_args] ]
The flags are:
...
#+END_SRC

Not all failures from missing variables are so simple and they can result in very odd crashes, or no crash on boot but failures within the application that depends on the value. We hope to improve this in future Rebar3 versions to enable a check before startup that will fail with a list of missing environment variables. But for now if you are seeing odd behavior, or a crash that you can't understand, checking the environment variables is a good place to start.

*** Coming Up

To get a feel for what we will be doing in the next chapters, copy =_build/prod/rel/service_discovery/service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70.tar.gz= to =/tmp/service_discovery=, unpack it and start the node.

#+BEGIN_SRC shell
$ export LOGGER_LEVEL=debug
$ export SBWT=none
$ export DB_HOST=localhost
$ mkdir /tmp/service_discovery
$ cp _build/prod/rel/service_discovery/service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70.tar.gz /tmp/service_discovery
$ cd /tmp/service_discovery
$ tar -xvf service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70.tar.gz && rm service_discovery-c9e1c805d57a78d9eb18af1124962960abe38e70.tar.gz
...
$ ls
bin  erts-10.5  lib  releases
$ bin/service_discovery console
...
(service_discovery@localhost)1>
#+END_SRC

Running with =console= gives an interactive Erlang shell. To run the release without an interactive shell use =foreground=, which is how we will be running the release in a Docker container in the next chapter. With the release running with =foreground= or =console=, open a separate terminal and try the same script with the argument =remote_console=:

#+BEGIN_SRC shell
$ export LOGGER_LEVEL=debug
$ export SBWT=none
$ export DB_HOST=localhost
$ bin/service_discovery remote_console
...
(service_discovery@localhost)1>
#+END_SRC

=remote_console= uses =-remsh= [[http://erlang.org/doc/man/erl.html#flags][erl]] flag for connecting an interactive shell to the running release. The command knows which running Erlang node to connect to based on the same configuration, from =vm.args=, that sets the name and cookie for the node that was run originally. This means any environment variables used to populate =vm.args= when the node is started must be set the same when connecting the remote console.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/production/">← Prev</a></div>
  <div><a href="/docs/production/docker/">Next →</a></div>
</div>
#+END_EXPORT

** DONE Docker
CLOSED: [2019-09-28 Sat 09:32]
:PROPERTIES:
:EXPORT_FILE_NAME: docker
:EXPORT_HUGO_MENU: :menu main :parent production
:END:

[[https://docker.com][Docker]] helped popularize Linux containers through its ease of use and registry of pre-built images, and became a word often used interchangeably with "Linux container".

Docker images contain multiple layers that are merged at runtime to make up the filesystem of the container. Docker creates the layers by running commands found in a =Dockerfile=, each command creates a new layer. Layers are shared between images, saving space, and can be used as a cache for speeding up the building of images. Additional space is saved, compared to other options like a virtual machine (VM), by not including the Linux kernel in the image. The size of the image is little larger than the size of the packaged Erlang release we are deploying.

The small size and ability to run like a regular Linux process (a new kernel isn't booted) makes for quicker start times and less resource consumption than using a traditional VM for isolation. Having little overhead means that the advantages of isolation when packaging and running a program can be standard practice instead of the burden it would be to have to run a VM per program.

Advantages of containers running with filesystem and network isolation are not having to perform operations that are common when programs are not isolated:

- Pre-installing shared libraries
- Updating configuration
- Finding an open port
- Finding a unique name for node name

#+attr_shortcode: note
#+begin_admonition
#+begin_export html
You might notice we will not be using the <code>latest</code> tag at all when using Docker. This tag is commonly misunderstood and misused. It is assigned to the last image used without a specific tag, it is not the latest created image. It should rarely, if ever, be relied on, unless you really don't care what version of an image will be used.
#+end_export
#+end_admonition

In this chapter we will cover efficiently building images for running the [[https://adoptingerlang.org/gh/servicediscovery][service_discovery]] project as well as images for running tests and dialyzer. Then, we will update the continuous integration pipeline to build and publish new images.

The minimum Docker version required for this chapter is 19.03 with [[https://github.com/docker/buildx][buildx]] installed. Installing =buildx= can be done with the following commands:

#+BEGIN_SRC shell
$ export DOCKER_BUILDKIT=1
$ docker build --platform=local -o . git://github.com/docker/buildx
$ mv buildx ~/.docker/cli-plugins/docker-buildx
#+END_SRC

*** Building Images

[[https://hub.docker.com/_/erlang/][Official Erlang Docker images]] are published for each new OTP release. They include Rebar3 and come in [[https://alpinelinux.org/][Alpine]] and [[https://www.debian.org/][Debian]] flavors -- the images are updated for new releases of Rebar3 and Alpine/Debian as well. Because the tagged images are updated for new releases it is recommended to both use the =sha256= digest of the image and to mirror the images used to your own repository, even if your repository is also on Docker Hub. Having a copy ensures the base image does not change without developer intervention and having a mirror in a registry separate from Docker Hub means you are not dependent on its availability. This best practice is why in the examples to follow and the =service_discovery= repository use images from =ghcr.io/adoptingerlang/service_discovery/= and =us.gcr.io/adoptingerlang/=.

**** Private Dependencies

The first stumbling block many in a work environment encounter when building Docker images is access to private dependencies. If you have private git repos or [[https://hex.pm/docs/rebar3_private][Hex organization packages]] as dependencies they will not be able to be fetched in the Docker container during the build. Often this leads people to not include =_build= in =.dockerignore= and risk polluting the build with local artifacts, possibly not being reproducible elsewhere, so the dependencies could be fetched with Rebar3 before running =docker build=. The other option is copying the host SSH credentials and/or Hex apikey into the build container, but this is not recommended because it will be kept in the Docker layer and leaked anywhere you push the image. Instead, in recent Docker releases (18.06 and later) the abilities to mount secrets and SSH agent connections or keys in a secure manner. The data does not leak to the final image or any commands it is not explicitly mounted to.

Since =service_discovery= has no private dependencies we will look at how to support them separately, before getting started with building the images for =service_discovery=.

***** Hex Dependencies

Rebar3 keeps the access keys for private Hex dependencies in a file =~/.config/rebar3/hex.config=. Using the experimental Dockerfile syntax =--mount=type=secret= the config can be mounted into the container for just the compile command. The file is mounted to a separate tmpfs filesystem and excluded from the build cache:

#+BEGIN_SRC dockerfile
# syntax=docker/dockerfile:1.2
RUN --mount=type=secret,id=hex.config,target=/root/.config/rebar3/hex.config rebar3 compile
#+END_SRC

To mount the host's =hex.config= when running =docker build= simply pass a secret with a matching =id= and the =src= path to the file:

#+BEGIN_SRC shell
$ docker build --secret id=hex.config,src=~/.config/rebar3/hex.config .
#+END_SRC

***** Git Dependencies

You could use the secret mount from the last section for mounting SSH keys, but Docker added a better solution with a mount type specifically for dealing with SSH. A =RUN= command that needs SSH access can use =--mount=type=ssh=:

#+BEGIN_SRC dockerfile
# syntax=docker/dockerfile:1.2
RUN apt install --no-cache openssh-client git && \
    mkdir -p -m 0600 ~/.ssh && \
    ssh-keyscan github.com >> ~/.ssh/known_hosts && \
    git config --global url."git@github.com:".insteadOf "https://github.com/"
WORKDIR /src
COPY rebar.config rebar.lock .
RUN --mount=type=ssh rebar3 compile
#+END_SRC

First, a =RUN= command installs the necessary dependencies, SSH and git. Then, =ssh-keyscan= is used to download the current public key for Github and add it to =known_hosts=. The public key being in =known_hosts= means SSH will not attempt to prompt to ask if you accept the host's public key. Next, the git config setting ensures that even if in the =rebar.config= the git url is using =https= it will instead use SSH. If the private repos are not on Github this url replacement has to be changed for the appropriate location.

Along with adding the previous snippet to the Dockerfiles we'll see later in this chapter you'll also need to add =--ssh default= to the build command when run and set =DOCKER_BUILDKIT=:

#+BEGIN_SRC shell
$ export DOCKER_BUILDKIT=1
$ docker build --ssh default .
#+END_SRC

Additional information and options for the SSH mount type can be found [[https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md#run---mounttypessh][in the Moby documentation]] -- Moby is the name of the project that makes up the core functionality of Docker.

**** Efficient Caching

***** Basic Instruction Ordering

The order of commands in a Dockerfile is very important to the build time and size of the images it creates. Each command in the Dockerfile creates a layer which is then reused in future builds to skip a command if nothing has changed. With Rebar3 we take advantage of this by creating a layer containing the built dependencies of our project:

#+BEGIN_SRC dockerfile
COPY rebar.config rebar.lock .
RUN rebar3 compile
#+END_SRC

The =COPY= command will only invalidate the cache of the command that runs =rebar3 compile= (and subsequent commands in the file) if =rebar.config= or =rebar.lock= are different from a previously created layer. Since none of the project's code was copied and Rebar3 only builds the dependencies, this results in a layer containing only the built dependencies under =_build/default/lib=.

After the dependencies are built and cached we can copy in the rest of the project and compile it:

#+BEGIN_SRC dockerfile
COPY . .
RUN rebar3 compile
#+END_SRC

Because of the order of operations in the Dockerfile each run of =docker build
.= only compiles the project's source, assuming there is a change, otherwise an
existing layer is used here as well. Any command that does not need to be rerun
when there are changes to the project need to come before either of the =COPY=
commands. For example, installing Debian packages, =RUN apt install git= and =WORKDIR /app/src= for setting the working directory.

Use of =COPY . .= is discouraged because it makes it more likely to invalidate the cache. If possible it is better to copy only the files and directories needed for the build or having a =.dockerignore= file filter out files that are not needed. The =.git= directory is one that because of its size and the fact changes to the contents in it do not effect the build artifacts is good to ignore. However, in =service_discovery= we rely on git commands for setting the version of the release and the applications that make up the release. In projects that do not rely on this feature of Rebar3 it is recommended to add =.git= to =.dockerignore=.

***** Experimental Mount Syntax

Copying files into an image and caching layers are no longer the only options for efficiency when building a Docker image. Caching the built dependencies in a layer is all well and good but that layer also contains the Hex package cache Rebar3 creates under =~/.cache/rebar3/hex=. Any change to =rebar.config= or =rebar.lock= will result in all the packages having to not only be rebuilt but also be re-fetched from Hex. Additionally, the instruction to copy in the whole project, creating an additional layer with all the source is wasteful since we only care about the build artifacts.

These issues are resolved as of Docker 19.03 through an experimental syntax for
mounting files to the context of a =RUN= command. To enable the experimental
syntax the environment variable =DOCKER_BUILDKIT= must be set or
={"features":{"buildkit": true}}= be set in =/etc/docker/daemon.json=, and =#
syntax=docker/dockerfile:1.2= used as the first line of the Dockerfile:

#+BEGIN_SRC dockerfile
# syntax=docker/dockerfile:1.2

[...]

WORKDIR /app/src

ENV REBAR_BASE_DIR /app/_build

# build and cache dependencies as their own layer
COPY rebar.config rebar.lock .
RUN --mount=id=hex-cache,type=cache,sharing=locked,target=/root/.cache/rebar3 \
    rebar3 compile

RUN --mount=target=. \
    --mount=id=hex-cache,type=cache,sharing=locked,target=/root/.cache/rebar3 \
    rebar3 compile
#+END_SRC

In this new set of instructions the build sets the =WORKDIR= to =/app/src=, this is then the current working directory of the commands that follow. And an environment variable =REBAR_BASE_DIR= is set to =/app/_build=. The base directory is where Rebar3 will output all build artifacts and by default is a directory =_build/= at the root of the project, in this case without the environment variable it would have been =/app/src/_build=.

The =COPY= of the Rebar3 configuration and lock file remains the same but the following =RUN= has changed to include a =--mount= option with type =cache=. This tells Docker to create a cache directory separate from the Docker layers and stored locally on the host. This cache will remain between runs of =docker build=, so future local runs of =docker build=, even if the config or lock file has changed, will mount this cache and only newly needed packages will be fetched from Hex.

Next, unlike the previous instructions for building the rest of the project the instruction =COPY . .= has been removed. Instead, a mount of type =bind= (the default) is used with a =target= of =.=. A bind mount, as opposed to the =cache= mount, means Docker will mount from the build context into the container, giving us the same result as =COPY . .=, but without creating a layer from copying in the files, making the build faster and smaller. With the =COPY= command there are two copies made from the host: the build context and the copy in the build container. Each run of =build= with =COPY= requires copying the whole project from the build context into the build container again.

By default the mount is immutable, meaning the build will error out if anything is written to =/app/src=, and that is why the Rebar3 base directory has been configured to =/app/_build=. There is an option to mount in =read-write= mode but the writes are not persisted and it removes optimizations around not having to make copies of the data from the build context for the build container. More can be read about the mount options in the Buildkit docs for [[https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md][Dockerfile frontend experimental syntax]].

The end result is a layer containing =/app/_build= with the compiled dependencies (along with =/app/src/rebar.*=, but they add very little to the size of the layer) followed by a layer containing =/app/_build= with the compiled project but nothing in =/app/src=. Separately there is a cache of all the hex packages that were downloaded.

***** Local vs Remote Cache

There are a couple types of caches we have used in this section and both have relied the builds being done on the same host to have access to the cache. In the case of the =hex-cache= that is mounted during =RUN= this is solely a feature for local caching and can not be exported or imported from a registry. However, the layers built from each instruction in the Dockerfile can be imported from a registry.

Having a build be setup to use a remote cache is particularly useful when working with continuous integration or any sort of build server. Unless there is only a single node running builds, it will end up wasting time rebuilding every step of a Dockerfile. To resolve this issue =docker build= can be told where to look for layers, including images in a remote registry, with =--cache-from=.

There are two versions of =--cache-from=, and because the newer one is still technically part of an experimental "tech preview" called [[https://github.com/docker/buildx][buildx]], we will cover both. However, since =buildx= is so much more efficient, easier to use and appears stable within the bounds of the features we require, it will be the default used by the =service_discovery= project.

The old =--cache-from= is not "multi-stage aware", meaning it requires the user manually building and pushing each stage in a multi-stage Dockerfile individually. When building stages which build on the earlier stage its image can be referenced through =--cache-from= and it will be pulled from the registry.

With =buildx= a cache manifest is built that will include information about previous stages in a multi-stage build. The argument =--cache-to= allows for this cache to be exported in various ways. We will use the =inline= option which writes the cache manifest directly to the image's metadata. The image can be pushed to the registry and then referenced in a later build with =--cache-from=. The unique part about the new cache manifest is that only layers that are cache hits will be downloaded, in the old form the entire image of the previous stage was downloaded when referenced by =--cache-from=.

#+attr_shortcode: danger "Caching and Security Updates"
#+begin_admonition
#+begin_export html
<p>There is a security concern to keep in mind when using layer caching. For example, since the <code>RUN</code> command only reruns if the text of the command changes, or a previous layer invalidated the cache, any system package installed will remain the same version even if a security fix has been released. For this reason it is good to occasionally run Docker with <code>--no-cache</code> which will not reuse any layers when building the image.</p>
#+end_export
#+end_admonition

**** Multi-Stage Build

For an Erlang project we are going to need an image with the built release and this image should not contain anything not required for running the release. Tools like Rebar3, the Erlang/OTP version used to build the project, git that was used for fetching dependencies from github, etc all must go. Instead of removing items after the build is complete, a multi-stage Dockerfile can be used to copy the final release, which bundles the Erlang runtime, from the stage it was built to a stage with a Debian base and only the shared libraries necessary to run the release, such as OpenSSL.

We will step through the stages in the =service_discovery= project's [[https://github.com/adoptingerlang/service_discovery/blob/docker-chapter/Dockerfile][Dockerfile]]. The first stage is named =builder=:

#+BEGIN_SRC dockerfile
# syntax=docker/dockerfile:1.2
FROM ghcr.io/adoptingerlang/service_discovery/erlang:26.0.2 as builder

WORKDIR /app/src
ENV REBAR_BASE_DIR /app/_build

RUN rm -f /etc/apt/apt.conf.d/docker-clean

# Install git for fetching non-hex depenencies.
# Add any other Debian libraries needed to compile the project here.
RUN --mount=target=/var/lib/apt/lists,id=apt-lists,type=cache,sharing=locked \
    --mount=type=cache,id=apt,target=/var/cache/apt \
    apt update && apt install --no-install-recommends -y git

# build and cache dependencies as their own layer
COPY rebar.config rebar.lock .
RUN --mount=id=hex-cache,type=cache,target=/root/.cache/rebar3 \
    rebar3 compile

FROM builder as prod_compiled

RUN --mount=target=. \
    --mount=id=hex-cache,type=cache,target=/root/.cache/rebar3 \
    rebar3 as prod compile
#+END_SRC

The =builder= stage starts with the base image =erlang:26.0.2=. =as builder= names the stage so we can use it as a base image to =FROM= in later stages.

#+attr_shortcode: info "Old Docker Caching"
#+begin_admonition
#+begin_export html
<p>For remote caching using the old <code>--cache-from</code>, as described in the previous section,  the <code>builder</code> stage would be built and tagged with an identifier that lets us reference the image based on the Rebar3 dependencies it contains. To do this we can use the command <code>cksum</code> on <code>rebar.config</code> and <code>rebar.lock</code>. This acts similarly to what Docker does before choosing to invalidate its cache or not.</p>

<code><pre>
$ CHKSUM=$(cat rebar.config rebar.lock | cksum | awk '{print $1}')
$ docker build --target builder -t service&#95;discovery:builder-${CHKSUM} .
$ docker push service&#95;discovery:builder-${CHKSUM}
</pre></code>

<p>When building any stage that uses <code>FROM builder</code> we would include <code>--cache-from=service&#95;discovery:builder-${CHKSUM}</code> to pull the previously built dependencies.</p>

<p>It is common for developers to be working on concurrent branches of the same project, potentially with varying dependencies, using a checksum of the current Rebar3 config and lock files when defining the image to use as a cache allows for multiple sets of dependencies for a project to be cached and the correct set to be used when building.</p>
#+end_export
#+end_admonition

The next stage, named =releaser=, uses the =prod_compiled= image as its base:

#+BEGIN_SRC dockerfile
FROM prod_compiled as releaser

WORKDIR /app/src

# create the directory to unpack the release to
RUN mkdir -p /opt/rel

# build the release tarball and then unpack
# to be copied into the image built in the next stage
RUN --mount=target=. \
    --mount=id=hex-cache,type=cache,target=/root/.cache/rebar3 \
    rebar3 as prod tar && \
    tar -zxvf $REBAR_BASE_DIR/prod/rel/*/*.tar.gz -C /opt/rel
#+END_SRC

This stage builds a tarball of the release using the =prod= profile:

#+NAME: rebar.config
#+BEGIN_SRC erlang
{profiles, [{prod, [{relx, [{dev_mode, false},
                            {include_erts, true},
                            {include_src, false},
                            {debug_info, strip}]}]
            }]}.
#+END_SRC

The profile having =include_erts= set to =true= means the tarball contains the Erlang runtime and can be run on a target that doesn't have Erlang installed. At the end the tarball is unpacked to =/opt/rel= so the stage that will copy the release out of the =releaser= stage does not need to have =tar= installed.

#+attr_shortcode: question "Why tar the release at all?"
#+begin_admonition
#+begin_export html
You might notice that a tarball of the release is created only to be untarred immediately. This is done, instead of copying the contents of the release directory, for two reasons. First, it ensures only what is explicitly defined to be in this version of the release is used. This is less important when building in a docker image since no previous release builds will be in the <code>&#95;build/prod/rel</code> directory, but still good to do. Second, there are some changes made to the release when tarring that are required to use tools like <code>release&#95;handler</code>, for example the boot script is renamed from <code>RelName.boot</code> to <code>start.boot</code>. For more details see the <a href='http://erlang.org/doc/man/systools.html#make&#95;tar-1'>systools</a> documentation.
#+end_export
#+end_admonition

Finally, the deployable image uses a regular OS image (=debian:bullseye=) as the base instead of a prior stage. Any shared libraries needed to run the release are installed first and then the unpacked release from the =releaser= stage is copied to =/opt/service_discovery=:

#+attr_latex: :options label=Dockerfile
#+BEGIN_SRC dockerfile
FROM ghcr.io/adoptingerlang/service_discovery/debian:bullseye as runner

WORKDIR /opt/service_discovery

ENV COOKIE=service_discovery \
    # write files generated during startup to /tmp
    RELX_OUT_FILE_PATH=/tmp \
    # service_discovery specific env variables to act as defaults
    DB_HOST=127.0.0.1 \
    LOGGER_LEVEL=debug \
    SBWT=none

RUN rm -f /etc/apt/apt.conf.d/docker-clean

# openssl needed by the crypto app
RUN --mount=target=/var/lib/apt/lists,id=apt-lists,type=cache,sharing=locked \
    --mount=type=cache,id=apt,sharing=locked,target=/var/cache/apt \
    apt update && apt install --no-install-recommends -y openssl ncurses-bin

COPY --from=releaser /opt/rel .

ENTRYPOINT ["/opt/service_discovery/bin/service_discovery"]
CMD ["foreground"]
#+END_SRC

In the =ENV= command we set some useful defaults for environment variables used when the release is run. =RELX_OUT_FILE_PATH=/tmp= is used by the release start script as the directory to output any files created by the script. This is done because when the release is run =sys.config= and =vm.args= need to be generated from their respective =.src= files and by default these are placed in the same directory as the original =.src= files. We do not want these files written to the release directory, where the =.src= files are, because it is a best practice for the container filesystem to not be written to. This image can be run as any user if it writes to =/tmp=, but it would have to run as =root= if it needed to write anywhere under =/opt/service_discovery=. So writing to =/tmp= allows another best practice to be followed, not running the container as =root=. We can go even further and make the runtime filesystem read-only, we will see that in [[Running a Container]].

=/opt/service_discovery= is owned by root and it is recommended to not run the container as root.   If =RELX_OUT_FILE_PATH= is set, its location will be used instead. Here, the =ENV= command is used to ensure the environment variable =RELX_OUT_FILE_PATH= is set to =/tmp= when the container is run.

#+BEGIN_SRC shell
$ docker buildx build -o type=docker --target runner --tag service_discovery:$(git rev-parse HEAD) .
#+END_SRC

Or using the script =service_discovery= contains for building and pushing images from CircleCI:

#+BEGIN_SRC shell
ci/build_images.sh -l
#+END_SRC

This script will also tag the image twice, once with the git ref, =git rev-parse HEAD=, as done in the manual command, and again with the name of the branch, =git symbolic-ref --short HEAD=. The branch tag is used referencing as a build manifest cache with =--cache-from=. The script will use an image tagged for the =master= branch and the current branch as caches when they are available, and to do so must include =--cache-to=type=inline= in the build command.

Using the current branch name and =master= as the images to inspect for cache hits is less exact than the use of a checksum of =rebar.config= and =rebar.lock= on an image only containing the stage that builds dependencies. There is nothing stopping a build from still making and pushing an explicit image tagged with the checksum and using it also as one of the =--cache-from= images. But, at least with this project, the ease of not needing to juggle additional images, since the Buildkit cache manifest keeps track of all stages, outweighs the potential for a cache miss on dependencies where one wouldn't happen with the other scheme.

Lastly, a difference to note with how the script is used in CircleCI, see [[Building and Publishing Images in CI]], compared to here is the =-l= option. In CI we only care about getting the image to the remote registry, so time can be saved by not loading the built image in to the Docker daemon. When building images locally we likely want to then run it, and we will in the next section [[Running a Container]], so loading into the Docker daemon is necessary.

*** Running a Container

Now that we have the image, =docker run= can be used to start the release for local verification and testing. By default, the =CMD=, =foreground=, is passed to the release start script, configured through =ENTRYPOINT= as =/opt/service_discovery/bin/service_discovery=. =CMD= can be overridden if with the last argument passed to =docker run=. Using the =console= command results in an interactive shell when the container is run:

#+BEGIN_SRC shell
$ docker run -ti service_discovery console
[...]
(service_discovery@localhost)1>
#+END_SRC

The =-ti= options tell =docker= we want an interactive shell. This is useful for local testing of the image where you want a shell for inspecting the running release. The default, set by =CMD= in the Dockerfile =runner= stage will use =foreground=. There is no use for =-ti= here so they can be dropped as well and the command is simply:

#+BEGIN_SRC shell
$ docker run service_discovery
Exec: /opt/service_discovery/erts-10.5/bin/erlexec -noshell -noinput +Bd -boot /opt/service_discovery/releases/8ec119fc36fa702a8c12a8c4ab0349b392d05515/start -mode embedded -boot_var ERTS_LIB_DIR /opt/service_discovery/lib -config /tmp/sys.config -args_file /tmp/vm.args -- foreground
Root: /opt/service_discovery
/opt/service_discovery
#+END_SRC

To prevent accidental shutdown you will not be able to stop this container with =Ctrl-c= so to stop the container use =docker kill <container id>=.

Note that =foreground= is the default because this is how it should be run in production, though it would be in the background:

#+BEGIN_SRC shell
$ docker run -d service_discovery
3c45b7043445164d713ab9ecc03e5dbfb18a8d801e1b46e291e1167ab91e67f4
#+END_SRC

Running with =-d= is short for =--detach= and the output is the container id. Logs that are written to =stdout= will be viewable with =docker log <container id>= and we will see in the [[/docs/production/kubernetes][next chapter]] how in Kubernetes the logs can be routed to your log store of choice. And even when

The running node can also be attached to with =docker exec=, the container id (seen in the output of =docker run -d= or use =docker ps= to find this) and command =remote_console=. It doesn't matter if the container was started with =console= or =foreground=, but this is, of course, most useful when you need a shell for a node you didn't already start with =console=. Because =exec= does not use the =ENTRYPOINT= defined in the image the command to run must start with the release start script, =bin/service_discovery=:

#+BEGIN_SRC shell
$ docker exec -ti 3c45 bin/service_discovery remote_console
[...]
(service_discovery@localhost)1>
#+END_SRC

Alternatively, a Linux shell could be run with =docker exec -ti 3c45 /bin/sh=, which will drop you into =/opt/service_discovery= where you could then connect with a =remote_console= or inspect other aspect of the running container.

To exit the remote console do not run =q().=, this will shutdown the Erlang node and the Docker container. Use =Ctrl-g= and enter =q=. =Ctrl-g= enters the shell into what is called the Job Control Mode. To read more about how this shell mode can be used see the [[http://erlang.org/doc/man/shell.html#jcl-mode][Job Control Mode (JCL Mode) documentation]].

In some cases, such as if the release will not boot, it can be useful to override the =ENTRYPOINT= and get a shell in a container that will attempt to boot the release:

#+BEGIN_SRC shell
$ docker run -ti --entrypoint /bin/sh service_discovery
/opt/service_discovery #
#+END_SRC

Lastly, in the previous section we saw how =RELX_OUT_FILE_PATH= was set to =/tmp= so no files would attempt to write to the release directory, which should remain read-only. Docker has a =diff= command that can will show the difference between the image filesystem and the current running container's filesystem:

#+BEGIN_SRC shell
$ docker container diff 3c45
C /tmp
A /tmp/sys.config
A /tmp/vm.args
#+END_SRC

This can be a useful command for easily inspecting what your release is writing to disk if you run into problems or want to verify you release is not doing something it shouldn't. And in cases where there are a lot of writes to disk from the release it is best to mount a [[https://docs.docker.com/storage/volumes/][volume]] and point all writes to it, but for the 2 small configuration files this is not necessary. Unless, when creating the config files from the templates there is sensitive data being used or you want to run the container with =--read-only=. In those cases a [[https://docs.docker.com/storage/tmpfs/][tmpfs]] mount is recommended. On Linux simply add =--tmpfs /tmp= to the =docker run= command. Then =/tmp= will not be part of the writable layer of the container but a separate volume that only exists in memory and is destroyed when the container is stopped.

#+attr_shortcode: danger "Beware Zombies!"
#+begin_admonition
#+begin_export html
<p>As of Erlang/OTP 19.3 an Erlang node will gracefully shutdown with <code>init:stop()</code> when a <code>TERM</code> signal is received, as is used by Docker and Kubernetes for shutting down containers.</p>

<p>But there are still potential issues you will encounter with zombies in a container. When using <code>docker exec</code> to run <code>remote&#95;console</code>, or any other release script command like <code>ping</code>, will leave zombie processes behind unless the container is started with the argument <code>--init</code> that Docker offers for starting a small init before your entrypoint.</p>

<p>This is usually not an issue, but like with atoms, it certainly can be if the uses aren't limited. An example of something to avoid for this reason, unless running with <code>--init</code> or some other tiny init as pid 0, would be using <code>ping</code> as a health check which is run periodically during the life of a container by the container runtime. A long running container like this will eventually have the kernel process table run out of slots and it will not be possible to create new processes.</p>
#+end_export
#+end_admonition

*** Building and Publishing Images in CI

Since it would be tedious to manually build and publish images to the registry for each release, it is common to include image building as part of the Continuous Integration process. Usually this is restricted to only occur on a merge to master or a new tag being created, but it can sometimes be useful to also build branch images for testing purposes. In this section we will cover a couple options for automating this process, but whatever CI tool you are already using will be capable of something similar.

**** CircleCI

In the [[/docs/development/testing][Testing]] chapter (coming soon...) we covered [[https://circleci.com][CircleCI]] for running tests. To build and publish Docker images for =service_discovery= a new job is added named =docker-build-push=. It uses a VM instead of a Docker image as the executor and first installs the latest Docker version, at the time of writing this the version available by default does not support features used in the =service_discovery= =Dockerfile=.

#+BEGIN_SRC yaml
jobs:
  docker-build-and-push:
    executor: docker/machine
    steps:
      - run:
          name: Install latest Docker
          command: |
            sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
            sudo apt-get update

            # upgrade to latest docker
            sudo apt-get install docker-ce
            docker version

            # install buildx
            mkdir -p ~/.docker/cli-plugins
            curl https://github.com/docker/buildx/releases/download/v0.3.0/buildx-v0.3.0.linux-amd64  --output ~/.docker/cli-plugins/docker-buildx
            chmod a+x ~/.docker/cli-plugins/docker-buildx
      - checkout
      - gcp-gcr/gcr-auth
      - run:
          name: Build and push images
          command: |
            ci/build_image.sh -p -t runner -r gcr.io/adoptingerlang
#+END_SRC

After installing the latest Docker the code =service_discovery= repository is checked out and since this is using Google Cloud for the registry and Kubernetes it authenticates with the registry. Lastly, a script found in the =ci/= directory of =service_discovery= is called to build and publish the images. The script uses the =docker build= commands discussed earlier in the chapter to build the individual stages and use =--cache-from= to reference the stages as caches during each build.

To run this job only after tests pass it can be added to the CircleCI workflow with a =requires= constraint on =rebar3/ct=.

#+BEGIN_SRC yaml
workflows:
  build-test-maybe-publish:
    jobs:

      [...]

    - docker-build-and-push:
        requires:
        - rebar3/ct
#+END_SRC

**** Google Cloud Build

In 2018 Google released a image build tool [[https://github.com/GoogleContainerTools/kaniko][Kaniko]] that runs in userspace and does not depend on a daemon, these features enable container image building in environments like a Kubernetes cluster. Kaniko is meant to be run as an image, =gcr.io/kaniko-project/executor=, and can be used as a =step= in Google Cloud Build.

Kaniko offers remote caching for each layer created by a =RUN= command. Builds check the layer cache in the image registry for matches before building any layer. However, the Buildkit features we used in the =service_discovery= Dockerfile are not available in Kaniko, so a separate Dockerfile, =ci/Dockerfile.cb=, is used in the Google Cloud Build configuration, =cloudbuild.yaml=:

#+BEGIN_SRC yaml
steps:
- name: 'gcr.io/kaniko-project/executor:latest'
  args:
  - --target=runner
  - --dockerfile=./ci/Dockerfile.cb
  - --build-arg=BASE_IMAGE=$_BASE_IMAGE
  - --build-arg=RUNNER_IMAGE=$_RUNNER_IMAGE
  - --destination=gcr.io/$PROJECT_ID/service_discovery:$COMMIT_SHA
  - --cache=true
  - --cache-ttl=8h

substitutions:
  _BASE_IMAGE: gcr.io/$PROJECT_ID/erlang:22
  _RUNNER_IMAGE: gcr.io/$PROJECT_ID/alpine:3.10
#+END_SRC

Because Kaniko works by checking for each instruction in a cache directory of the registry there is no need for instructing it to use a specific image as a cache like we did with Docker's =--cache-from=. Docker's build cache can act more like Kaniko's by setting it to export the cache metadata to the registry and to export layers for all stages, =--cache-to=type=registry,mode=max=, but this is not supported by most registries at the time of this being written so is not covered.

For more details on using Kaniko in Google Cloud Build see their [[https://cloud.google.com/cloud-build/docs/kaniko-cache][Using Kaniko cache]] documentation.

*** Next Steps

In this chapter we built images for our service and finished off by creating a CI pipeline for continually building and publishing those images when changes are made to the repository. In the next chapter we will cover how to build a deployment to Kubernetes from these images. After we are running in Kubernetes the following chapter covers observability, such as connecting to a running node, well structured logs, reporting metrics and distributed traces.


#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/production/releases/">← Prev</a></div>
  <div><a href="/docs/production/kubernetes/">Next →</a></div>
</div>
#+END_EXPORT

** DONE Kubernetes
CLOSED: [2020-02-04 Tue 10:21]
:PROPERTIES:
:EXPORT_FILE_NAME: kubernetes
:EXPORT_HUGO_MENU: :menu main :parent production
:END:

Kubernetes is a container orchestration system. When a backend contains many distinct services that need to be kept running across nodes, with cross communication and scaling, an orchestration system becomes essential. And with the growing popularity of containers, using Kubernetes for orchestration has grown in popularity as well.

With containers and Kubernetes we have common methods of deployment, discovery, scaling and monitoring across all the languages and runtimes within our system. In this chapter we will go over the core components of Kubernetes and how to use them to deploy and manage the =service_discovery= service we built in the previous chapters.

*** Running Kubernetes

In order to develop and test the Kubernetes deploy, we will need to run
Kubernetes locally for testing purposes. The following sections were written
with [[https://kind.sigs.k8s.io][kind]] as the local test Kubernetes but should work on any of the local or
cloud options for running Kubernetes. The tool [[https://github.com/tilt-dev/ctlptl][ctlptl]] is used for managing the
test Kubernetes clusters in this text. It supports multiple bankends, making it
even easier to swap out the one used here for your preferred solution.
Instructions for Continuous Integration and deployment to production will use
Google Kubernetes Engine. But the majority of the content should work easily
with any of the offerings in the following two sections.

**** Locally

- [[https://kind.sigs.k8s.io/][kind]]: Kubernetes in Docker is a projected backed by a [[https://github.com/kubernetes-sigs][Kubernetes SIG]] for running Kubernetes in Docker originally for testing Kubernetes itself.
- [[https://microk8s.io/][microk8s]]: An offering from Canonical, the company behind Ubuntu, but runs on many [[https://snapcraft.io/docs/installing-snapd][Linux distros]], [[https://multipass.run/#install][Windows]] and [[https://multipass.run/#install][MacOS]]. microk8s is simple to install and get started with, and it runs locally, not within a VM like minikube, so is less of a resource hog.
- [[https://github.com/kubernetes/minikube][minikube]]: The oldest and most flexible option, that is also now an official part of the Kubernetes project. =minikube= offers support for various hypervisors and has an option to run without creating a new virtual machine, but it still suggests doing so within a Linux VM.
- [[https://k3s.io/][k3s]] and [[https://github.com/rancher/k3d][k3d]]: k3s is lightweight Kubernetes from [[https://rancher.com/][Rancher Labs]]. k3s removes legacy and non-default features to cut down on size and replaces etcd with SQLite3, making k3s a good option for CI and local testing. k3d is a helper for running k3s in Docker.
- [[https://docs.docker.com/docker-for-mac/#kubernetes][Docker for Mac
  Kubernetes]]: The easiest to get going with when running on MacOS.
- [[https://docs.docker.com/desktop/kubernetes/][Docker for Windows]]: The easiest to get going with when running on Windows.

All of these, minus =microk8s=, are supported by =ctlptl=.

**** Production

All the big cloud providers offer managed Kubernetes clusters:

- [[https://cloud.google.com/kubernetes-engine/][Google Kubernetes Engine]]:
  Google Cloud was used for this chapter when not using =kind=, because it
  offers [[https://cloud.google.com/free/][$300 of credit when signing up as a
  new user]]
- [[https://www.digitalocean.com/products/kubernetes/][Digital Ocean Kubernetes]]
- [[https://aws.amazon.com/eks/][AWS Elastic Container Service for Kubernetes]]
- [[https://azure.microsoft.com/en-us/services/kubernetes-service/][Azure Kubernetes Service]]

There are [[https://kubernetes.io/docs/setup/pick-right-solution/#table-of-solutions][many more options]] for deploying to the cloud or on-premise. Many factors go into play when choosing a solution, like in the case of an existing company, where are your services currently being hosted. Luckily with Kubernetes your deployment will not be locked in to any one provider.

*** Deployment

In Kubernetes, containers are part of a =Pod=. Each =Pod= has 1 or more containers, and 0 or more =init-containers= which run once to completion before the other containers are started. For an application, a higher level abstraction called a =Deployment= is used, so individual =Pods= don't have to be manually created for deploying or scaling. A =Deployment= is a declarative way to create and update the =Pods= of an application.

Each Kubernetes resource is defined in a =yaml= file containing the =apiVersion=, the =kind= of resource, =metadata=, such as a name, and the resource's specification:

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-discovery
spec:
[...]
#+END_SRC

The =Deployment= spec entries we will cover here are =selector=, =replicas=, and =template=. The =selector= is a specification for which =Pods= belong to a =Deployment=. =matchLabels= means that =Pods= with the label =app= with value =service-discovery=, within the same =Namespace=, will be considered part of the =Deployment=. =replicas= declares how many instances of the =Pod= should be running.

#+BEGIN_SRC yaml
spec:
  selector:
    matchLabels:
      app: service-discovery
  replicas: 1
  template:
[...]
#+END_SRC

The =template= section is a =Pod= template and defines the specification of the =Pods= to be run by the =Deployment=:

#+BEGIN_SRC yaml
template:
  metadata:
    labels:
      app: service-discovery
  spec:
    shareProcessNamespace: true
    containers:
    - name: service-discovery
      image: service_discovery
      ports:
      - containerPort: 8053
        protocol: UDP
        name: dns
      - containerPort: 3000
        protocol: TCP
        name: http
      - containerPort: 8081
        protocol: TCP
        name: grpc
#+END_SRC

First, the =Pod= metadata sets the label to match with the =selector= from the =Deployment= spec -- in the upcoming section [[Using Kustomize to Simplify Deployment]] we will see how the need to define the =labels= twice is removed. Then, the =Pod= spec has a list of containers. In this case there is one container that exposes ports for accessing through DNS, HTTP and GRPC.

#+attr_shortcode: danger "The Zombie Killer!"
#+begin_admonition
#+begin_export html
<p>The issue of zombie processes (processes who exit but their PID sticks around because the parent hasn't called <code>wait()</code>) discussed in the <a href="/docs/production/docker/#running-a-container">Docker chapter</a> applies to running containers with Kubernetes as well. One way to protect against them, and the one used in the <code>service&#95;discovery</code> project is to use a shared process namespace between containers in the Pod.</p>

<p>This is done with <code>shareProcessNamespace: true</code> in the Pod spec. This setting means the process started in the container will not be PID 1. Instead, PID 1 is the Kubernetes <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">pause container</a>. The pause container is always the parent container in a Pod but with this setting, making it PID 1 for all processes in the Pod, it is able to reap zombies processes that might result from any container in the Pod.</p>

<p>Read more about <a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/">shared process namespaces in the Kubernetes docs</a>.</p>
#+end_export
#+end_admonition

**** Container Resources

Each container spec in the =Pod= spec can include resource requests and limits for memory and CPU. Requests are used to schedule the pod. Scheduling involves choosing a node that the requested CPU and memory is available (not already requested by other Pods on the node).

#+BEGIN_SRC yaml
resources:
  requests:
    memory: "250Mi"
    cpu: "500m"
  limits:
    memory: "1Gi"
    cpu: "2000m"
#+END_SRC

Additionally, CPU requests are translated to the cgroup property =cpu.shares=. Each CPU is considered 1024 slices and a CPU request tells the kernel how many of those slices to try to give to the process. But there is no upper bound to how much a process can ultimately take when only setting the shares. For throttling a process that is using too much CPU time we need the Kubernetes resource limits.

The limits are translated to CPU bandwidth controls for the Linux kernel scheduler to enforce on the process. Bandwidth control has a period, which is some number of microseconds, and a quota, the maximum number of microseconds a process can use within a period. The period is always 100000 microseconds, so in the case of the example above with CPU limit set to =2000m= the cgroup quota set will be =200_000=, meaning 2 CPUs can be used every =100_000= microseconds.

If the limit is exceeded during a period, the kernel will throttle the process by not allowing it to run again until the next period. This is why it is important to correctly set the number of Erlang VM schedulers that are used and to limit the amount of busy waiting done by the VM.

A busy wait is a tight loop an Erlang scheduler will enter waiting for more work to do before eventually going to sleep. This tight loop burns CPU just waiting to do actual work and can lead to much worse performance because your Erlang program will be likely to get throttled by the kernel scheduler. Then when there is actual work to be done, it may happen in a period the Erlang VM gets no CPU slices at all. This will be even worse if the VM is running more schedulers than CPUs it is allocated. The CPU limit of =2000m=, which we think of as meaning 2 CPU cores, does not actually restrict the process to 2 cores. If 8 schedulers are being used and there are the same number of cores on the node, those 8 will still spread across all the cores and run in parallel. But the quota is still =200000= and more likely to be exceeded when 8 schedulers are taking time on 8 cores. Even without the busy wait, a scheduler must do work of its own and can be unnecessary overhead when trying to stay within some CPU usage constraints.

In order to disable the scheduler busy waiting we set the VM arguments =+sbwt= in =vm.args.src= as shown here:

#+BEGIN_SRC shell
+sbwt ${SBWT}
#+END_SRC

#+attr_shortcode: note "As of OTP-23"
#+begin_admonition
#+begin_export html
<p>With OTP-23, released in 2020, the Erlang VM is "container aware" and will s
the proper number of active schedulers automatically based on the containers
allocated resources. Before OTP-23 the <code>+S</code> argument was needed to
set the number of active schedulers equal the container's CPU limit.</p>

<p>Additionally, <code>+sbwt</code> defaults to <code>very&#95;short</code>
since OTP-23. This is an improvement but it is likely you still want to set the value to <code>none</code> when running in Kubernetes, or similar environment. But, as always is the case, be sure to benchmark to find the optimal value for your particular workload.</p>
#+end_export
#+end_admonition

**** Container Environments and ConfigMaps

As we've seen in the Releases chapter, runtime configuration is done through environment variable substitution in =vm.args.src= and =sys.config.src=. We therefore have to insert those variables to the environment of the container. Each container in a Kubernetes Pod can have an =env= field declaring a set of environment variables. The simplest case is explicitly providing a =name= and a =value=:

#+BEGIN_SRC yaml
env:
- name: LOGGER_LEVEL
  value: error
- name: SBWT
  value: none
#+END_SRC

This configuration would result in the environment variable =LOGGER_LEVEL= with value =error=.

There are other environment variables that must be set based on the state of the container that is running. An example of this is setting the =NODE_IP= variable based on the IP of the Pod:

#+BEGIN_SRC yaml
env:
- name: NODE_IP
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
#+END_SRC

The =status.podIP= declaration under =fieldRef= will return the current Pod's IP when it is created.

The user-defined environment variables like =LOGGER_LEVEL= can be better tracked in a Kubernetes resource specifically for configuration called a =ConfigMap=. A =ConfigMap= contains key-value pairs and can be populated from files, directories of files or literal values. Here we will use a literal value to set =LOGGER_LEVEL= to =error=:

#+BEGIN_SRC yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap
data:
  LOGGER_LEVEL: error
#+END_SRC

Then back in the =Deployment= resource the =LOGGER_LEVEL= value can be made a reference to the =ConfigMap=:

#+BEGIN_SRC yaml
env:
- name: LOGGER_LEVEL
  valueFrom:
    configMapKeyRef:
      name: configmap
      key: LOGGER_LEVEL
#+END_SRC

A shortcut for bringing in all the variables defined in a ConfigMap as environment variables for the container can be done with =envFrom=:

#+BEGIN_SRC yaml
envFrom:
- configMapRef:
    name: configmap
#+END_SRC

Now all the variables defined in the ConfigMap will be added to the container without having to individually specify each one. For more on defining ConfigMaps see the upcoming section [[Using Kustomize to Simplify Deployment]].

#+attr_shortcode: tip
#+begin_admonition
#+begin_export html
Updating a value in a <code>ConfigMap</code> will <b>not</b> trigger the <code>Deployments</code> that have references to the <code>ConfigMap</code> to restart their containers with the new environment variables. Instead, the way to update the containers when the configuration changes is to create a new <code>ConfigMap</code> with the necessary changes and a new name, then modify the name of the <code>ConfigMap</code> referenced by the <code>Deployment</code> under <code>configMapRef</code>. The old <code>ConfigMaps</code> will eventually be garbage collected by Kubernetes since they are not referenced anywhere and the <code>Deployment</code> will recreate its <code>Pods</code> with the new configuration.
#+end_export
#+end_admonition

**** Init Containers

Our release of =service_discovery= depends on a Postgres database for storing state, to ensure the database is properly setup before running our containers we will use an [[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/][Init Container]]. Each =Init Container= is a container that runs to completion before any of the main containers in a Pod are run. The database migration tool =flyway= we are using for running migrations is used to validate the database is up to date before allowing the main containers to run:

#+BEGIN_SRC yaml
volumes:
- name: migrations
  emptyDir:
    medium: Memory

initContainers:
- name: flyway
  image: flyway/flyway:9.22
  name: flyway-validate
  args:
  - "-url=jdbc:postgresql://$(POSTGRES_SERVICE):5432/$(POSTGRES_DB)"
  - "-user=$(POSTGRES_USER)"
  - "-password=$(POSTGRES_PASSWORD)"
  - "-connectRetries=60"
  - "-skipCheckForUpdate"
  - validate
  volumeMounts:
  - name: migrations
    mountPath: /flyway/sql
    env:
    - name: POSTGRES_SERVICE
      value: POSTGRES_SERVICE
    - name: POSTGRES_DB
      value: POSTGRES_DB
    - name: POSTGRES_USER
      value: POSTGRES_USER
    - name: POSTGRES_PASSWORD
      value: POSTGRES_PASSWORD

- name: service-discovery-sql
  image: service_discovery
  command: ["/bin/sh"]
  args: ["-c", "cp /opt/service_discovery/sql/* /flyway/sql"]
  volumeMounts:
    - name: migrations
      mountPath: /flyway/sql
#+END_SRC

The volume =migrations= exists only in memory and is used to copy the current SQL migration files from the release image to a shared directory that the =flyway= container will use to run validation. The =emptyDir= type of volume with =medium= =memory= is used because we are not using this volume to persist any data, only to share between two =Init Containers=. If =medium: Memory= was not included, the =emptyDir= is created on the host file system but it is still empty initially for each new =Pod=, and is removed when the =Pod= is deleted.

**** Readiness, Liveness and Startup Probes

Each container in a =Pod= can define a =readiness=, a =liveness=, and a =startup= probe.
Probes are defined either as a command to execute in the container, as an HTTP
=GET= request made to a given port and path, or as an attempt to open a TCP
connection to a specific port.

While kubernetes will restart any failed container, there are some times where a
process becomes unresponsive, such as deadlocks. A =livenessProbe= can be
used for such cases, where stopping to respond prompts a restart. For this
reason if a =livenessProbe= is included it should be kept extremely simple. It is
more often than not safer to leave out the =livenessProbe= entirely and rely on
the node crashes, metric based alerting and auto-scaling.

One part of auto-scaling is based on the =readinessProbe=. The =readinessProbe=
tells Kubernetes the =Pod= is ready to receive traffic. When a =Pod= becomes =unready=
because a container is failing the =readinessProbe=, it is removed from the
=Service= resulting in traffic being directed at the remaining =Pods=, increasing
their load, then a new =Pod= will be started.

Not using the =livenessProbe= for this purpose means the containers are still
available, since they aren't killed, and the reason for failing the check can be
examined. This may mean attaching a shell if it isn't completely frozen or
forcing a crash dump to be written. In a very resource constrained environment
you may prefer to have the =livenessProbe= in order to have deadlocked containers
and their =Pods= cleaned up right away, instead of waiting for manual intervention
and the auto-scaler to scale down after load falls.

Another use for the =readinessProbe= is if the application has to periodically do work
where you don't want to be handling requests, or have to do some sort of
maintenance, it can return a =503= status response for readiness causing it to be
removed from the =Service= backends.

In the =Pod= configuration nothing special is needed to have a graceful shutdown,
whether it is during a deploy, =livenessProbe= failure or scale down. Erlang will
call =init:stop()= when =SIGTERM= is sent by Kubernetes to signal shutdown. As
discussed in [[Releases]], each Applications will be stopped in the reverse they
were started and each Application's supervision tree terminates children in
reverse order.

By default the process can wait up to 30 seconds (configurable with the
=terminationGracePeriodSeconds= =PodSpec= option) before Kubernetes sends a =SIGKILL=
signal that will force the process to terminate.

An important note about deployments and =readinessProbe= is the behaviour if a
=readinessProbe= never passes. During deployment if the =Pod= never passes its
=readinessProbe= then, depending on the deployment strategy, =Pods= from the
previous version of the deployment that are =Ready= may remain until they do. This
is covered more in the follow section [[Rolling Deployments]].

In the =service_discovery= project only =readinessProbe= is defined for the purpose
of removing the =Pod= from =Service= backends without killing the container:

#+BEGIN_SRC yaml
readinessProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
#+END_SRC

With this configuration we expect the initial check to fail at least once
because the actual startup time before the =service_discovery_http= Application
has bound to a port to listen for HTTP requests is more than =0= seconds. This
will result in a =connection refused= error handled by the =Probe=:

#+BEGIN_SRC
service-disc… │ [event: pod service-discovery-dev/service-discovery-dev-645659d699-plxfw] Readiness probe failed: Get "http://10.244.0.114:3000/ready": dial tcp 10.244.0.114:3000: connect: connection refused
#+END_SRC

Despite failing and =periodSeconds= being =10= it likely won't take 10 seconds for
the =Pod= to be ready. With an =initialDelaySeconds= of =0= the =readinessProbe= is
checked immediately during startup and does not wait the configured
=periodSeconds= to check a second time. Because the service does little on startup
it will pass the second check. If it doesn't it will then wait the =periodSeconds=
until it is checked again and the =Pod= becomes ready.

To be as simple as possible, the =readinessProbe= is defined in =service_discovery_http= as:

#+BEGIN_SRC erlang
handle('GET', [<<"ready">>], _Req) ->
    {ok, [], <<>>};
#+END_SRC

Meaning it matches on a =GET= request to =/ready= and returns a =200= response
immediately. There are no additional checks of functionality of the service
except that it is able to receive and respond to HTTP requests.

**** Rolling Deployments

Kubernetes =Deployment= resources have a configurable [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy][strategy]] for what happens when a new version of an image is deployed. The =service_discovery= =Deployment= uses a rolling update strategy. An alternative strategy is =Recreate= which means Kubernetes first terminates all =Pods= in the =Deployment=, and then starts the new =Pods=. For the =RollingUpdate= strategy the Kubernetes =Deployment= controller will bring up new =Pods= and then terminate the old. There are two configuration variables to tell the controller how many =Pods= above the desired amount are allowed (=maxSurge=) and how many are allowed to be unavailable during the deploy (=maxUnavailable=).

#+BEGIN_SRC yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0
    maxSurge: 25%
#+END_SRC

=service_discovery= is configured with a =maxUnavailable= of =0= and =maxSurge= of =25%=. This means that if it has been scaled up to four replicas then during deployment one new =Pod= will be started, when the =readinessProbe= for that =Pod= is passing one of the old =Pods= will begin termination and a new =Pod= will be started. This process will continue four times.

*** Service

A =Service= is a resource for defining how to expose an application on the network. Each =Pod= has its own IP address and can expose ports. A =Service= provides a single IP address (the =Service's= =ClusterIP=) and can map a port on that IP to the exposed port for each =Pod= of the application. In the following resource definition a =Service= named =service-discovery= is created with a selector of =app: service-discovery= to match the =Pods= created in the previous section:

#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: service-discovery
spec:
  selector:
    app: service-discovery
  ports:
  - name: dns
    protocol: UDP
    port: 8053
    targetPort: dns
  - name: http
    protocol: TCP
    port: 3000
    targetPort: http
  - name: grpc
    protocol: TCP
    port: 8081
    targetPort: grpc
#+END_SRC

The =Deployment's= container exposed three ports and gave each one a name, =dns=, =http= and =grpc=. The =Service= uses those names as the =targetPort= for each port it exposes. With this resource applied to a Kubernetes cluster there will be an IP listening on those three ports that will proxy to one of the running =Pods=.

A proxy (=kube-proxy=) is used to route traffic to =Pods= instead of adding each =Pod= IP to a DNS record because of the rate of change for what =Pods= are actually running for a =Service=. If DNS were used it would require a low or zero time to live (TTL) and depend on the client to fully respect the TTL value. Having every request be routed through a proxy means as long as the proxy's data has been updated the right set of =Pod's= will be routed to.

Kubernetes-aware DNS services, like [[https://coredns.io/][CoreDNS]], will watch for new =Services= and create DNS records =[service-name].[namespace]= for each service. When using named ports, like the three ports in the =service-discovery= =Service=, there are also SRV records created of the form =_[name]._[protocol].[service-name].[namespace]=. Querying the DNS service for the SRV record, such as =_http._tcp.service-discovery.default= will return the =Service= DNS name =service-discovery.default= and port 3000.

In a later section on clustering we will see how to use a =Headless Service= (a =Service= with =ClusterIP= set to =None=) resource and named ports for connecting Erlang nodes over distributed Erlang without the [[http://erlang.org/doc/man/epmd.html][Erlang Port Mapper Daemon (epmd)]].

*** Using Kustomize to Simplify Deployment

Now that we have talked about the two main Kubernetes resources used for our application we can go into how we would actually write and deploy these resources. The reason the YAML shown in the previous sections for each resource is not used for deployment is that it is static, at times repetitive (like having to define the same set of labels in a =Deployment= twice) and requires manually modifying or having duplicates of the resources for deploying to different environments which require different configuration. We need something that makes it easy and clear to do changes like updating the image used in a container, using a different name or namespace for resources depending on the environment or using different values for a =ConfigMap= depending on the environment.

There are a few open source options for working with Kubernetes resources. The two most popular are [[https://helm.sh/][Helm]] and [[https://kustomize.io/][Kustomize]]. Both are part of the Kubernetes project but have very different solutions to the problem. Helm uses [[https://golang.org/pkg/text/template/][Go templates]] for writing and rendering YAML. Some, this author included, find templating YAML to be overly complex, error prone and just plain annoying. Luckily, the other solution, =Kustomize,= does not rely on templates.

#+attr_shortcode: note "Helm 3"
#+begin_admonition
#+begin_export html
<p>Even if Helm is not used for deployment of your project itself it can still be useful for deployment of dependencies. There are a lot of <a href="https://github.com/helm/charts">Helm Charts</a> available and is likely how you would want to offer your project to the outside world if it was not simply for internal usage.</p>

<p>There were a number of issues with Helm aside from the templating that we won't get into here, except to say that they are being worked out in the latest major release, <a href="https://v3.helm.sh/">Helm v3</a>, that had its first stable release in November 2019. So I would still suggest taking a look at the latest offering from Helm for yourself.</p>
#+end_export
#+end_admonition

[[https://kustomize.io/][Kustomize]] is a =kubectl= built-in tool (as of v1.14.0) that provides a template-free way to customize Kubernetes YAML resources. We will use it to create different configurations for various environments, starting with a development environment, =dev=, which will then be used in the next section, [[Tilt for Local Development]], to run =service_discovery= locally.

The setup is a base configuration with overlays for different environments. Overlays can add additional resources and make modifications to the resources from the base layer. The directory layout for the base configuration and two overlays is:

#+BEGIN_SRC shell
$ tree deployment
deployment
├── base
│   ├── default.env
│   ├── deployment.yaml
│   ├── init_validation.yaml
│   ├── kustomization.yaml
│   ├── namespace.yaml
│   └── service.yaml
├── overlays
│   ├── dev
│   │   ├── dev.env
│   │   └── kustomization.yaml
│   └── stage
│       ├── kustomization.yaml
│       └── stage.env
└── postgres
    ├── flyway-job.yaml
    ├── kustomization.yaml
    ├── pgdata-persistentvolumeclaim.yaml
    ├── postgres-deployment.yaml
    └── postgres-service.yaml
#+END_SRC

The base =kustomization.yaml= includes the main resources of the =service_discovery= project, a =Namespace=, =Deployment= and =Service=:

#+BEGIN_SRC yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: service-discovery
commonLabels:
  app: service-discovery
resources:
- namespace.yaml
- deployment.yaml
- service.yaml
#+END_SRC

The labels under =commonLabels= will be added to each resource and simplifies the =Deployment= configuration from the earlier section by being able to remove both the =labels= entry and the =selector=. So now the =Deployment= resource, found in =deployment/base/deployment.yaml= for =service_discovery= looks like:

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-discovery
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: service-discovery
        image: service_discovery
[...]
#+END_SRC

Kustomize will insert the labels under the =metadata= when rendering the resources, as well as inserting the same labels under the =spec='s =selector= field =matchLabels= automatically. To see what Kustomize generates run =kubectl kustomize= on the base and it will print the resources to stdout:

#+BEGIN_SRC yaml
$ kubectl kustomize deployment/base
[...]
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: service-discovery
  name: service-discovery
  namespace: service-discovery
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-discovery
  template:
    metadata:
      labels:
        app: service-discovery
[...]
#+END_SRC

Another feature for improving creation of =Deployments= is generation of =ConfigMaps=. In the previous section on environment variables and ConfigMaps we ended with a =Deployment= that included environment variables from the data in a =ConfigMap=:

#+BEGIN_SRC yaml
envFrom:
- configMapRef:
    name: configmap
#+END_SRC

With Kustomize's =configMapGenerator= we can declare that the =ConfigMap= be generated from any file with lines like =VarName=VarValue=:

#+BEGIN_SRC yaml
configMapGenerator:
- name: configmap
  envs:
  - dev.env
#+END_SRC

The content of =dev.env= is:

#+BEGIN_SRC shell
LOGGER_LEVEL=debug
#+END_SRC

The resulting ConfigMap seen in the output for =kubectl kustomize deployment/overlays/dev= is:

#+BEGIN_SRC yaml
apiVersion: v1
data:
  LOGGER_LEVEL: debug
kind: ConfigMap
metadata:
  labels:
    overlay: dev
  name: configmap-dev-2hfc445577
  namespace: service-discovery-dev
#+END_SRC

Note that the name is no longer simply =configmap= but instead =configmap-dev-2hfc445577=. Kustomize will create a new =ConfigMap= with a different name if the content changes and it will update any reference to the =ConfigMap=. Updating the reference to the =ConfigMap= in the =Deployment= spec ensures that =Pods= are restarted when their configuration changes. Simply modifying a =ConfigMap= would not otherwise result in an update to the running =Pods=. So in the Kustomize output the =Deployment= will have:

#+BEGIN_SRC yaml
- envFrom:
  - configMapRef:
      name: configmap-dev-2hfc445577
#+END_SRC

The same is done for =Secrets=.

To apply the resources generated by Kustomize in one step the =-k= option can be passed to =kubectl apply=:

#+BEGIN_SRC shell
$ kubectl apply -k deployment/overlays/dev
#+END_SRC

This command will generate the resources based on the =dev= overlay and apply them to the Kubernetes cluster.

*** Database Migrations

**** Jobs

A Kubernetes =Job= creates one or more =Pods= and runs them until the specified number of them complete successfully. In the case of database migrations for =service_discovery= the =Job= is one =Pod= with a container running [[https://flywaydb.org/][Flyway]] sharing a =Volume= with an =initContainer= which copies over the =SQL= files from the image of =service_discovery= being deployed. Being =initContainers= means they must all run to completion and succeed (finish with a status code of 0) before the main containers of the =Pod= are started. So before =flyway migrate= can be run by the main container of the =Job='s =Pod= the container with the migrations, it is actually the same image with the full =service_discovery= release we use to run the =Deployment=, must have successfully copied the migration to the shared =Volume= under directory =/flyway/sql=:

#+BEGIN_SRC yaml
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    service: flyway
  name: flyway
spec:
  ttlSecondsAfterFinished: 0
  template:
    metadata:
      labels:
        service: flyway
    spec:
      restartPolicy: OnFailure
      volumes:
      - name: migrations
        emptyDir:
          medium: Memory

      containers:
      - args:
        - "-url=jdbc:postgresql://$(POSTGRES_SERVICE):5432/$(POSTGRES_DB)"
        - -user=$(POSTGRES_USER)
        - -password=$(POSTGRES_PASSWORD)
        - -connectRetries=60
        - -skipCheckForUpdate
        - migrate
        image: flyway/flyway:9.22
        name: flyway
        volumeMounts:
        - name: migrations
          mountPath: /flyway/sql
        env:
        - name: POSTGRES_SERVICE
          value: POSTGRES_SERVICE
        - name: POSTGRES_DB
          value: POSTGRES_DB
        - name: POSTGRES_USER
          value: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          value: POSTGRES_PASSWORD

      initContainers:
      - name: service-discovery-sql
        image: service_discovery
        command: ["/bin/sh"]
        args: ["-c", "cp /opt/service_discovery/sql/* /flyway/sql"]
        volumeMounts:
          - name: migrations
            mountPath: /flyway/sql
#+END_SRC

The trouble with using a =Job= for migrations is applying a set of Kubernetes YAML resources can not update the image of a completed =Job= and cause it to be rerun. Applying a =Job= of the same name with a different =service_discovery= image will result in an error. There are a couple options to deal with this limitation.

One option, which is still an alpha feature as of Kubernetes 1.16, is setting =ttlSecondsAfterFinished: 0= in the =Job= specification. With this setting the =Job= will be eligible for deletion immediately after completion. Then, when the next deployment applies the new Kubernetes resources a new =Job= will be created, instead of attempting to update the =Job= from the last deployment.

The =ttlSecondsAfterFinished= options is still an alpha feature and requires manually enabling. You can try the feature on Google Cloud by creating an [[https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-clusters][alpha cluster]], but these clusters can only live for 30 days. An alternative solution is to manually (or with a script that can run in a CI pipeline) run migrations and delete the =Job= after completing a deployment.

An advantage of running the =Job= separately from the main =kubectl apply= is the deployment can be stopped if the =Job= failed.

**** Validating the Migration

If we want to be sure our =service_discovery= container does not start until the database has been successfully migrated the =flyway= command =validate= can be used. The container running =flyway validate= will succeed once the database it is checking has run all the migrations. The resulting container setup is essentially the same as in the last section when we did the migrations, but instead of =migrate= the command is =validate= and both the =flyway= container and the container copying the migrations to the shared volume are =initContainers= in this case:

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-discovery
spec:
  replicas: 1
  template:
    spec:
      volumes:
      - name: migrations
        emptyDir: {}
      initContainers:
      - name: service-discovery-sql
        image: service_discovery
        volumeMounts:
          - name: migrations
            mountPath: /flyway/sql
        command: ["/bin/sh"]
        args: ["-c", "cp /opt/service_discovery/sql/* /flyway/sql"]

      - image: flyway/flyway:9.22
        name: flyway-validate
        args:
        - "-url=jdbc:postgresql://$(POSTGRES_SERVICE):5432/$(POSTGRES_DB)"
        - "-user=$(POSTGRES_USER)"
        - "-password=$(POSTGRES_PASSWORD)"
        - "-connectRetries=60"
        - "-skipCheckForUpdate"
        - validate
        volumeMounts:
        - name: migrations
          mountPath: /flyway/sql
        env:
        - name: POSTGRES_SERVICE
          value: POSTGRES_SERVICE
        - name: POSTGRES_DB
          value: POSTGRES_DB
        - name: POSTGRES_USER
          value: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          value: POSTGRES_PASSWORD
#+END_SRC

Each init container is run in order to completion before the next is started, which makes the order in this case very important. If the container which copies the migration files to =/flyway/sql= were not first in the list of =initContainers=, and required by Kubernetes to run to successful completion before the next container is started, the migrations would not be copied to the volume before =flyway= is run.

*** Tilt for Local Development

[[https://tilt.dev/][Tilt]] is a tool for deploying and updating Docker images and Kubernetes deploys
locally for development purposes. By default it will only build and deploy
services against a local Kubernetes, such as =kind=, =microk8s=, etc. to safeguard against accidentally sending your development environment into production!

The easiest way to get started with Tilt is using a registry within the local
Kubernetes cluster. For =kind= with =ctlptl= a registry can be enabled easily:

#+BEGIN_SRC
apiVersion: ctlptl.dev/v1alpha1
kind: Registry
name: ctlptl-registry
port: 5005
---
apiVersion: ctlptl.dev/v1alpha1
kind: Cluster
product: kind
registry: ctlptl-registry
name: kind-adoptingerlang
#+END_SRC

To create the cluster use `ctlptl apply`:

#+BEGIN_SRC shell
$ ctlptl apply -f cluster.yaml
#+END_SRC



Tilt works off of a file named =Tiltfile= at the root of the project. Stepping through the =Tiltfile= at the root of [[https://github.com/adoptingerlang/service_discovery][service_discovery]] it begins with:

#+BEGIN_SRC python
default_registry('127.0.0.1:32000')
#+END_SRC

=default_registry= sets where to push Docker images built by Tilt, in this case the registry enabled in =microk8s= is used. This line is not actually required because Tilt will pick up that =microk8s= with an enabled registry is being used and automatically configure itself to use that registry.

Next the build of a couple of Docker images is configured, starting with =service_discovery_sql=:

#+BEGIN_SRC python
custom_build(
    'service_discovery_sql',
    'docker buildx build -o type=docker --target dev_sql --tag $EXPECTED_REF .',
    ['apps/service_discovery_postgres/priv/migrations'],
    entrypoint="cp /app/sql/* /flyway/sql"
)
#+END_SRC

This image is built from the =Dockerfile= target =dev_sql=:

#+BEGIN_SRC dockerfile
FROM busybox as dev_sql

COPY apps/service_discovery_postgres/priv/migrations/ /app/sql/
#+END_SRC

It only contains the SQL migration files and uses =busybox= so that the =entrypoint= can use =cp=. The third argument to =custom_build=, =['apps/service_discovery_postgres/priv/migrations']=, tells Tilt to rebuild the image if any file in that directory, in this case the SQL migration files, changes. We will see how this image is used when we get to the kustomize deployment later in the =Tiltfile=.

Notice the function [[https://docs.tilt.dev/custom_build.html][custom_build]] is used. Tilt offers a simpler function =docker_build= for building Docker images if that suites your needs. In the case of =service_discovery= we have specific =targets= in the =Dockerfile= to use and wanted to ensure the use of =buildx=.

The next image built is =service_discovery=, the main image used by the =Deployment=:

#+BEGIN_SRC python
custom_build(
    'service_discovery',
    'docker buildx build -o type=docker --target dev_release --tag $EXPECTED_REF .',
    ['.'],
    live_update=[
        sync('rebar.config', '/app/src/rebar.config'),
        sync('apps', '/app/src/apps'),
        run('rebar3 as tilt compile'),
        run('/app/_build/tilt/rel/service_discovery/bin/service_discovery restart')
    ],
    ignore=["rebar.lock", "apps/service_discovery_postgres/priv/migrations/"]
)
#+END_SRC

Note that =rebar.lock= is explicitly ignored, this is because it gets rewritten at times when it hasn't actually changed and Tilt does no comparison to verify a change has actually occurred, so would needlessly run the =live_update= instructions when Rebar3 is run locally. The migration files are also ignored because that is handled by the other Docker image.

The target is =dev_release= because we want the ability to do [[https://blog.tilt.dev/2019/04/02/fast-kubernetes-development-with-live-update.html][live updates]] simply by recompiling and restarting inside the running image:

#+BEGIN_SRC dockerfile
# image to use in tilt when running the release
FROM builder as dev_release

COPY . .
RUN rebar3 as tilt release

ENTRYPOINT ["/app/_build/tilt/rel/service_discovery/bin/service_discovery"]
CMD ["foreground"]
#+END_SRC

The =live_update= instructions will run in the container any time a file changes. The Rebar3 profile =tilt= uses =dev_mode= for the release building, meaning to update the compiled modules in the release requires only running =compile= and not having to rebuild the whole release -- see the [[https://adoptingerlang.org/docs/production/releases/#building-a-development-release][Releases Chapter]] for more details on =dev_mode= and release building -- so the update commands simply sync the =apps= directory to the running container, run =compile= and restart the release.

Finally, the Kubernetes resources to deploy are configured in the =TiltFile=, and we set a watcher to rerun it when any of the kustomize files change.

#+BEGIN_SRC python
k8s_yaml(kustomize('deployment/overlays/dev'))

watch_file('deployment/')
#+END_SRC

Tilt has built-in support for kustomize so we use =kustomize('deployment/overlays/dev')= to render the =dev= overlay and pass to =k8s_yaml= which tells Tilt what Kubernetes resources to deploy and track.

The important differences from the =base= overlay in the =dev= overlay's =kustomization.yaml= is the inclusion of the Postgres kustomize resources and a patch that is merged on the =base= resources:

#+BEGIN_SRC yaml
bases:
- ../../base
- ../../postgres
patchesStrategicMerge:
- flyway_job_patch.yaml
#+END_SRC

The =fly_job_patch.yaml= is used to configure the Flyway job to work with the Tilt setup:

#+BEGIN_SRC yaml
# For tilt we make an image named service_discovery_sql with the migrations.
# This patch replaces the image used in the flyway migration job to match.
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    service: flyway
  name: flyway
spec:
  template:
    spec:
      initContainers:
      - name: service-discovery-sql
        image: service_discovery_sql
        volumeMounts:
          - name: migrations
            mountPath: /flyway/sql
        command: ["/bin/sh"]
        args: ["-c", "cp /app/sql/* /flyway/sql"]
#+END_SRC

This patch changes the image name in the =Job= resource to =service_discovery_sql=, the same name as the first =custom_build= image in the =Tiltfile=. Tilt will update the image with the latest tag, which it sets as =$EXPECTED_REF= in the environment of the Docker build command in =custom_build=, and reruns the =Job=. This way while =service_discovery= is running in the local Kubernetes if a new migration is added it will automatically be picked up and run on the database, keeping our development cluster in sync with our local development.

After running =tilt=:

#+BEGIN_SRC shell
$ tilt up
#+END_SRC

A console UI is brought up that shows the status of bringing up the difference resources that were passed to =k8s_yaml= in the =Tiltfile= and the logs associated with them:

[[./static/img/tilt-console.png]]

Tilt will also automatically open a page in the browser showing the same information:

[[./static/img/tilt-webui.png]]

Using =kubectl= the IP of =service_discovery= can be found:

#+BEGIN_SRC shell
$ kubectl get services --namespace=service-discovery-dev
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
postgres-dev            ClusterIP   10.152.183.116   <none>        5432/TCP                     16m
service-discovery-dev   ClusterIP   10.152.183.54    <none>        8053/UDP,3000/TCP,8081/TCP   16m
#+END_SRC

And we can interact with the running service_discovery through =curl= and =dig= to verify it is functioning properly:

#+BEGIN_SRC shell
$ curl -v -XPUT http://10.152.183.54:3000/service \
    -d '{"name": "service1", "attributes": {"attr-1": "value-1"}}'
$ curl -v -XGET http://10.152.183.54:3000/services
[{"attributes":{"attr-1":"value-1"},"name":"service1"}]
#+END_SRC


*** Clustering

Coming soon...

*** StatefulSets

Coming soon...

** TODO Operations
:PROPERTIES:
:EXPORT_FILE_NAME: operations
:EXPORT_HUGO_MENU: :menu main :parent production
:END:
*** Remote access
*** How to have a crashdump saved to a persistent volume
*** Metrics
**** VM
**** Libraries
**** Custom
*** Logging
*** Distributed tracing
* Team Building
:PROPERTIES:
:EXPORT_HUGO_SECTION: docs/team_building
:END:

** DONE Index
CLOSED: [2019-08-08 Thu 08:06]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_FRONT_MATTER_KEY_REPLACE: title>label
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :title "Team Building"
:EXPORT_HUGO_MENU: :menu main :weight 4001 :identifier team_building
:END:

#+BEGIN_EXPORT html
<header>
  <h1>Team Building</h1>
  <h5>
    <strong>August 3, 2019</strong>
  </h5>
</header>
#+END_EXPORT

No business works only with code; it is way more frequent to hear about projects that failed due to bad communications and dysfunctional teams than it is to hear about projects that failed purely due to bad technological choices. This part of the book is dedicated to team-building, to figure out how to bootstrap a team of Erlang developers when you're starting without any in-house expertise.

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/production">← Prev</a></div>
  <div><a href="/docs/team_building/">Next →</a></div>
</div>
#+END_EXPORT

** DONE Who to Put on The Team
:PROPERTIES:
:EXPORT_FILE_NAME: on_the_team
:EXPORT_HUGO_MENU: :menu main :parent team_building
:END:

In this chapter, we'll look into the best ways to set up your team for success when getting started. We're not talking about tools, but rather about who is going to be on that team. Don't worry, you shouldn't need to fire or replace anyone; however, you'll have to create expertise in a new area where you currently have little.

We'll give you two principal approaches: building around an expert or creating your expertise in-house. We'll also tackle the topic of remote teams.

One piece of advice that is valuable in all cases: start by favoring those who are willing and excited to learn about new technology, Erlang in this case. You can't really force people to learn against their will, but you can absolutely help those who are willing to do so.

*** Building Around an Expert

The easiest way to gain expertise into a new technology or a new business domain is often to just hire someone with that expertise. Unfortunately, this never truly solves your problem on its own; you then depend on your expert for everything. This ends up creating bottlenecks for all changes going through the system, resulting in massive slowdowns when your expert is on leave.

#+attr_shortcode: info "Hiring an Expert"
#+begin_admonition
#+begin_export html
For this section, we'll more or less assume you know how to find an expert on your own, and will focus on structuring the team and figuring out what such an expert will need to help you with. For help on actually hiring people, we'll add a "How to Hire" chapter that should help clear things up.
#+end_export
#+end_admonition

At the time of this writing, the average tenure time in the industry is below 5 years. This means that a team willing to be sustainable would be hiring expert after expert with no end in sight. The secret is to train your own experts after having found your first one. We consider it healthier for a company to be able to consistently hire more junior employees and see them leave as experts over the years than it would be to just poach experts and senior-level employees all the time and see them leave all the same.

Make no mistake here, we're not arguing against expertise. We're arguing in favor of it: expertise is so important that you should be focusing on producing it in-house. Places that can't offer the environment that helps their employees grow are at the mercy of their competition doing it for them. As such, building around an expert implies that the expert is your partner in building the rest of your team. In fact, the rest of your team should also be aware of this so they get in the right mindset, and make sure to get the most they can out of it.

This should be a role of which they are fully aware, and which you discuss with them over time. The expectation should be that you start slow with your expert and the people in your organization who are most enthusiastic about learning Erlang. In your first few weeks, your expert's role will be to mostly give your folks the basic knowledge they need to get going. This will usually follow the pattern of most beginners' books: the basic syntax and data types, teaching recursion and writing small modules, showing them how to test their code, then slowly adding in multiprocessing and OTP behaviours.

Once your team starts feeling comfortable enough on the basics, you'll be able to have your people start working on their first prototype of the system you want them to write and maintain in the long term. An interesting approach there is to actually use that first prototype as a training setup and nothing more. Everyone on board knows the prototype is not meant to reach production, and if it ever does, the first thing on the roadmap is to replace it once you have the proper experience.

The prototype is important, though; by picking a problem they're familiar with, it becomes a test bed where your employees are free to experiment in order to get comfortable around the language and its peculiarities. This creates an opportunity for them to revisit their domain knowledge and to teach it to the expert, while the expert works with them to figure out how they could re-frame what they know within the context and semantics of Erlang. This will hopefully create a rapport and complicity between them, where the relationship shifts from "trainer and trainee" to just having a lot of people teaching things to each other.

As this prototyping work is being done, your expert should start guiding your team towards broader and more complex concepts: developing for fault tolerance (how do you structure supervision trees properly?), building and deploying OTP releases, and, finally, performance-related and production debugging concerns.

One of our favorite experiments to run is to have the expert guide the team into building their own supervision structure, ideally with the prototype the team has been working on:

1. Start with the various components the team has identified (web servers, connection handlers, encoders/decoders, configuration managers, and so on), and draw them on a whiteboard
2. Draw communication channels between the components. Highlight the places where messages are being passed or where network communication takes place.
3. Ask of each communication channel "is this something we expect to fail often, what happens when it does?". Make the team aware of the potential for faults in the system
4. For each of the faults, ask what should be happening with the state held by each component. Is it going to be transient? Is it allowed to be dropped? Can it be re-built or fetched from somewhere?
5. Insert supervisors above the components that could be failing and assume that after each fault, each of the components starts from a predictable non-dynamic state (initial arguments passed by the supervisor)
6. Iteratively rework the components and re-structure them; move the state you can't afford to lose out from the boxes where failure is far more easy to foresee, and think of how information gets transferred there on each restart.

This exercise is where we've seen the most "enlightenment" about Erlang. The team suddenly realizes they have amassed enough understanding to re-frame and re-structure their whole approach to designing their software. They start making use of features unique to the Erlang virtual machine and the OTP framework, and build differently, leveraging new affordances. Even if some of your developers drop out of the Erlang train and go back to their prior tasks, they will likely take that experience in how to structure things and apply it in older places. We've heard of people doing it and saying they now thought differently about their C#, Go, or even Ruby design once they tried that one.

Once you think they get it, ask the whole team to re-design the prototype with what they now know. They can possibly keep some of the components and reuse them, but they should not feel bad at all about throwing a lot of it away, and neither should you: you'll be rewriting a thing written by near-complete beginners for something written by people with some experience. As a retrospective, it's interesting to compare what they did in the prototype with what they ended up with in the real product.

Doing the full learning experiment may take a few months, but even after a few weeks you'll likely see some of your non-expert employees slowly gain confidence and start helping others. Your expert, knowing you want his expertise to spread, should actually encourage that behaviour, and start delegating on the more basic questions. Then the transition to a more mature and functional team will be taking place. People will be teaching each other about various topics; your expert's expertise will be less and less necessary—likely only for tricky issues that are more and more rare—and they'll then be free to either participate to the projects on more equal footing, or start moving to other teams you have that also want to adopt the technology.

#+attr_shortcode: tip
#+begin_admonition
#+begin_export html
Regardless of how you run things at that point, make sure that from time to time your expert sits down with the rest of the team, and asks them about the various points of friction they have. Figure out what annoys them about the toolings, the language, and so on. This will help everyone re-calibrate and improve things. It's otherwise frequent for newcomers to just assume "this is how it is" and they may end up living with far more annoyances than they otherwise would.
#+end_export
#+end_admonition

*** Building Without an Expert

Building without an expert is definitely a bit trickier. It's not impossible (some people had to become experts on their own to get going) and there are a lot of resources out there—books, talks, forums, chat rooms, and so on—but it's definitely more of a challenge and it can be costlier. The biggest risk in building without an expert is that you may not know how long you're going to spend in wrong paths and bad avenues, and the bad learning you do early can stick for a longer time.

This is, ultimately, not too different from regular development, but you'll be headed that way willingly. As such, you may want to structure things a bit differently. Rather than picking one expert and your enthusiastic folks, you will want to explode the requirements and roles across everyone, to try and get the dynamics that an expert would have facilitated.

Identify people who have prior knowledge of functional languages; they won't know everything, but they already have a head start. Also identify some self-directed learners, those who enjoy sitting down and digging into something. Their job will be to read books and figure things out a few weeks ahead of everyone else; they're quite literally the piano teacher only ahead of you by a couple of weeks. You'll want to find someone who enjoys teaching, writing documentation, or preparing demos. That's often pretty hard, so if you can't find anyone, assign a rotating schedule to that responsibility. Put them in charge of doing broader information dissemination, maintaining docs, and a quick "getting started" track to what you do in your organization.

Have everyone do bits of development and getting used to the feel of the language, getting involved with things in the community, and leverage the code already existing out there. Here are a few ideas:

- look into existing code, and do a show-and-tell of various libraries and their approaches; dig in to figure out how they are structured.
- if you can't figure out why an open-source library does what it does, feel free to ask their authors in an issue, or people on slack, mailing lists, or a forum. Get the real answer, and show it out there
- do code reviews on the code you have written, where the main objective is everyone discussing some things where you (so far) had not had agreements. This may include syntax and alignment, but will more likely include things like just how much do you document, the better testing approaches you'd prefer to normalize on, and so on.
- when you get stumped, have a quick chat to figure out what approaches are possible to work around issues, what you will do to experiment, and finally figure out how to make a decision to move ahead.
- do experiment reports; if a few people on your team worked on something tricky, have them report back and share their experience to other people.

You'll also want to ask people for help online. Answers might be slow, but they can eventually come. StackOverflow is alright, but has never really managed to foster a community for Erlang. More interactive areas ([[https://erlef.org/slack-invite/erlanger][slack]], IRC, [[https://erlang.org/mailman/listinfo/erlang-questions][mailing lists]], [[https://elixirforum.com/c/erlang-forum/84][forums]]) tend to have longer-lasting memberships with a better way to figure out what you actually need. Remember that newcomers to a technology are always at risk of suffering from the [[https://en.wikipedia.org/wiki/XY_problem][XY problem]], and so will your team.

If you do have issues that you have trouble moving past, or feel you need a pair of more expert eyes to make sure you're going the right way, another option is possibly to hire a consultant or a consulting company for a week or two to answer your biggest question and help fix your most glaring issues, which might be less trouble than finding and hiring an expert.

*** To Remote or Not To Remote

The problem with Erlang and its community is that there are always more people looking for work than there are people hiring, and at the same time people hiring who can't find employees. This is essentially due to the fact that the geographical distribution of Erlang developers and Erlang companies does not necessarily align. The people who want Erlang jobs and the people who want Erlang developers don't tend to be at the same place at the same time.

Going for remote workers means that finding an expert or adept Erlang engineers will be a lot easier, and they will be able to tap into their own network to bring more on board. It does however come with change in habits, which following the 2020 global COVID-19 pandemic, might turn out to be something most tech companies will have tried for a while.

And so the final choice on this front will come from the preferences you have to run a team; direct availability might be something to push you towards remote workers to add expertise to your team more rapidly; most places in the world don't have a ready supply of Erlang experts waiting to work for them in their local market. There can be huge time and cost savings in getting someone out-of-market to help you quickly, particularly when comparing the alternative of doing everything from scratch and developing your own in-home expertise without direct help.

Such savings would need to be done by potentially changing the way your team works and communicates. If you currently do not do remote or distributed work and your expert would be the only remote person, this also represents a challenge on your expert; being the only remote person on a team can be particularly isolating, and keeping them happy may turn complicated.

*** What Next

So we've covered broad strokes of how to get going and how to structure things when you start with the help of an expert, or without their help. In all cases, focus on learning, training, and growing your team. It's a healthy pattern, and it is healthy whether you are remote or all on-site.

With these broad lines covered, we can start exploring the processes and practices that you'll want to put in place regardless of which approach you have taken.

** TODO Processes
:PROPERTIES:
:EXPORT_FILE_NAME: processes
:EXPORT_HUGO_MENU: :menu main :parent team_building
:END:

*** Code Reviews
*** Common Architecture Decisions
*** Prototype and Throw Away
*** Internal Training

** TODO How To Hire
:PROPERTIES:
:EXPORT_FILE_NAME: how_to_hire
:EXPORT_HUGO_MENU: :menu main :parent team_building
:END:

*** It Takes One to Know One
**** Hiring for Mentorship
*** It is Easier to Train than Hire

** DONE How to Do Polyglot Right
:PROPERTIES:
:EXPORT_FILE_NAME: how_to_polyglot
:EXPORT_HUGO_MENU: :menu main :parent team_building
:END:

You're adopting Erlang; unless you're a new company with no existing code base, that means that you're planning to become a polyglot organisation. Even if your roadmap plans for a total replacement of all existing code, there will be a transition period during which you will have to be polyglot. Therefore, you should plan to properly support polyglot workflows.

In this chapter, we'll cover common polyglot myths and misguided ideas, propose to impose standards for language viability, and raise a few challenging points your organisation will likely go through.

*** Polyglot Myths

There are plenty of beliefs and arguments that get brought up when engineers start discussing the idea of changing an organisation to be polyglot. Let's look at common complaints:

- build engineering becomes more complex as more tools and build chains need support
- more dependencies need auditing and upkeep to keep everything current
- as time goes on, some languages become uncool and require more effort to train, hire for, or rewrite
- we're already doing polyglot and there's this one component nobody wants to touch because a dev once wrote it in a language nobody else knows

Build engineering is a legitimate concern, in the same way many organizations with a dedicated IT department ship out a single type of computer for every developer to in order to reduce the number of configurations that must be supported. That being said, this is a less prevalent issue with the advent of pervasive containerization. Docker images and runtimes such as Kubernetes already provide a one-size-fits-all interface that extends from local development to production runs. Maintaining build images with the help of people familiar with the programming languages on board mostly reduces the complexity of everything involved over time.

There isn't necessarily a great way to work around dependency explosion and monitoring, short of using tools that scan and report issues for you. Dependency surface area is mostly a problem of managing complexity, and isn't necessarily tied to how many languages you're using. For example, we've seen a single Javascript demo project with a dependency tree larger than established Erlang software used for entire departments in a company. It's also becoming more common for other tools, such as dependency vulnerability scanners, to support Hex packages.

If your company has a procedure where each dependency must be vetted by the security team, the legal department (for licensing purposes), and engineers (for code quality), you'll find out that in practice, it doesn't work very well. People will either stick to a pre-approved list of libraries or will only review top-level dependencies. In some cases, they'll never update outdated libraries because they prefer to avoid all the red tape. In other cases the developers will just write the dependency into their project in order to avoid having to cross many departments of reviews if they feel they gotta ship things fast, and you'll end up with code that barely gets reviewed at all, and possibly of poor quality because they may have rewritten established libraries. We consider this to be an organisational issue, not a programming language issue.

Uncool languages or languages that are "hard to hire for" will always be a problem. Unfortunately, this isn't a problem you can  actually solve, because no one knows the future.  Is it worse to have a single main language people stop liking and then none of your stack is easy to staff, or to have only half of it being hard to hire for? What about cases like Python, where the migration from 2.x to 3.x has been so hard that libraries and components essentially consider them to be distinct languages? We consider that organisations that do not marry themselves to a single language but instead make a habit of switching between a few of them (and not just a ton of them) train themselves to better adjust to variations in the market of programming languages.

Then we have the complaint about the lone developer not caring about what the organisation does or knows, and who starts shipping things in a language nobody else knows. That developer might be doing this as part of resume padding, and then leave everyone behind after hopping to a new job. There's no protection against that, even when you mandate a single language. Hell, a developer could do the same by using your company's main language but by trying a new framework or new paradigm supported by the language or one of its libraries. Having a single unsupported [[https://aws.amazon.com/lambda/][Lambda]] could be trickier than a new language within a known deployment context.

We mostly consider this negative behaviour, letting such work take place in complete isolation, a failure of process that must be addressed, rather than blaming the tool for being used.

Finally, there's one major argument to counter. It's commonly accepted that having many languages creates barriers, and that you will have an easier time moving developers around if you use one language and one framework. In fact, we would call this the biggest myth against polyglot organisations.

It's misguided because it assumes developers are like mechanical parts. As long as the interface is the same, they can be interchangeable. It ignores everything not related to the language. Different teams have different habits and dynamics you need to get used to. They work on projects with different histories and challenges, in possibly distinct domains, with different focuses and compromises done in how they engineer things. To add to this, even within a single language, there is a cost of migration: different frameworks, libraries, toolchains, or versions of these toolchains all require some adaptation.

We argue that together, these factors represent a challenge that is often as significant—if not more so—than learning a new programming language. Even if it's significant, the cost of learning new languages is less than it seems, mostly because it encompasses all of these underlying issues, which may still exist within a single language. It looks like the cost is high _because_ we traditionally don't provide support for any of the challenging things and they must be learned from scratch when switching ecosystems or teams.


*** Minimum Viable Language

Based on these myths and common complaints mentioned earlier, we suggest an approach that addresses these underlying issues directly, regardless of the programming language being used. We also propose minimal standards which must be met in order for a programming language to be adopted. Although it may be tempting to initially propose these factors as hard rules to be enforced, we believe that letting engineers participate in their creation will provide better buy-in and self-policing than the alternatives.

The first perspective to keep in mind in deciding what represents a _Minimum Viable Language_ (MVL) is that you will want to help your organization become a learning organization, where your developers train each other to continuously adapt. The objective is to guide learning and provide boundaries around the maturity levels of a piece of technology to be considered worth checking out.

To do this, we suggest making the costs of longer-term support obvious. As long as your developers can provide a reasonable path forward for that support, there would be no reason to ban that language. For example, ask to have the following questions answered:

- How do you deploy it?
- How do you provide observability for it? What are the metrics, logs, and tracing support like, and how do you add them to a project?
- Which build tools are going to be blessed for people to use, and how should developers set up their development environment?
- What test frameworks are provided, and how do you reconcile them with the overall test standards you have?
- How do you write documentation for it?
- Who are we going to ask questions about it in the future? Do we have any local experts and advocates? How many, and what are we going to do if we fall below a given threshold?
- If you do RPC, message passing, data serialization, use a specific database, which libraries should be used for each? Is there a need to write them?
- How do you approach structuring, writing, and debugging code in the language? Are there basic guidelines or tools that should be provided?
- How do you hire for these developers? How do you train those who do not know about the language?
- Do we have any basic on-boarding documentation to help people get started with any of the factors above?

Add or remove questions as you see fit. You can contextualize some like memory footprint, requirements for front-end languages, mobile development, back-end languages, desktop software, and so on. Come up with a baseline your engineers think should be provided. Answer them in the abstract, without thinking of a single programming language. Classify some of these items as critical, nice to have, and so on. You might come up with a table looking like this:

| Item | Critical | Nice to Have | In an Ideal World |
|------+---------+----------+--------------+-------|
| HTTP Server | bare server | web framework | popular web framework with community support |
| gRPC Library  | serialization/deserialization | full gRPC client/server | well-documented examples |
| Micro services | sample implementation | templates library | full tutorial |
| Number of local Experts | 2 | 4 | 10+ |
| Hireability | sizeable local user group | taught in college | everyone knows it |
| On-boarding | people pick things up on their own | some documentation exists | tutorials and training provided |
| Observability | logging and metrics library | guidelines on language- or framework-specific logs/metrics | OpenTelemetry support |
| Testing | unit testing framework | integration testing framework | advanced testing frameworks (Property-Based Testing, mutation testing, etc.) |
| ... | ... | ... | ... |

Establish a base criteria to hit, the critical items without which you can't adopt the language, and so on. Turn it into a rather objective scorecard you can use to validate decisions. Otherwise, what will happen is that a CTO or architect will push for whatever tech they fancy, without regards to actual support. The default reflex of organizations is actually to just find something someone higher up likes, and push for it and let the rest of the organization adjust to these whims. A score card (as simple or complex as you want it to be) aims to make people aware of the costs of adopting or changing technologies, and as a checklist of what they need to provide to help each other use it.

Run the scorecard against a language you want to adopt, and see if you could indeed adopt it successfully or what might be missing to do it well; run the scorecard against languages you already have adopted, and see if they would pass. If they don't, start by fixing the gaps. Grow a knowledge base that is guided by what your engineers feel they'd need to do their job more effectively. Specialize and adapt the requirements for some areas of the company, and see where this lands you.

Chances are that even in a company with a single language being mandated, you'll be missing plenty of the "nice to haves" or above points; When we choose to leave it to the community to handle that support, we end up underfunding internal support as well. That's how legacy codebases get created. You might also find out that different teams judge things very differently around their specific domain of expertise. Folks who talk to infrastructure components all the time may end up with very distinct requirements from teams who work on end-user features. You can start specializing score cards, or incorporating domain-specific elements to it.

*** A Production Language

As you iterate over all of these criteria you have established, you should keep some end-goal in mind. For example, you might want to end up with the following things covered (note that many of these are covered in this text to make things easier for more Erlang users out there):

- How to install and set-up your development environment with all the versions required
- A blessed list of libraries that have been validated and that most projects are using
- The procedures around starting new projects (setting up tests, which linters to use, how to get CI/CD going, code snippets and templates, and so on)
- The places where you can get help (such as internal mailing lists, chatrooms, or the names of people to contact)
- A library of learning resources, which can be books, tutorials, videos, or internal documents you found useful
- Security guidelines
- Checklists of things to do to make a service or application production grade
- How to benchmark your code
- How to instrument your code
- Architecture and contribution guidelines
- Runbooks and playbooks for common problems
- Interview questions

Not all of these need to be expansive tutorials in wikis; some can be accomplished by pairing, lunch and learns, presentations, and so on. Do note that regardless of how you choose to help on-board people for these items, the developers you hire or move across teams already have to figure _all of these_ out, many without explicit support. This invisible work is being done all the time. The aim is to make it visible.

In choosing to support polyglot languages and surfacing what your team believes is necessary to help them move from one project to the next (without creating orphaned technology), you're likely to actually make switching between teams easier than if you just had one programming language without that support. If you really want to help make such principles part of your culture, show that you value teams that participate in reaching and improving standard practices, and who do work to bring existing codebases up to the desired level of excellence. It can be tricky to get enough buy-in to progressively build these resources and keep them alive, so that last bit is particularly important.

In short, let's focus on how people learn and help them do it, rather than pretending they already know everything. Prepare starter guides around domain knowledge. Map out areas where you are currently under-serving your engineers and in which you possibly need to invest. Establish ways to socially keep language-specific knowledge and tooling alive. And possibly discover factors that you did not know were important but turn out to be critical to your teams doing their work.

** TODO Selling and Defending Erlang
:PROPERTIES:
:EXPORT_FILE_NAME: selling_and_defending
:EXPORT_HUGO_MENU: :menu main :parent team_building
:END:

Just adding notes:

"we talked to people who used tech x without support and they hated it" -> well duh, they didn't necessarily hate the tech as much as they hated the experience of using it without support. Over-focusing on the tech itself is a losing proposition.

also https://twitter.com/mononcqc/status/1265295824092893185

* Appendix 1: Erlang/OTP Cheat Sheets
:PROPERTIES:
:EXPORT_HUGO_SECTION: docs/cheat_sheets
:END:

** DONE Index
CLOSED: [2019-08-08 Thu 08:06]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_FRONT_MATTER_KEY_REPLACE: title>label
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :title "Appendix 1: Erlang/OTP Cheat Sheets"
:EXPORT_HUGO_MENU: :menu main :weight 5001
:END:

#+BEGIN_EXPORT html
<header>
  <h1>Cheat Sheets</h1>
  <h5>
    <strong>April 28, 2021</strong>
  </h5>
</header>
#+END_EXPORT

This section contains various reminders to jog your memory if you're not too fresh on basic Erlang data, types, or syntax.

*** Data Types

| Name  | Description  | Dialyzer | Example Syntax |
|-------+--------------+----------+----------------|
| integer | number without decimals | =integer()=, =pos_integer()=, =non_neg_integer()= | =1=, =2=, =3=, =-213=, =16#01FF=, =2#101011= |
| float   | number with decimals | =float()= | =1.0=, =-1.0=, =123.12=, =1.0e232= |
| number | either floats or integers | =number()= | =1.0=, =1= |
| atom | literals, constants with their own name for value | =atom()= | =abc=, ='abc'=, =some_atom@erlang=, ='atom with spaces'= |
| boolean | atoms =true= or =false= | =boolean()= | =true=, =false= |
| reference | unique opaque value | =reference()= | =make_ref()= |
| fun | anonymous function | =fun()=, =fun((ArgType) -> RetType)= | <code>fun(X) -> X end, fun F(0) -> []; F(N) -> [1 \vert F(N-1)] end</code> |
| port | opaque type for a file descriptor | =port()= | N/A |
| pid  | process identifier | =pid()= | =<0.213.0>= |
| tuple | group a known set of elements | =tuple()=, ={A, B, C}= | ={celsius, 42=}, ={a, b, c}=, ={ok, {X, Y}}= |
| map  | a dictionary of terms | =map()=, ~#{KType => VType}~, ~#{specific_key := VType}~ | ~#{a => b, c => d}~, ~Existing#{key := Updated}~ |
| nil  | an empty list | =[]= | =[]= |
| list | recursive structure for a list of terms | =list()=, =[Type]= | =[a, b, c]=, <code>[a \vert [b \vert [c \vert []]]]</code>, ="a string is a list"= |
| binary | a flat byte sequence | =binary()= | =<<1,2,3,4>>=, =<<"a string can be a binary">>=, =<<X:Size/type, _Rest/binary>>= |

Term ordering: =number < atom < reference < fun < port < pid < tuple < map < nil < list < binary=

*** Modules and Syntax

#+NAME: all_syntax_mod
#+BEGIN_SRC erlang
%%% This is a module-level comment
%%% @doc This tag includes officiel EDoc documentation.
%%% It can be useful for people to consule
%%% @end
%%% Generate documentation with rebar3 edoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Let's start with Module Attributes %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% This is an attribute or function-specific comment
%% attributes start with a `-', functions with letters.
%% This file should be saved as `sample.erl'
-module(sample).

%% Functions are described in the form Name/Arity, and must
%% be exported through an `-export([...]).' module attribute
-export([f/0, f/1]).
-export([x/0]).         % multiple export attributes can exist

%% You can "import" functions from another module, but
%% for clarity's sake (and because there's no namespaces)
%% nobody really does that
-import(module, [y/0]).

%% .hrl files contain headers, and are imported directly
%% within the module.
%% The following includes a private header file from src/
%% or a public header file from include/ in the current app
-include("some_file.hrl").
%% The following includes a public header file from the
%% include/ file of another application
-include_lib("appname/include/some_file.hrl").

%% specify an interface you implement:
-behaviour(gen_server).

%% Define a record (a tuple that compilers handles in a
%% special way)
-record(struct, {key = default :: term(),
                 other_key     :: undefined | integer()}).

%% Just C-style macros
-define(VALUE, 42).        % ?VALUE in this module becomes `42'
-define(SQUARE(X), (X*X)). % function macro
-define(DBG(Call),         % a fancy debug macro: ?DBG(2 + 2)
        io:format("DBG: ~s (~p): ~p~n",
                  [??Call, {?MODULE, ?LINE}, Call])).

%% Conditionals
-ifdef(MACRO_NAME).        % opposite: -ifndef(MACRO_NAME).
-define(OTHER_MACRO, ok).
-else.                     % other option: -elif(NAME).
-define(MACRO_NAME, ok).
-endif.

%% Type definitions
-type my_type() :: number() | boolean().
-type my_container(T) :: {[T], [T], my_type(), mod:type()}
-export_type([my_type/0, my_container/1]).

%% you can also define custom attributes:
-my_attribute(hello_there).
-author("Duke Erlington").

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% And now modules for code and functions %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% @doc A function with 0 arguments returning an atom
-spec f() -> term(). % optional spec
f() -> ok.

-spec f(number()) -> float().
f(N) -> N + 1.0.

%% Pattern matching with clauses
x([]) -> [];  % base recursive clause for a list
x([_H|T] -> [x | T]. % replace list element with `x' atom

%% @private variable binding rules
same_list(X = [_|_], X) -> true;
same_list([], []) -> true;
same_list(_, _) -> false.

%% Operators in the language
operators(X, Y) ->
    +X, -Y, % unary
    X + Y, X - Y, X * Y, X / Y,   % any numbers
    X div Y, X rem Y,             % integers-only
    X band Y, X bor Y, X bxor Y,  % binary operators
    X bsl Y, X bsr L,             % bit shifting
    not X,                        % boolean not
    X andalso Y, X orelse Y,      % shortcircuit boolean operators
    X < Y, X > Y, X >= Y, X =< Y, % comparison
    X == Y, X /= Y,               % equality (float == int)
    X =:= Y, X =/= Y,             % strict equality (float =/= int)
    X ++ Y, X -- Y,               % append Y to X, delete Y from X
    X ! Y.                        % send message Y to process X

%% Using guards. Valid guard expressions at:
%% erlang.org/doc/reference_manual/expressions.html#guard-sequences
comfortable({celsius, X}) when X >= 18, X =< 26 -> % AND clauses
    true;
comfortable({celsius, _}) ->
    false.

uncomfortable({celsius, X}) when X =< 18; X >= 26 -> % OR clauses
    true;
uncomfortable({celsius, _}) ->
    false.

%% difference with 'andalso' and 'orelse'
conds(X) when (is_number(X) orelse is_integer(X))
               andalso X < 9 ->
    %% equivalent (A AND B) OR C
    true;
conds(X) when is_number(X); is_integer(X), X < 9 ->
    %% - parentheses impossible with , or ;
    %% - equivalent to A OR (B AND C)
    true;
conds(T) when element(1, T) == celsius; is_integer(T) ->
    %% element/2 extracts an element from a tuple. If `T' is
    %% not a tuple, the call fails and `is_integer/1' is tried
    %% instead
    true;
conds(T) when element(1, T) == celsius orelse is_integer(T) ->
    %% this can never work: if element/2 fails, the whole
    %% `orlese' expressoin fails and `is_integer/1' is skipped
    true.

%% Conditionals
conditional('if', Light) ->
    if Light == red -> stop;
       Light == green; Light == yellow -> go_fast;
       true -> burnout % else clause!
    end;
conditional('case', {Light, IsLate}) ->
    case Light of
        green -> go;
        yellow when IsLate -> go_fast;
        _ -> stop
    end;
conditional(pattern, green) -> go;
conditional(pattern, yellow) -> slow;
conditional(pattern, red) -> stop.

%% List and binary comprehensions
comp(ListA, ListB) ->
    [X*X || X <- ListA, X rem 2 == 0], % square even numbers
    [{X,Y} || X <- ListA, Y <- ListB], % all possible pairs
    << <<X:8>> || X <- ListA >>.       % turn list into bytes
comp(BinA, BinB) -> % now with binaries
    << <<X*X:32>> || <<X:8>> <= Bin, X rem 2 == 0 >>,
    [{X,Y} || <<X:32>> <= BinA, <<Y:8>> <= BinB],
    [X || <<X:8>> <= BinA].

%% Anonymous and higher order functions
higher_order() ->
    If = fun(Light) -> conditional('if', Light) end,
    Case = fun(Light) -> conditional('case', {Light, true}) end,
    lists:map(If, [green, yellow, red]),
    lists:map(Case, [green, yellow, red]),
    If(red), % can be called literally
    lists:map(fun(X) -> X*X end, [1,2,3,4,5]).

try_catch() ->
    try
        some_call(),     % exceptions in this call are caught as well
        {ok, val},       % common good return value to pattern match
        {error, reason}, % common bad return value to pattern match
        % any of these expression aborts the execution flow
        throw(reason1), % non-local returns, internal exceptions
        error(reason2), % unfixable error
        exit(reason3)   % the process should terminate
    of  % this section is optional: exceptions here are not caught
        {ok, V} ->
            do_something(V),
            try_catch(); % safely recurse without blowing stack
        {error, R} ->
            {error, R} % just return
    catch % this section is optional: various patterns
        throw:reason1 -> handled;
        reason2 -> oops; % never matches, `throw' is implicit type
        error:reason2 -> handled;
        exit:reason3 -> handled;
        throw:_ -> wildcard_throws;
        E:R when is_error(E) -> any_error;
        _:_:S -> {stacktrace, S}; % extract stacktrace
    after -> % this is an optional 'finally' block
        finally
    end.
#+END_SRC


*** Processes and Signals

#+NAME: concurrency_constructs
#+BEGIN_SRC erlang
%% Start a new process
Pid = spawn(fun() -> some_loop(Arg) end)
Pid = spawn('name@remote.host', fun() -> some_loop(Arg) end)
Pid = spawn(some_module, some_loop, [Arg])
Pid = spawn('name@remote.host', some_module, some_loop, [Arg])
%% Spawn a linked process
Pid = spawn_link(...) % 1-4 arguments as with spawn/1-4
%% Spawn a monitored process atomically
{Pid, Ref} = spawn_monitor(fun() -> some_loop(Arg) end)
{Pid, Ref} = spawn_monitor(some_module, some_loop, [Arg])
%% Spawn with fancy options
spawn_opt(Fun, Opts)
spawn_opt(Node, Fun, Opts)
spawn_opt(Mod, Fun, Args, Opts)
spawn_opt(Node, Mod, Fun, Args, Opts)
%% Options must respect the following spec; many are advanced
[link | monitor |
 {priority, low | normal | high | max} |    % don't touch
 {fullsweep_after, integer() >= 0} |        % full GC
 {min_heap_size, Words :: integer() >= 0} | % perf tuning
 {min_bin_heap_size, Words} |
 {max_heap_size,                    % heap size after which
   Words |                          % the process may be killed. Use
   #{size => integer() >= 0,        % to indirectly set max queue sizes
     kill => boolean(),
     error_logger => boolean()}}

%% send an exit signal to a process
exit(Pid, Reason)

%% Receive a message
receive
    Pattern1 when OptionalGuard1 ->
        Expression1;
    Pattern2 when OptionalGuard2 ->
        Expression2
after Milliseconds -> % optional
    Expression
end

%% Naming processes
true = register(atom_name, Pid)
true = unregister(atom_name)
Pid | undefined = whereis(atom_name)

%% Monitor
Ref = erlang:monitor(process, Pid)
true = erlang:demonitor(Ref)
true | false = erlang:demonitor(Ref, [flush | info])

%% Links
link(Pid)
unlink(Pid)
process_info(trap_exit, true | false)
#+END_SRC

And the semantics for links and monitors, in diagram forms:

#+CAPTION: Monitors are unidirectional informational signals, and they stack
#+NAME: fig:sig_mon
[[./static/img/sig_mon_sm.png]]

#+CAPTION: Untrapped links are bidirectional and kill the other process, except if the reason is 'normal'
#+NAME: fig:sig_linked_notrap
[[./static/img/sig_linked_notrap_sm.png]]

#+CAPTION: Trapped links are converted to messages, except for the untrappable 'kill' reason
#+NAME: fig:sig_linked_trap
[[./static/img/sig_linked_trap_sm.png]]

OTP processes do have slightly different semantics due to supervision shenanigans:

#+CAPTION: Untrapped links work the same for OTP
#+NAME: fig:sig_otp_notrap
[[./static/img/sig_otp_notrap_sm.png]]

#+CAPTION: Trapped links behave in a special way when the parent of a process is the one that dies
#+NAME: fig:sig_otp_trap
[[./static/img/sig_otp_trap_sm.png]]

#+CAPTION: Supervisors log things differently based on the termination reason
#+NAME: fig:sig_otp_own
[[./static/img/sig_otp_own_sm.png]]

*** Behaviours

Not all OTP behaviours are listed here, only thee most frequently-used ones.

**** Applications

| Trigger | Called By | Handled By | Return | Description |
|---------+-----------+------------+--------+-------------|
| =application:start/1-2= | client or booting VM | =start(Type, Args)= | <code>{ok, pid()} \vert {ok, pid(), State}</code> | should start the root supervisor |
| ={start_phases, [{Phase, Args}]}= in app file | =kernel= booting the app | =start_phase(Phase, Type, Args)= | <code>ok \vert {error, Reason}</code> | Optional. Can isolate specific steps of initialization |
| =application:stop/1= | app shutting down | =prop_stop(State)= | =State= | Optional. Called before the supervision tree is shut down |
| =application:stop/1= | app shutting down | =stop(State)= | =term()= | called once the app is done running to clean things up |
| Hot code update | SASL's release handler | =config_change(Changed::[{K,V}], New::[{K,V}], Removed::[K])= | =ok= | Called after a hot code update using the VM's relup functionality, if the configuration values changed |

**** Supervisors

| Trigger | Called By | Handled By | Return | Description |
|---------+-----------+------------+--------+-------------|
| =supervisor:start_link/2-3= | parent process | =init(Arg)= | <code>ignore \vert {ok, {SupFlag, [Child]}}</code> | Specifies a supervisor. Refer to official documentation |

**** gen_server

| Trigger | Called By | Handled By | Return | Description |
|---------+-----------+------------+--------+-------------|
| =gen_server:start_link/3-4= | supervisor | =init(Arg)= | <code>{ok, State [, Option]} \vert ignore \vert {stop, Reason}</code> | Set up the initial state of the process |
| =gen_server:call/2-3= | client | =handle_call(Msg, From, State)= | <code>{Type::reply \vert noreply, State [, Option]} \vert {stop, Reason [, Reply], State}code> | Request/response pattern. A message is received and expects an answer |
| =gen_server:cast/2= | client | =handle_cast(Msg, State)= | <code>{noreply, State [, Option]} \vert {stop, Reason, State}</code> | Information sent to the process; fire and forget |
| =Pid ! Msg= | client | =handle_info(Msg, State)= | same as =handle_cast/2= | Out-of-band messages, including monitor signals and ='EXIT'= messages when trappig exit |
| Setting an =Option= value to ={continue, Val}= | the server itself | =handle_continue(Val, State)= | same as =handle_cast/2= | Used to break longer operations into triggerable internal events |
| =gen_server:stop/1,3= | client or supervisor | =terminate(Reason, State)= | =term()= | Called when the process is shutting down willingly or through errors. If the process does not trap exits, this callback may be omitted |
| =sys:get_status/2-3=, crash logs | client, the server itself | <code>format_status(normal \vert terminate, [PDict, State])</code> | =[{data, [{"State", Term}]}]= | Used to add or remove information that would make it to debugging calls or error logs |
| N/A | supervisor | =code_change(OldVsn, State, Extra)= | ={ok, NewState}= | called to update a stateful process if the proper instructions are given during a hot code upgrade with releases |

**** gen_statem

***** Process management

| Trigger | Called By | Handled By | Return | Description |
|---------+-----------+------------+--------+-------------|
| =gen_statem:start_link/3-4= | supervisor | =init(Arg)= | <code>{ok, State, Data [, Actions]} \vert ignore \vert {stop, Reason}</code> | Sets the initial state and data for the state machine |
| N/A | internal | =callback_mode()= | <code>[state_functions \vert handle_event_function [, state_enter]]</code> | Defines the type of FSM and whether entering a state triggers a special internal event |
| =gen_statem:stop/1,3= | client or supervisor | =terminate(Reason, State, Data)= | =term()= | Called when the process is shutting down willingly or through errors. If the process does not trap exits, this callback may be omitted |
| =sys:get_status/2-3=, crash logs | client, the server itself | <code>format_status(normal \vert terminate, [PDict, State, Data])</code> | =[{data, [{"State", Term}]}]= | Used to add or remove information that would make it to debugging calls or error logs |
| N/A | supervisor | =code_change(OldVsn, State, Data, Extra)= | ={ok, NewState, NewData}= | called to update a stateful process if the proper instructions are given during a hot code upgrade with releases |

***** State handling and transitions

Handled by either =handle_event/4= or =StateName/3= functions, based on the value of =callback_mode()=. The function signatures are either:

- =handle_event(EventType, EventDetails, State, Data)=
- =State(EventType, EventDetails, Data)=

If the value of =State= is not a list, even though =callback_mode()= defined =state_functions=, then =handle_event/4= will be called. All possible return values for either functions are one of:

- ={next_state, State, Data}=
- ={next_state, State, Data, [Actions, ...]}=
- ={stop, Reason, Data}=
- ={stop, Reason, Data, [Actions, ...]}=

Various short forms exist, such as =keep_state_and_data=, ={keep_state, Data}=, ={repeat_state, Data}=, and many more. Refer to the documentation for their content.

The =Actions= value is any combination of the following list (non-inclusive): =postpone=, ={next_event, EventType, EventDetails}=, =hibernate=, ={timeout, Delay, EventDetails}=, ={state_timeout, Delay, EventDetails}=, ={reply, From, Reply}=, =hibernate=. Consult the documentation for more options.


| Trigger | Called By | Event Type | Event Details | Description |
|---------+-----------+------------+---------------+-------------|
| =gen_statem:call/2-3= | client | ={call, From}= | =term()= | Request/response pattern. A message is received and is expected to receive an answer |
| =gen_statem:cast/2= | client | =cast= | =term()= | Information must be sent to the process; fire and forget |
| Pid ! Msg | client | =info= | =Msg= | Out-of-band messages, including monitor messages and ='EXIT'= signals that are trapped |
| ={timeout, T, Msg}= | =Action= return value | =timeout= | =Msg= | A specific timeout that can be set and received internally when the state machine has not received a new event in =T= milliseconds |
| ={state_timeout, T, Msg}= | =Action= return value | =state_timeout= | =Msg= | A specific timeout that can be set and received internally when the state machine has not transitioned to a new different state in =T= milliseconds |
| ={next_event, internal, Msg}= | =Action= return value | =internal= | =Msg= | Internal messages that can be generated by a state machine wanting to trigger itself without looking like external calls |

#+BEGIN_EXPORT html
<div class="pagination">
  <div><a href="/docs/team_building">← Prev</a></div>
</div>
#+END_EXPORT

* TODO Appendix 2: Releases

** The Boot Script

The boot file has variables that must be set properly in order to find libraries and the runtime, the bundled script takes care of making sure these are set properly and passed to =erl=.

=-boot_var ERTS_LIB_DIR "$ERTS_LIB_DIR"=

** Relx Configuration

** Release Start Script Commands

=ping=, =rpc=, =rpcterm=, =eval=, ...

* TODO Appendix 4: Systemd
:PROPERTIES:
:EXPORT_FILE_NAME: systemd
:EXPORT_FILE_NAME: systemd
:EXPORT_HUGO_MENU: :menu main :weight 6001
:END:

*** Systemd Unit

#+BEGIN_src shell
[Unit]
Description=Service Discovery Runner
After=network.target

[Service]
WorkingDirectory=/opt/service_discovery
EnvironmentFile=/etc/default/service_discovery.env
ExecStart=/opt/service_discovery/bin/service_discovery foreground
Restart=on-failure
Environment=RELX_OUT_FILE_PATH=/tmp/
Environment=COOKIE=service_discovery_cookie

[Install]
WantedBy=multi-user.target
#+END_src

As of OTP 19.3 a =SIGTERM= signal causes the OTP VM to gracefully shutdown.

#+BEGIN_src shell
journalctl service_discovery
#+END_src
* TODO Appendix 5: Using Docker Images for Continuous Integration Testing

The second image using =builder= as a base is a cache of the Dialyzer's Persistent Lookup Table (PLT):

#+BEGIN_SRC dockerfile
FROM builder as plt

RUN --mount=target=. \
    --mount=id=hex-cache,type=cache,target=/root/.cache/rebar3 \
    rebar3 dialyzer --succ-typings=false

ENTRYPOINT ["rebar3"]

CMD ["dialyzer"]
#+END_SRC

Rebar3 will store only the PLT of the OTP libraries in the global cache directory that is part of the =hex-cache=. A PLT that contains both the OTP libraries and the dependencies of the project is output to the build directory, in this case =/app/_build/=. This means we have a cached layer with the full PLT as well as the separate local cache of the OTP PLT. It is important to note that the base directory is still set to =/app/_build= in this image because when it is used to run =dialyzer= on the project's current code with =docker run= the mounting of the project's source would have replaced =/app/src/_build=, resulting in the PLT needing to be rebuilt from scratch. This is done because when running the container the project must be mounted as a volume to the same directory. Mounting a volume overrides any existing files on the path it is mounted to. Storing the PLT in a separate location ensure it is not lost when the volume is mounted and isn't rebuilt on each run.

Now to build the PLT image using the builder image as a cache (note that the stages =releaser= and =runner= will be skipped since they are not needed by the =plt= stage):

#+BEGIN_SRC shell
$ docker build --cache-from=service_discovery:builder-$CHKSUM --target plt -t service_discovery:plt-${CHKSUM} .
#+END_SRC

To run dialyzer on the current code use a volume to mount the current directory to =/src=:

#+BEGIN_SRC dockerfile
$ docker run -v $(pwd):/app/src service_discovery:plt-${CHKSUM}
===> Verifying dependencies...
===> Compiling service_discovery_storage
===> Compiling service_discovery
===> Compiling service_discovery_http
===> Compiling service_discovery_grpc
===> Compiling service_discovery_postgres
===> Dialyzer starting, this may take a while...
===> Updating plt...
===> Resolving files...
===> Checking 296 files in "/app/_build/default/rebar3_22.1_plt"...
===> Doing success typing analysis...
===> Resolving files...
===> Analyzing 19 files with "/app/_build/default/rebar3_22.1_plt"...
#+END_SRC

If there is a change to the dependencies the run will take a little longer while it updates the PLT. It is important to note that it will not update the saved layer when this happens. To update the cached PLT a new image must be built with =docker build= and the new checksum.
